CODA-19: Using a Non-Expert Crowd to Annotate Research Aspects on 10,000+ Abstracts in the COVID-19 Open Research Dataset | Ting-Hao Kenneth Huang | this paper introduces coda-19 , a human-annotated dataset that codes the background , purpose , method , finding/contribution , and other sections of 10 , 966 english abstracts in the covid-19 open research dataset. coda-19 was created by 248 crowd workers from amazon mechanical turk within 10 days , and achieved labeling quality comparable to that of experts. each abstract was annotated by nine different workers , and the final labels were acquired by majority vote. the inter-annotator agreement \( cohen’s kappa \) between the crowd and the biomedical expert \( 0.741 \) is comparable to inter-expert agreement \( 0.788 \) . coda-19’s labels have an accuracy of 82.2% when compared to the biomedical expert’s labels , while the accuracy between experts was 85.0%. reliable human annotations help scientists access and integrate the rapidly accelerating coronavirus literature , and also serve as the battery of ai/nlp research , but obtaining expert annotations can be slow. we demonstrated that a non-expert crowd can be rapidly employed at scale to join the fight against covid-19.
