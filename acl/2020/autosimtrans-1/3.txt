Improving Autoregressive NMT with Non-Autoregressive Model | Long Zhou | autoregressive neural machine translation \( nmt \) models are often used to teach non-autoregressive models via knowledge distillation. however , there are few studies on improving the quality of autoregressive translation \( at \) using non-autoregressive translation \( nat \) . in this work , we propose a novel encoder-nad-ad framework for nmt , aiming at boosting at with global information produced by nat model. specifically , under the semantic guidance of source-side context captured by the encoder , the non-autoregressive decoder \( nad \) first learns to generate target-side hidden state sequence in parallel. then the autoregressive decoder \( ad \) performs translation from left to right , conditioned on source-side and target-side hidden states. since ad has global information generated by low-latency nad , it is more likely to produce a better translation with less time delay. experiments on wmt14 en-de , wmt16 en-ro , and iwslt14 de-en translation tasks demonstrate that our framework achieves significant improvements with only 8% speed degeneration over the autoregressive nmt.
