What Question Answering can Learn from Trivia Nerds | Jordan Boyd-Graber | in addition to the traditional task of machines answering questions , question answering \( qa \) research creates interesting , challenging questions that help systems how to answer questions and reveal the best systems. we argue that creating a qa dataset—and the ubiquitous leaderboard that goes with it—closely resembles running a trivia tournament: you write questions , have agents \( either humans or machines \) answer the questions , and declare a winner. however , the research community has ignored the hard-learned lessons from decades of the trivia community creating vibrant , fair , and effective question answering competitions. after detailing problems with existing qa datasets , we outline the key lessons—removing ambiguity , discriminating skill , and adjudicating disputes—that can transfer to qa research and how they might be implemented.
