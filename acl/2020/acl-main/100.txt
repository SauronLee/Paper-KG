Towards Faithful Neural Table-to-Text Generation with Content-Matching Constraints | Zhenyi Wang | text generation from a knowledge base aims to translate knowledge triples to natural language descriptions. most existing methods ignore the faithfulness between a generated text description and the original table , leading to generated information that goes beyond the content of the table. in this paper , for the first time , we propose a novel transformer-based generation framework to achieve the goal. the core techniques in our method to enforce faithfulness include a new table-text optimal-transport matching loss and a table-text embedding similarity loss based on the transformer model. furthermore , to evaluate faithfulness , we propose a new automatic metric specialized to the table-to-text generation problem. we also provide detailed analysis on each component of our model in our experiments. automatic and human evaluations show that our framework can significantly outperform state-of-the-art by a large margin.
