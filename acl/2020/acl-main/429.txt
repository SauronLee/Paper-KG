Influence Paths for Characterizing Subject-Verb Number Agreement in LSTM Language Models | Kaiji Lu | lstm-based recurrent neural networks are the state-of-the-art for many natural language processing \( nlp \) tasks. despite their performance , it is unclear whether , or how , lstms learn structural features of natural languages such as subject-verb number agreement in english. lacking this understanding , the generality of lstm performance on this task and their suitability for related tasks remains uncertain. further , errors cannot be properly attributed to a lack of structural capability , training data omissions , or other exceptional faults. we introduce *influence paths* , a causal account of structural properties as carried by paths across gates and neurons of a recurrent neural network. the approach refines the notion of influence \( the subject’s grammatical number has influence on the grammatical number of the subsequent verb \) into a set of gate or neuron-level paths. the set localizes and segments the concept \( e.g. , subject-verb agreement \) , its constituent elements \( e.g. , the subject \) , and related or interfering elements \( e.g. , attractors \) . we exemplify the methodology on a widely-studied multi-layer lstm language model , demonstrating its accounting for subject-verb number agreement. the results offer both a finer and a more complete view of an lstm’s handling of this structural aspect of the english language than prior results based on diagnostic classifiers and ablation.
