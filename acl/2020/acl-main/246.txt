Span Selection Pre-training for Question Answering | Michael Glass | bert \( bidirectional encoder representations from transformers \) and related pre-trained transformers have provided large gains across many language understanding tasks , achieving a new state-of-the-art \( sota \) . bert is pretrained on two auxiliary tasks: masked language model and next sentence prediction. in this paper we introduce a new pre-training task inspired by reading comprehension to better align the pre-training from memorization to understanding. span selection pretraining \( sspt \) poses cloze-like training instances , but rather than draw the answer from the modelâ€™s parameters , it is selected from a relevant passage. we find significant and consistent improvements over both bert-base and bert-large on multiple machine reading comprehension \( mrc \) datasets. specifically , our proposed model has strong empirical evidence as it obtains sota results on natural questions , a new benchmark mrc dataset , outperforming bert-large by 3 f1 points on short answer prediction. we also show significant impact in hotpotqa , improving answer prediction f1 by 4 points and supporting fact prediction f1 by 1 point and outperforming the previous best system. moreover , we show that our pre-training approach is particularly effective when training data is limited , improving the learning curve by a large amount.
