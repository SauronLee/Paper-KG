Unsupervised Cross-lingual Representation Learning at Scale | Alexis Conneau | this paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. we train a transformer-based masked language model on one hundred languages , using more than two terabytes of filtered commoncrawl data. our model , dubbed xlm-r , significantly outperforms multilingual bert \( mbert \) on a variety of cross-lingual benchmarks , including +14.6% average accuracy on xnli , +13% average f1 score on mlqa , and +2.4% f1 score on ner. xlm-r performs particularly well on low-resource languages , improving 15.7% in xnli accuracy for swahili and 11.4% for urdu over previous xlm models. we also present a detailed empirical analysis of the key factors that are required to achieve these gains , including the trade-offs between \( 1 \) positive transfer and capacity dilution and \( 2 \) the performance of high and low resource languages at scale. finally , we show , for the first time , the possibility of multilingual modeling without sacrificing per-language performance; xlm-r is very competitive with strong monolingual models on the glue and xnli benchmarks. we will make our code and models publicly available.
