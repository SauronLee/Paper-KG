Self-Attention Guided Copy Mechanism for Abstractive Summarization | Song Xu | copy module has been widely equipped in the recent abstractive summarization models , which facilitates the decoder to extract words from the source into the summary. generally , the encoder-decoder attention is served as the copy distribution , while how to guarantee that important words in the source are copied remains a challenge. in this work , we propose a transformer-based model to enhance the copy mechanism. specifically , we identify the importance of each source word based on the degree centrality with a directed graph built by the self-attention layer in the transformer. we use the centrality of each source word to guide the copy process explicitly. experimental results show that the self-attention graph provides useful guidance for the copy distribution. our proposed models significantly outperform the baseline methods on the cnn/daily mail dataset and the gigaword dataset.
