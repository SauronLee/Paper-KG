Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics | Nitika Mathur | automatic metrics are fundamental for the development and evaluation of machine translation systems. judging whether , and to what extent , automatic metrics concur with the gold standard of human evaluation is not a straightforward problem. we show that current methods for judging metrics are highly sensitive to the translations used for assessment , particularly the presence of outliers , which often leads to falsely confident conclusions about a metricâ€™s efficacy. finally , we turn to pairwise system ranking , developing a method for thresholding performance improvement under an automatic metric against human judgements , which allows quantification of type i versus type ii errors incurred , i.e. , insignificant human differences in system quality that are accepted , and significant human differences that are rejected. together , these findings suggest improvements to the protocols for metric evaluation and system performance evaluation in machine translation.
