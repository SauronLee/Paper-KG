Fast and Accurate Deep Bidirectional Language Representations for Unsupervised Learning | Joongbo Shin | even though bert has achieved successful performance improvements in various supervised learning tasks , bert is still limited by repetitive inferences on unsupervised tasks for the computation of contextual language representations. to resolve this limitation , we propose a novel deep bidirectional language model called a transformer-based text autoencoder \( t-ta \) . the t-ta computes contextual language representations without repetition and displays the benefits of a deep bidirectional architecture , such as that of bert. in computation time experiments in a cpu environment , the proposed t-ta performs over six times faster than the bert-like model on a reranking task and twelve times faster on a semantic similarity task. furthermore , the t-ta shows competitive or even better accuracies than those of bert on the above tasks. code is available at https://github.com/joongbo/tta.
