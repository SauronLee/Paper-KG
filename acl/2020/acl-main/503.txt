The Cascade Transformer: an Application for Efficient Answer Sentence Selection | Luca Soldaini | large transformer-based language models have been shown to be very effective in many classification tasks. however , their computational complexity prevents their use in applications requiring the classification of a large set of candidates. while previous works have investigated approaches to reduce model size , relatively little attention has been paid to techniques to improve batch throughput during inference. in this paper , we introduce the cascade transformer , a simple yet effective technique to adapt transformer-based models into a cascade of rankers. each ranker is used to prune a subset of candidates in a batch , thus dramatically increasing throughput at inference time. partial encodings from the transformer model are shared among rerankers , providing further speed-up. when compared to a state-of-the-art transformer model , our approach reduces computation by 37% with almost no impact on accuracy , as measured on two english question answering datasets.
