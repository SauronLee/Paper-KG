Clinical Reading Comprehension: A Thorough Analysis of the emrQA Dataset | Xiang Yue | machine reading comprehension has made great progress in recent years owing to large-scale annotated datasets. in the clinical domain , however , creating such datasets is quite difficult due to the domain expertise required for annotation. recently , pampari et al. \( emnlp’18 \) tackled this issue by using expert-annotated question templates and existing i2b2 annotations to create emrqa , the first large-scale dataset for question answering \( qa \) based on clinical notes. in this paper , we provide an in-depth analysis of this dataset and the clinical reading comprehension \( clinirc \) task. from our qualitative analysis , we find that \( i \) emrqa answers are often incomplete , and \( ii \) emrqa questions are often answerable without using domain knowledge. from our quantitative experiments , surprising results include that \( iii \) using a small sampled subset \( 5%-20% \) , we can obtain roughly equal performance compared to the model trained on the entire dataset , \( iv \) this performance is close to human expert’s performance , and \( v \) bert models do not beat the best performing base model. following our analysis of the emrqa , we further explore two desired aspects of clinirc systems: the ability to utilize clinical domain knowledge and to generalize to unseen questions and contexts. we argue that both should be considered when creating future datasets.
