A Tale of Two Perplexities: Sensitivity of Neural Language Models to Lexical Retrieval Deficits in Dementia of the Alzheimerâ€™s Type | Trevor Cohen | in recent years there has been a burgeoning interest in the use of computational methods to distinguish between elicited speech samples produced by patients with dementia , and those from healthy controls. the difference between perplexity estimates from two neural language models \( lms \) - one trained on transcripts of speech produced by healthy participants and one trained on those with dementia - as a single feature for diagnostic classification of unseen transcripts has been shown to produce state-of-the-art performance. however , little is known about why this approach is effective , and on account of the lack of case/control matching in the most widely-used evaluation set of transcripts \( dementiabank \) , it is unclear if these approaches are truly diagnostic , or are sensitive to other variables. in this paper , we interrogate neural lms trained on participants with and without dementia by using synthetic narratives previously developed to simulate progressive semantic dementia by manipulating lexical frequency. we find that perplexity of neural lms is strongly and differentially associated with lexical frequency , and that using a mixture model resulting from interpolating control and dementia lms improves upon the current state-of-the-art for models trained on transcript text exclusively.
