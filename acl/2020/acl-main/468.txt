What Does BERT with Vision Look At? | Liunian Harold Li | pre-trained visually grounded language models such as vilbert , lxmert , and uniter have achieved significant performance improvement on vision-and-language tasks but what they learn during pre-training remains unclear. in this work , we demonstrate that certain attention heads of a visually grounded language model actively ground elements of language to image regions. specifically , some heads can map entities to image regions , performing the task known as entity grounding. some heads can even detect the syntactic relations between non-entity words and image regions , tracking , for example , associations between verbs and regions corresponding to their arguments. we denote this ability as
