A Generative Model for Joint Natural Language Understanding and Generation | Bo-Hsiang Tseng | natural language understanding \( nlu \) and natural language generation \( nlg \) are two fundamental and related tasks in building task-oriented dialogue systems with opposite objectives: nlu tackles the transformation from natural language to formal representations , whereas nlg does the reverse. a key to success in either task is parallel training data which is expensive to obtain at a large scale. in this work , we propose a generative model which couples nlu and nlg through a shared latent variable. this approach allows us to explore both spaces of natural language and formal representations , and facilitates information sharing through the latent space to eventually benefit nlu and nlg. our model achieves state-of-the-art performance on two dialogue datasets with both flat and tree-structured formal representations. we also show that the model can be trained in a semi-supervised fashion by utilising unlabelled data to boost its performance.
