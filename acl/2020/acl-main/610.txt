FLAT: Chinese NER Using Flat-Lattice Transformer | Xiaonan Li | recently , the character-word lattice structure has been proved to be effective for chinese named entity recognition \( ner \) by incorporating the word information. however , since the lattice structure is complex and dynamic , the lattice-based models are hard to fully utilize the parallel computation of gpus and usually have a low inference speed. in this paper , we propose flat: flat-lattice transformer for chinese ner , which converts the lattice structure into a flat structure consisting of spans. each span corresponds to a character or latent word and its position in the original lattice. with the power of transformer and well-designed position encoding , flat can fully leverage the lattice information and has an excellent parallel ability. experiments on four datasets show flat outperforms other lexicon-based models in performance and efficiency.
