Span-based Localizing Network for Natural Language Video Localization | Hao Zhang | given an untrimmed video and a text query , natural language video localization \( nlvl \) is to locate a matching span from the video that semantically corresponds to the query. existing solutions formulate nlvl either as a ranking task and apply multimodal matching architecture , or as a regression task to directly regress the target video span. in this work , we address nlvl task with a span-based qa approach by treating the input video as text passage. we propose a video span localizing network \( vslnet \) , on top of the standard span-based qa framework , to address nlvl. the proposed vslnet tackles the differences between nlvl and span-based qa through a simple and yet effective query-guided highlighting \( qgh \) strategy. the qgh guides vslnet to search for matching video span within a highlighted region. through extensive experiments on three benchmark datasets , we show that the proposed vslnet outperforms the state-of-the-art methods; and adopting span-based qa framework is a promising direction to solve nlvl.
