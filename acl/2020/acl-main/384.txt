Quantifying Attention Flow in Transformers | Samira Abnar | in the transformer model , “self-attention” combines information from attended embeddings into the representation of the focal embedding in the next layer. thus , across layers of the transformer , information originating from different tokens gets increasingly mixed. this makes attention weights unreliable as explanations probes. in this paper , we consider the problem of quantifying this flow of information through self-attention. we propose two methods for approximating the attention to input tokens given attention weights , attention rollout and attention flow , as post hoc methods when we use attention weights as the relative relevance of the input tokens. we show that these methods give complementary views on the flow of information , and compared to raw attention , both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.
