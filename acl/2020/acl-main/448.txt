A Transformer-based Approach for Source Code Summarization | Wasi Ahmad | generating a readable summary that describes the functionality of a program is known as source code summarization. in this task , learning code representation by modeling the pairwise relationship between code tokens to capture their long-range dependencies is crucial. to learn code representation for summarization , we explore the transformer model that uses a self-attention mechanism and has shown to be effective in capturing long-range dependencies. in this work , we show that despite the approach is simple , it outperforms the state-of-the-art techniques by a significant margin. we perform extensive analysis and ablation studies that reveal several important findings , e.g. , the absolute encoding of source code tokensâ€™ position hinders , while relative encoding significantly improves the summarization performance. we have made our code publicly available to facilitate future research.
