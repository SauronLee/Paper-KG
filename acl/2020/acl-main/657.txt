A Call for More Rigor in Unsupervised Cross-lingual Learning | Mikel Artetxe | we review motivations , definition , approaches , and methodology for unsupervised cross-lingual learning and call for a more rigorous position in each of them. an existing rationale for such research is based on the lack of parallel data for many of the worldâ€™s languages. however , we argue that a scenario without any parallel data and abundant monolingual data is unrealistic in practice. we also discuss different training signals that have been used in previous work , which depart from the pure unsupervised setting. we then describe common methodological issues in tuning and evaluation of unsupervised cross-lingual models and present best practices. finally , we provide a unified outlook for different types of research in this area \( i.e. , cross-lingual word embeddings , deep multilingual pretraining , and unsupervised machine translation \) and argue for comparable evaluation of these models.
