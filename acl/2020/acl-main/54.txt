Evaluating Dialogue Generation Systems via Response Selection | Shiki Sato | existing automatic evaluation metrics for open-domain dialogue response generation systems correlate poorly with human evaluation. we focus on evaluating response generation systems via response selection. to evaluate systems properly via response selection , we propose a method to construct response selection test sets with well-chosen false candidates. specifically , we propose to construct test sets filtering out some types of false candidates: \( i \) those unrelated to the ground-truth response and \( ii \) those acceptable as appropriate responses. through experiments , we demonstrate that evaluating systems via response selection with the test set developed by our method correlates more strongly with human evaluation , compared with widely used automatic evaluation metrics such as bleu.
