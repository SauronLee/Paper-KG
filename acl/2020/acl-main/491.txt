Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions | Xiaochuang Han | modern deep learning models for nlp are notoriously opaque. this has motivated the development of methods for interpreting such models , e.g. , via gradient-based saliency maps or the visualization of attention weights. such approaches aim to provide explanations for a particular model prediction by highlighting important words in the corresponding input text. while this might be useful for tasks where decisions are explicitly influenced by individual tokens in the input , we suspect that such highlighting is not suitable for tasks where model decisions should be driven by more complex reasoning. in this work , we investigate the use of influence functions for nlp , providing an alternative approach to interpreting neural text classifiers. influence functions explain the decisions of a model by identifying influential training examples. despite the promise of this approach , influence functions have not yet been extensively evaluated in the context of nlp , a gap addressed by this work. we conduct a comparison between influence functions and common word-saliency methods on representative tasks. as suspected , we find that influence functions are particularly useful for natural language inference , a task in which ‘saliency maps’ may not have clear interpretation. furthermore , we develop a new quantitative measure based on influence functions that can reveal artifacts in training data.
