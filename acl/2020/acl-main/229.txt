Cross-media Structured Common Space for Multimedia Event Extraction | Manling Li | we introduce a new task , multimedia event extraction , which aims to extract events and their arguments from multimedia documents. we develop the first benchmark and collect a dataset of 245 multimedia news articles with extensively annotated events and arguments. we propose a novel method , weakly aligned structured embedding \( wase \) , that encodes structured representations of semantic information from textual and visual data into a common embedding space. the structures are aligned across modalities by employing a weakly supervised training strategy , which enables exploiting available resources without explicit cross-media annotation. compared to uni-modal state-of-the-art methods , our approach achieves 4.0% and 9.8% absolute f-score gains on text event argument role labeling and visual event extraction. compared to state-of-the-art multimedia unstructured representations , we achieve 8.3% and 5.0% absolute f-score gains on multimedia event extraction and argument role labeling , respectively. by utilizing images , we extract 21.4% more event mentions than traditional text-only methods.
