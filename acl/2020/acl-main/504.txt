Transformers to Learn Hierarchical Contexts in Multiparty Dialogue for Span-based Question Answering | Changmao Li | we introduce a novel approach to transformers that learns hierarchical representations in multiparty dialogue. first , three language modeling tasks are used to pre-train the transformers , token- and utterance-level language modeling and utterance order prediction , that learn both token and utterance embeddings for better understanding in dialogue contexts. then , multi-task learning between the utterance prediction and the token span prediction is applied to fine-tune for span-based question answering \( qa \) . our approach is evaluated on the friendsqa dataset and shows improvements of 3.8% and 1.4% over the two state-of-the-art transformer models , bert and roberta , respectively.
