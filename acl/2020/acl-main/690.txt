Translationese as a Language in “Multilingual” NMT | Parker Riley | machine translation has an undesirable propensity to produce “translationese” artifacts , which can lead to higher bleu scores while being liked less by human raters. motivated by this , we model translationese and original \( i.e. natural \) text as separate languages in a multilingual model , and pose the question: can we perform zero-shot translation between original source text and original target text \? there is no data with original source and original target , so we train a sentence-level classifier to distinguish translationese from original target text , and use this classifier to tag the training data for an nmt model. using this technique we bias the model to produce more natural outputs at test time , yielding gains in human evaluation scores on both accuracy and fluency. additionally , we demonstrate that it is possible to bias the model to produce translationese and game the bleu score , increasing it while decreasing human-rated quality. we analyze these outputs using metrics measuring the degree of translationese , and present an analysis of the volatility of heuristic-based train-data tagging.
