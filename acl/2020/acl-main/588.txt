Differentiable Window for Dynamic Local Attention | Thanh-Tung Nguyen | we propose differentiable window , a new neural module and general purpose component for dynamic window selection. while universally applicable , we demonstrate a compelling use case of utilizing differentiable window to improve standard attention modules by enabling more focused attentions over the input regions. we propose two variants of differentiable window , and integrate them within the transformer architecture in two novel ways. we evaluate our proposed approach on a myriad of nlp tasks , including machine translation , sentiment analysis , subject-verb agreement and language modeling. our experimental results demonstrate consistent and sizable improvements across all tasks.
