What is Learned in Visually Grounded Neural Syntax Acquisition | Noriyuki Kojima | visual features are a promising signal for learning bootstrap textual models. however , blackbox learning models make it difficult to isolate the specific contribution of visual components. in this analysis , we consider the case study of the visually grounded neural syntax learner \( shi et al. , 2019 \) , a recent approach for learning syntax from a visual training signal. by constructing simplified versions of the model , we isolate the core factors that yield the model’s strong performance. contrary to what the model might be capable of learning , we find significantly less expressive versions produce similar predictions and perform just as well , or even better. we also find that a simple lexical signal of noun concreteness plays the main role in the model’s predictions as opposed to more complex syntactic reasoning.
