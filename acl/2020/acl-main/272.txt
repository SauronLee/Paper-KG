A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation | Yongjing Yin | multi-modal neural machine translation \( nmt \) aims to translate source sentences into a target language paired with images. however , dominant multi-modal nmt models do not fully exploit fine-grained semantic correspondences between semantic units of different modalities , which have potential to refine multi-modal representation learning. to deal with this issue , in this paper , we propose a novel graph-based multi-modal fusion encoder for nmt. specifically , we first represent the input sentence and image using a unified multi-modal graph , which captures various semantic relationships between multi-modal semantic units \( words and visual objects \) . we then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions to learn node representations. finally , these representations provide an attention-based context vector for the decoder. we evaluate our proposed encoder on the multi30k datasets. experimental results and in-depth analysis show the superiority of our multi-modal nmt model.
