End-to-End Neural Word Alignment Outperforms GIZA++ | Thomas Zenkel | word alignment was once a core unsupervised learning task in natural language processing because of its essential role in training statistical machine translation \( mt \) models. although unnecessary for training neural mt models , word alignment still plays an important role in interactive applications of neural machine translation , such as annotation transfer and lexicon injection. while statistical mt methods have been replaced by neural approaches with superior performance , the twenty-year-old giza++ toolkit remains a key component of state-of-the-art word alignment systems. prior work on neural word alignment has only been able to outperform giza++ by using its output during training. we present the first end-to-end neural word alignment method that consistently outperforms giza++ on three data sets. our approach repurposes a transformer model trained for supervised translation to also serve as an unsupervised word alignment model in a manner that is tightly integrated and does not affect translation quality.
