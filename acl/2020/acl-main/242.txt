Posterior Control of Blackbox Generation | Xiang Lisa Li | text generation often requires high-precision output that obeys task-specific rules. this fine-grained control is difficult to enforce with off-the-shelf deep learning models. in this work , we consider augmenting neural generation models with discrete control states learned through a structured latent-variable approach. under this formulation , task-specific knowledge can be encoded through a range of rich , posterior constraints that are effectively trained into the model. this approach allows users to ground internal model decisions based on prior knowledge , without sacrificing the representational power of neural generative models. experiments consider applications of this approach for text generation. we find that this method improves over standard benchmarks , while also providing fine-grained control.
