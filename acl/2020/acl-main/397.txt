TaPas: Weakly Supervised Table Parsing via Pre-training | Jonathan Herzig | answering natural language questions over tables is usually seen as a semantic parsing task. to alleviate the collection cost of full logical forms , one popular approach focuses on weak supervision consisting of denotations instead of logical forms. however , training semantic parsers from weak supervision poses difficulties , and in addition , the generated logical forms are only used as an intermediate step prior to retrieving the denotation. in this paper , we present tapas , an approach to question answering over tables without generating logical forms. tapas trains from weak supervision , and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection. tapas extends bertâ€™s architecture to encode tables as input , initializes from an effective joint pre-training of text segments and tables crawled from wikipedia , and is trained end-to-end. we experiment with three different semantic parsing datasets , and find that tapas outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on sqa from 55.1 to 67.2 and performing on par with the state-of-the-art on wikisql and wikitq , but with a simpler model architecture. we additionally find that transfer learning , which is trivial in our setting , from wikisql to wikitq , yields 48.7 accuracy , 4.2 points above the state-of-the-art.
