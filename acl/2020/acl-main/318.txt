A Reinforced Generation of Adversarial Examples for Neural Machine Translation | Wei Zou | neural machine translation systems tend to fail on less decent inputs despite its significant efficacy , which may significantly harm the credibility of these systemsâ€”fathoming how and when neural-based systems fail in such cases is critical for industrial maintenance. instead of collecting and analyzing bad cases using limited handcrafted error features , here we investigate this issue by generating adversarial examples via a new paradigm based on reinforcement learning. our paradigm could expose pitfalls for a given performance metric , e.g. , bleu , and could target any given neural machine translation architecture. we conduct experiments of adversarial attacks on two mainstream neural machine translation architectures , rnn-search , and transformer. the results show that our method efficiently produces stable attacks with meaning-preserving adversarial examples. we also present a qualitative and quantitative analysis for the preference pattern of the attack , demonstrating its capability of pitfall exposure.
