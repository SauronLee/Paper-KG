Finding Universal Grammatical Relations in Multilingual BERT | Ethan A. Chi | recent work has found evidence that multilingual bert \( mbert \) , a transformer-based multilingual masked language model , is capable of zero-shot cross-lingual transfer , suggesting that some aspects of its representations are shared cross-lingually. to better understand this overlap , we extend recent work on finding syntactic trees in neural networksâ€™ internal representations to the multilingual setting. we show that subspaces of mbert representations recover syntactic tree distances in languages other than english , and that these subspaces are approximately shared across languages. motivated by these results , we present an unsupervised analysis method that provides evidence mbert learns representations of syntactic dependency labels , in the form of clusters which largely agree with the universal dependencies taxonomy. this evidence suggests that even without explicit supervision , multilingual masked language models learn certain linguistic universals.
