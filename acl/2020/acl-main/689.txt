Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem | Danielle Saunders | training data for nlp tasks often exhibits gender bias in that fewer sentences refer to women than to men. in neural machine translation \( nmt \) gender bias has been shown to reduce translation quality , particularly when the target language has grammatical gender. the recent winomt challenge set allows us to measure this effect directly \( stanovsky et al , 2019 \) ideally we would reduce system bias by simply debiasing all data prior to training , but achieving this effectively is itself a challenge. rather than attempt to create a ‘balanced’ dataset , we use transfer learning on a small set of trusted , gender-balanced examples. this approach gives strong and consistent improvements in gender debiasing with much less computational cost than training from scratch. a known pitfall of transfer learning on new domains is ‘catastrophic forgetting’ , which we address at adaptation and inference time. during adaptation we show that elastic weight consolidation allows a performance trade-off between general translation quality and bias reduction. at inference time we propose a lattice-rescoring scheme which outperforms all systems evaluated in stanovsky et al , 2019 on winomt with no degradation of general test set bleu. we demonstrate our approach translating from english into three languages with varied linguistic properties and data availability.
