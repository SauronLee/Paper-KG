STARC: Structured Annotations for Reading Comprehension | Yevgeni Berzak | we present starc \( structured annotations for reading comprehension \) , a new annotation framework for assessing reading comprehension with multiple choice questions. our framework introduces a principled structure for the answer choices and ties them to textual span annotations. the framework is implemented in onestopqa , a new high-quality dataset for evaluation and analysis of reading comprehension in english. we use this dataset to demonstrate that starc can be leveraged for a key new application for the development of sat-like reading comprehension materials: automatic annotation quality probing via span ablation experiments. we further show that it enables in-depth analyses and comparisons between machine and human reading comprehension behavior , including error distributions and guessing ability. our experiments also reveal that the standard multiple choice dataset in nlp , race , is limited in its ability to measure reading comprehension. 47% of its questions can be guessed by machines without accessing the passage , and 18% are unanimously judged by humans as not having a unique correct answer. onestopqa provides an alternative test set for reading comprehension which alleviates these shortcomings and has a substantially higher human ceiling performance.
