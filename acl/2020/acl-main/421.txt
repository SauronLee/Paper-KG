Similarity Analysis of Contextual Word Representation Models | John Wu | this paper investigates contextual word representation models from the lens of similarity analysis. given a collection of trained models , we measure the similarity of their internal representations and attention. critically , these models come from vastly different architectures. we use existing and novel similarity measures that aim to gauge the level of localization of information in the deep models , and facilitate the investigation of which design factors affect model similarity , without requiring any external linguistic annotation. the analysis reveals that models within the same family are more similar to one another , as may be expected. surprisingly , different architectures have rather similar representations , but different individual neurons. we also observed differences in information localization in lower and higher layers and found that higher layers are more affected by fine-tuning on downstream tasks.
