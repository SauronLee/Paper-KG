On the Encoder-Decoder Incompatibility in Variational Text Modeling and Beyond | Chen Wu | variational autoencoders \( vaes \) combine latent variables with amortized variational inference , whose optimization usually converges into a trivial local optimum termed posterior collapse , especially in text modeling. by tracking the optimization dynamics , we observe the encoder-decoder incompatibility that leads to poor parameterizations of the data manifold. we argue that the trivial local optimum may be avoided by improving the encoder and decoder parameterizations since the posterior network is part of a transition map between them. to this end , we propose coupled-vae , which couples a vae model with a deterministic autoencoder with the same structure and improves the encoder and decoder parameterizations via encoder weight sharing and decoder signal matching. we apply the proposed coupled-vae approach to various vae models with different regularization , posterior family , decoder structure , and optimization strategy. experiments on benchmark datasets \( i.e. , ptb , yelp , and yahoo \) show consistently improved results in terms of probability estimation and richness of the latent space. we also generalize our method to conditional language modeling and propose coupled-cvae , which largely improves the diversity of dialogue generation on the switchboard dataset.
