Highway Transformer: Self-Gating Enhanced Self-Attentive Networks | Yekun Chai | self-attention mechanisms have made striking state-of-the-art \( sota \) progress in various sequence learning tasks , standing on the multi-headed dot product attention by attending to all the global contexts at different locations. through a pseudo information highway , we introduce a gated component self-dependency units \( sdu \) that incorporates lstm-styled gating units to replenish internal semantic importance within the multi-dimensional latent space of individual representations. the subsidiary content-based sdu gates allow for the information flow of modulated latent embeddings through skipped connections , leading to a clear margin of convergence speed with gradient descent algorithms. we may unveil the role of gating mechanism to aid in the context-based transformer modules , with hypothesizing that sdu gates , especially on shallow layers , could push it faster to step towards suboptimal points during the optimization process.
