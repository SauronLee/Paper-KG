How Does Selective Mechanism Improve Self-Attention Networks? | Xinwei Geng | self-attention networks \( sans \) with selective mechanism has produced substantial improvements in various nlp tasks by concentrating on a subset of input words. however , the underlying reasons for their strong performance have not been well explained. in this paper , we bridge the gap by assessing the strengths of selective sans \( ssans \) , which are implemented with a flexible and universal gumbel-softmax. experimental results on several representative nlp tasks , including natural language inference , semantic role labelling , and machine translation , show that ssans consistently outperform the standard sans. through well-designed probing experiments , we empirically validate that the improvement of ssans can be attributed in part to mitigating two commonly-cited weaknesses of sans: word order encoding and structure modeling. specifically , the selective mechanism improves sans by paying more attention to content words that contribute to the meaning of the sentence.
