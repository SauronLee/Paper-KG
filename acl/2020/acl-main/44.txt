Dice Loss for Data-imbalanced NLP Tasks | Xiaoya Li | many nlp tasks such as tagging and machine reading comprehension are faced with the severe data imbalance issue: negative examples significantly outnumber positive examples , and the huge number of easy-negative examples overwhelms the training. the most commonly used cross entropy \( ce \) criteria is actually an accuracy-oriented objective , and thus creates a discrepancy between training and test: at training time , each training instance contributes equally to the objective function , while at test time f1 score concerns more about positive examples. in this paper , we propose to use dice loss in replacement of the standard cross-entropy objective for data-imbalanced nlp tasks. dice loss is based on the s√∏rensen--dice coefficient or tversky index , which attaches similar importance to false positives and false negatives , and is more immune to the data-imbalance issue. to further alleviate the dominating influence from easy-negative examples in training , we propose to associate training examples with dynamically adjusted weights to deemphasize easy-negative examples. theoretical analysis shows that this strategy narrows down the gap between the f1 score in evaluation and the dice loss in training. with the proposed training objective , we observe significant performance boost on a wide range of data imbalanced nlp tasks. notably , we are able to achieve sota results on ctb5 , ctb6 and ud1.4 for the part of speech tagging task; sota results on conll03 , ontonotes5.0 , msra and ontonotes4.0 for the named entity recognition task; along with competitive results on the tasks of machine reading comprehension and paraphrase identification.
