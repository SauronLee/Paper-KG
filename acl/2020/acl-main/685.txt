HAT: Hardware-Aware Transformers for Efficient Natural Language Processing | Hanrui Wang | transformers are ubiquitous in natural language processing \( nlp \) tasks , but they are difficult to be deployed on hardware due to the intensive computation. to enable low-latency inference on resource-constrained hardware platforms , we propose to design hardware-aware transformers \( hat \) with neural architecture search. we first construct a large design space with arbitrary encoder-decoder attention and heterogeneous layers. then we train a supertransformer that covers all candidates in the design space , and efficiently produces many subtransformers with weight sharing. finally , we perform an evolutionary search with a hardware latency constraint to find a specialized subtransformer dedicated to run fast on the target hardware. extensive experiments on four machine translation tasks demonstrate that hat can discover efficient models for different hardware \( cpu , gpu , iot device \) . when running wmt’14 translation task on raspberry pi-4 , hat can achieve 3× speedup , 3.7× smaller size over baseline transformer; 2.7× speedup , 3.6× smaller size over evolved transformer with 12 , 041× less search cost and no performance loss. hat is open-sourced at https://github.com/mit-han-lab/hardware-aware-transformers.
