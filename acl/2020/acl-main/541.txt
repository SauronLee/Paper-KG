Curriculum Learning for Natural Language Understanding | Benfeng Xu | with the great success of pre-trained language models , the pretrain-finetune paradigm now becomes the undoubtedly dominant solution for natural language understanding \( nlu \) tasks. at the fine-tune stage , target task data is usually introduced in a completely random order and treated equally. however , examples in nlu tasks can vary greatly in difficulty , and similar to human learning procedure , language models can benefit from an easy-to-difficult curriculum. based on this idea , we propose our curriculum learning approach. by reviewing the trainset in a crossed way , we are able to distinguish easy examples from difficult ones , and arrange a curriculum for language models. without any manual model architecture design or use of external data , our curriculum learning approach obtains significant and universal performance improvements on a wide range of nlu tasks.
