SenseBERT: Driving Some Sense into BERT | Yoav Levine | the ability to learn from large unlabeled corpora has allowed neural language models to advance the frontier in natural language understanding. however , existing self-supervision techniques operate at the word form level , which serves as a surrogate for the underlying semantic content. this paper proposes a method to employ weak-supervision directly at the word sense level. our model , named sensebert , is pre-trained to predict not only the masked words but also their wordnet supersenses. accordingly , we attain a lexical-semantic level language model , without the use of human annotation. sensebert achieves significantly improved lexical understanding , as we demonstrate by experimenting on semeval word sense disambiguation , and by attaining a state of the art result on the ‘word in context’ task.
