Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation | Chao Zhao | generating sequential natural language descriptions from graph-structured data \( e.g. , knowledge graph \) is challenging , partly because of the structural differences between the input graph and the output text. hence , popular sequence-to-sequence models , which require serialized input , are not a natural fit for this task. graph neural networks , on the other hand , can better encode the input graph but broaden the structural gap between the encoder and decoder , making faithful generation difficult. to narrow this gap , we propose dualenc , a dual encoding model that can not only incorporate the graph structure , but can also cater to the linear structure of the output text. empirical comparisons with strong single-encoder baselines demonstrate that dual encoding can significantly improve the quality of the generated text.
