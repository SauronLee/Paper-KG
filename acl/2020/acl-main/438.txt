Pretraining with Contrastive Sentence Objectives Improves Discourse Performance of Language Models | Dan Iter | recent models for unsupervised representation learning of text have employed a number of techniques to improve contextual word representations but have put little focus on discourse-level representations. we propose conpono , an inter-sentence objective for pretraining language models that models discourse coherence and the distance between sentences. given an anchor sentence , our model is trained to predict the text k sentences away using a sampled-softmax objective where the candidates consist of neighboring sentences and sentences randomly sampled from the corpus. on the discourse representation benchmark discoeval , our model improves over the previous state-of-the-art by up to 13% and on average 4% absolute across 7 tasks. our model is the same size as bert-base , but outperforms the much larger bert-large model and other more recent approaches that incorporate discourse. we also show that conpono yields gains of 2%-6% absolute even for tasks that do not explicitly evaluate discourse: textual entailment \( rte \) , common sense reasoning \( copa \) and reading comprehension \( record \) .
