Estimating Mutual Information Between Dense Word Embeddings | Vitalii Zhelezniak | word embedding-based similarity measures are currently among the top-performing methods on unsupervised semantic textual similarity \( sts \) tasks. recent work has increasingly adopted a statistical view on these embeddings , with some of the top approaches being essentially various correlations \( which include the famous cosine similarity \) . another excellent candidate for a similarity measure is mutual information \( mi \) , which can capture arbitrary dependencies between the variables and has a simple and intuitive expression. unfortunately , its use in the context of dense word embeddings has so far been avoided due to difficulties with estimating mi for continuous data. in this work we go through a vast literature on estimating mi in such cases and single out the most promising methods , yielding a simple and elegant similarity measure for word embeddings. we show that mutual information is a viable alternative to correlations , gives an excellent signal that correlates well with human judgements of similarity and rivals existing state-of-the-art unsupervised methods.
