Why Overfitting Isnâ€™t Always Bad: Retrofitting Cross-Lingual Word Embeddings to Dictionaries | Mozhi Zhang | cross-lingual word embeddings \( clwe \) are often evaluated on bilingual lexicon induction \( bli \) . recent clwe methods use linear projections , which underfit the training dictionary , to generalize on bli. however , underfitting can hinder generalization to other downstream tasks that rely on words from the training dictionary. we address this limitation by retrofitting clwe to the training dictionary , which pulls training translation pairs closer in the embedding space and overfits the training dictionary. this simple post-processing step often improves accuracy on two downstream tasks , despite lowering bli test accuracy. we also retrofit to both the training dictionary and a synthetic dictionary induced from clwe , which sometimes generalizes even better on downstream tasks. our results confirm the importance of fully exploiting training dictionary in downstream tasks and explains why bli is a flawed clwe evaluation.
