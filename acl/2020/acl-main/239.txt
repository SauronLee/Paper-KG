Masked Language Model Scoring | Julian Salazar | pretrained masked language models \( mlms \) require finetuning for most nlp tasks. instead , we evaluate mlms out of the box via their pseudo-log-likelihood scores \( plls \) , which are computed by masking tokens one by one. we show that plls outperform scores from autoregressive language models like gpt-2 in a variety of tasks. by rescoring asr and nmt hypotheses , roberta reduces an end-to-end librispeech model’s wer by 30% relative and adds up to +1.7 bleu on state-of-the-art baselines for low-resource translation pairs , with further gains from domain adaptation. we attribute this success to pll’s unsupervised expression of linguistic acceptability without a left-to-right bias , greatly improving on scores from gpt-2 \( +10 points on island effects , npi licensing in blimp \) . one can finetune mlms to give scores without masking , enabling computation in a single inference pass. in all , plls and their associated pseudo-perplexities \( pppls \) enable plug-and-play use of the growing number of pretrained mlms; e.g. , we use a single cross-lingual model to rescore translations in multiple languages. we release our library for language model scoring at https://github.com/awslabs/mlm-scoring.
