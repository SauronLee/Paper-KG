Parallel Corpus Filtering via Pre-trained Language Models | Boliang Zhang | web-crawled data provides a good source of parallel corpora for training machine translation models. it is automatically obtained , but extremely noisy , and recent work shows that neural machine translation systems are more sensitive to noise than traditional statistical machine translation methods. in this paper , we propose a novel approach to filter out noisy sentence pairs from web-crawled corpora via pre-trained language models. we measure sentence parallelism by leveraging the multilingual capability of bert and use the generative pre-training \( gpt \) language model as a domain filter to balance data domains. we evaluate the proposed method on the wmt 2018 parallel corpus filtering shared task , and on our own web-crawled japanese-chinese parallel corpus. our method significantly outperforms baselines and achieves a new state-of-the-art. in an unsupervised setting , our method achieves comparable performance to the top-1 supervised method. we also evaluate on a web-crawled japanese-chinese parallel corpus that we make publicly available.
