Are we Estimating or Guesstimating Translation Quality? | Shuo Sun | recent advances in pre-trained multilingual language models lead to state-of-the-art results on the task of quality estimation \( qe \) for machine translation. a carefully engineered ensemble of such models won the qe shared task at wmt19. our in-depth analysis , however , shows that the success of using pre-trained language models for qe is over-estimated due to three issues we observed in current qe datasets: \( i \) the distributions of quality scores are imbalanced and skewed towards good quality scores; \( iii \) qe models can perform well on these datasets while looking at only source or translated sentences; \( iii \) they contain statistical artifacts that correlate well with human-annotated qe labels. our findings suggest that although qe models might capture fluency of translated sentences and complexity of source sentences , they cannot model adequacy of translations effectively.
