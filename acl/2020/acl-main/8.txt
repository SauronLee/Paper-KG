PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable | Siqi Bao | pre-training models have been proved effective for a wide range of natural language processing tasks. inspired by this , we propose a novel dialogue generation pre-training framework to support various kinds of conversations , including chit-chat , knowledge grounded dialogues , and conversational question answering. in this framework , we adopt flexible attention mechanisms to fully leverage the bi-directional context and the uni-directional characteristic of language generation. we also introduce discrete latent variables to tackle the inherent one-to-many mapping problem in response generation. two reciprocal tasks of response generation and latent act recognition are designed and carried out simultaneously within a shared network. comprehensive experiments on three publicly available datasets verify the effectiveness and superiority of the proposed framework.
