A Recipe for Creating Multimodal Aligned Datasets for Sequential Tasks | Angela Lin | many high-level procedural tasks can be decomposed into sequences of instructions that vary in their order and choice of tools. in the cooking domain , the web offers many , partially-overlapping , text and video recipes \( i.e. procedures \) that describe how to make the same dish \( i.e. high-level task \) . aligning instructions for the same dish across different sources can yield descriptive visual explanations that are far richer semantically than conventional textual instructions , providing commonsense insight into how real-world procedures are structured. learning to align these different instruction sets is challenging because: a \) different recipes vary in their order of instructions and use of ingredients; and b \) video instructions can be noisy and tend to contain far more information than text instructions. to address these challenges , we use an unsupervised alignment algorithm that learns pairwise alignments between instructions of different recipes for the same dish. we then use a graph algorithm to derive a joint alignment between multiple text and multiple video recipes for the same dish. we release the microsoft research multimodal aligned recipe corpus containing ~150k pairwise alignments between recipes across 4262 dishes with rich commonsense information.
