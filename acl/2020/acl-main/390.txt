Encoder-Decoder Models Can Benefit from Pre-trained Masked Language Models in Grammatical Error Correction | Masahiro Kaneko | this paper investigates how to effectively incorporate a pre-trained masked language model \( mlm \) , such as bert , into an encoder-decoder \( encdec \) model for grammatical error correction \( gec \) . the answer to this question is not as straightforward as one might expect because the previous common methods for incorporating a mlm into an encdec model have potential drawbacks when applied to gec. for example , the distribution of the inputs to a gec model can be considerably different \( erroneous , clumsy , etc. \) from that of the corpora used for pre-training mlms; however , this issue is not addressed in the previous methods. our experiments show that our proposed method , where we first fine-tune a mlm with a given gec corpus and then use the output of the fine-tuned mlm as additional features in the gec model , maximizes the benefit of the mlm. the best-performing model achieves state-of-the-art performances on the bea-2019 and conll-2014 benchmarks. our code is publicly available at: https://github.com/kanekomasahiro/bert-gec.
