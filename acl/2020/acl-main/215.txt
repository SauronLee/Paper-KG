Multimodal and Multiresolution Speech Recognition with Transformers | Georgios Paraskevopoulos | this paper presents an audio visual automatic speech recognition \( av-asr \) system using a transformer-based architecture. we particularly focus on the scene context provided by the visual information , to ground the asr. we extract representations for audio features in the encoder layers of the transformer and fuse video features using an additional crossmodal multihead attention layer. additionally , we incorporate a multitask training criterion for multiresolution asr , where we train the model to generate both character and subword level transcriptions. experimental results on the how2 dataset , indicate that multiresolution training can speed up convergence by around 50% and relatively improves word error rate \( wer \) performance by upto 18% over subword prediction models. further , incorporating visual information improves performance with relative gains upto 3.76% over audio only models. our results are comparable to state-of-the-art listen , attend and spell-based architectures.
