Using Context in Neural Machine Translation Training Objectives | Danielle Saunders | we present neural machine translation \( nmt \) training using document-level metrics with batch-level documents. previous sequence-objective approaches to nmt training focus exclusively on sentence-level metrics like sentence bleu which do not correspond to the desired evaluation metric , typically document bleu. meanwhile research into document-level nmt training focuses on data or model architecture rather than training procedure. we find that each of these lines of research has a clear space in it for the other , and propose merging them with a scheme that allows a document-level evaluation metric to be used in the nmt training objective. we first sample pseudo-documents from sentence samples. we then approximate the expected document bleu gradient with monte carlo sampling for use as a cost function in minimum risk training \( mrt \) . this two-level sampling procedure gives nmt performance gains over sequence mrt and maximum-likelihood training. we demonstrate that training is more robust for document-level metrics than with sequence metrics. we further demonstrate improvements on nmt with ter and grammatical error correction \( gec \) using gleu , both metrics used at the document level for evaluations.
