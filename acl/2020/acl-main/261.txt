Is Your Classifier Actually Biased? Measuring Fairness under Uncertainty with Bernstein Bounds | Kawin Ethayarajh | most nlp datasets are not annotated with protected attributes such as gender , making it difficult to measure classification bias using standard measures of fairness \( e.g. , equal opportunity \) . however , manually annotating a large dataset with a protected attribute is slow and expensive. instead of annotating all the examples , can we annotate a subset of them and use that sample to estimate the bias \? while it is possible to do so , the smaller this annotated sample is , the less certain we are that the estimate is close to the true bias. in this work , we propose using bernstein bounds to represent this uncertainty about the bias estimate as a confidence interval. we provide empirical evidence that a 95% confidence interval derived this way consistently bounds the true bias. in quantifying this uncertainty , our method , which we call bernstein-bounded unfairness , helps prevent classifiers from being deemed biased or unbiased when there is insufficient evidence to make either claim. our findings suggest that the datasets currently used to measure specific biases are too small to conclusively identify bias except in the most egregious cases. for example , consider a co-reference resolution system that is 5% more accurate on gender-stereotypical sentences â€“ to claim it is biased with 95% confidence , we need a bias-specific dataset that is 3.8 times larger than winobias , the largest available.
