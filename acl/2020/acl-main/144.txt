Character-Level Translation with Self-attention | Yingqiang Gao | we explore the suitability of self-attention models for character-level neural machine translation. we test the standard transformer model , as well as a novel variant in which the encoder block combines information from nearby characters using convolutions. we perform extensive experiments on wmt and un datasets , testing both bilingual and multilingual translation to english using up to three input languages \( french , spanish , and chinese \) . our transformer variant consistently outperforms the standard transformer at the character-level and converges faster while learning more robust character-level alignments.
