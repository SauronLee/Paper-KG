Neural Topic Modeling with Bidirectional Adversarial Training | Rui Wang | recent years have witnessed a surge of interests of using neural topic models for automatic topic extraction from text , since they avoid the complicated mathematical derivations for model inference as in traditional topic models such as latent dirichlet allocation \( lda \) . however , these models either typically assume improper prior \( e.g. gaussian or logistic normal \) over latent topic space or could not infer topic distribution for a given document. to address these limitations , we propose a neural topic modeling approach , called bidirectional adversarial topic \( bat \) model , which represents the first attempt of applying bidirectional adversarial training for neural topic modeling. the proposed bat builds a two-way projection between the document-topic distribution and the document-word distribution. it uses a generator to capture the semantic patterns from texts and an encoder for topic inference. furthermore , to incorporate word relatedness information , the bidirectional adversarial topic model with gaussian \( gaussian-bat \) is extended from bat. to verify the effectiveness of bat and gaussian-bat , three benchmark corpora are used in our experiments. the experimental results show that bat and gaussian-bat obtain more coherent topics , outperforming several competitive baselines. moreover , when performing text clustering based on the extracted topics , our models outperform all the baselines , with more significant improvements achieved by gaussian-bat where an increase of near 6% is observed in accuracy.
