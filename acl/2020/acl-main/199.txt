To Pretrain or Not to Pretrain: Examining the Benefits of Pretrainng on Resource Rich Tasks | Sinong Wang | pretraining nlp models with variants of masked language model \( mlm \) objectives has recently led to a significant improvements on many tasks. this paper examines the benefits of pretrained models as a function of the number of training samples used in the downstream task. on several text classification tasks , we show that as the number of training examples grow into the millions , the accuracy gap between finetuning bert-based model and training vanilla lstm from scratch narrows to within 1%. our findings indicate that mlm-based models might reach a diminishing return point as the supervised data size increases significantly.
