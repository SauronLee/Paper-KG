Addressing Posterior Collapse with Mutual Information for Improved Variational Neural Machine Translation | Arya D. McCarthy | this paper proposes a simple and effective approach to address the problem of posterior collapse in conditional variational autoencoders \( cvaes \) . it thus improves performance of machine translation models that use noisy or monolingual data , as well as in conventional settings. extending transformer and conditional vaes , our proposed latent variable model measurably prevents posterior collapse by \( 1 \) using a modified evidence lower bound \( elbo \) objective which promotes mutual information between the latent variable and the target , and \( 2 \) guiding the latent variable with an auxiliary bag-of-words prediction task. as a result , the proposed model yields improved translation quality compared to existing variational nmt models on wmt ro↔en and de↔en. with latent variables being effectively utilized , our model demonstrates improved robustness over non-latent transformer in handling uncertainty: exploiting noisy source-side monolingual data \( up to +3.2 bleu \) , and training with weakly aligned web-mined parallel data \( up to +4.7 bleu \) .
