Adversarial and Domain-Aware BERT for Cross-Domain Sentiment Analysis | Chunning Du | cross-domain sentiment classification aims to address the lack of massive amounts of labeled data. it demands to predict sentiment polarity on a target domain utilizing a classifier learned from a source domain. in this paper , we investigate how to efficiently apply the pre-training language model bert on the unsupervised domain adaptation. due to the pre-training task and corpus , bert is task-agnostic , which lacks domain awareness and can not distinguish the characteristic of source and target domain when transferring knowledge. to tackle these problems , we design a post-training procedure , which contains the target domain masked language model task and a novel domain-distinguish pre-training task. the post-training procedure will encourage bert to be domain-aware and distill the domain-specific features in a self-supervised way. based on this , we could then conduct the adversarial training to derive the enhanced domain-invariant features. extensive experiments on amazon dataset show that our model outperforms state-of-the-art methods by a large margin. the ablation study demonstrates that the remarkable improvement is not only from bert but also from our method.
