Learning Architectures from an Extended Search Space for Language Modeling | Yinqiao Li | neural architecture search \( nas \) has advanced significantly in recent years but most nas systems restrict search to learning architectures of a recurrent or convolutional cell. in this paper , we extend the search space of nas. in particular , we present a general approach to learn both intra-cell and inter-cell architectures \( call it ess \) . for a better search result , we design a joint learning method to perform intra-cell and inter-cell nas simultaneously. we implement our model in a differentiable architecture search system. for recurrent neural language modeling , it outperforms a strong baseline significantly on the ptb and wikitext data , with a new state-of-the-art on ptb. moreover , the learned architectures show good transferability to other systems. e.g. , they improve state-of-the-art systems on the conll and wnut named entity recognition \( ner \) tasks and conll chunking task , indicating a promising line of research on large-scale pre-learned architectures.
