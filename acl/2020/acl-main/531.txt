Tagged Back-translation Revisited: Why Does It Really Work? | Benjamin Marie | in this paper , we show that neural machine translation \( nmt \) systems trained on large back-translated data overfit some of the characteristics of machine-translated texts. such nmt systems better translate human-produced translations , i.e. , translationese , but may largely worsen the translation quality of original texts. our analysis reveals that adding a simple tag to back-translations prevents this quality degradation and improves on average the overall translation quality by helping the nmt system to distinguish back-translated data from original parallel data during training. we also show that , in contrast to high-resource configurations , nmt systems trained in low-resource settings are much less vulnerable to overfit back-translations. we conclude that the back-translations in the training data should always be tagged especially when the origin of the text to be translated is unknown.
