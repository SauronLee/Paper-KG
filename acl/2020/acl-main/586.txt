A Mixture of h - 1 Heads is Better than h Heads | Hao Peng | multi-head attentive neural architectures have achieved state-of-the-art results on a variety of natural language processing tasks. evidence has shown that they are overparameterized; attention heads can be pruned without significant performance loss. in this work , we instead “reallocate” them—the model learns to activate different heads on different inputs. drawing connections between multi-head attention and mixture of experts , we propose the mixture of attentive experts model \( mae \) . mae is trained using a block coordinate descent algorithm that alternates between updating \( 1 \) the responsibilities of the experts and \( 2 \) their parameters. experiments on machine translation and language modeling show that mae outperforms strong baselines on both tasks. particularly , on the wmt14 english to german translation dataset , mae improves over “transformer-base” by 0.8 bleu , with a comparable number of parameters. our analysis shows that our model learns to specialize different experts to different inputs.
