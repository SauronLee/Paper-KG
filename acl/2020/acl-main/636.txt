Modeling Long Context for Task-Oriented Dialogue State Generation | Jun Quan | based on the recently proposed transferable dialogue state generator \( trade \) that predicts dialogue states from utterance-concatenated dialogue context , we propose a multi-task learning model with a simple yet effective utterance tagging technique and a bidirectional language model as an auxiliary task for task-oriented dialogue state generation. by enabling the model to learn a better representation of the long dialogue context , our approaches attempt to solve the problem that the performance of the baseline significantly drops when the input dialogue context sequence is long. in our experiments , our proposed model achieves a 7.03% relative improvement over the baseline , establishing a new state-of-the-art joint goal accuracy of 52.04% on the multiwoz 2.0 dataset.
