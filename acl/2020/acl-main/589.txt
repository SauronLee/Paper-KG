Evaluating and Enhancing the Robustness of Neural Network-based Dependency Parsing Models with Adversarial Examples | Xiaoqing Zheng | despite achieving prominent performance on many important tasks , it has been reported that neural networks are vulnerable to adversarial examples. previously studies along this line mainly focused on semantic tasks such as sentiment analysis , question answering and reading comprehension. in this study , we show that adversarial examples also exist in dependency parsing: we propose two approaches to study where and how parsers make mistakes by searching over perturbations to existing texts at sentence and phrase levels , and design algorithms to construct such examples in both of the black-box and white-box settings. our experiments with one of state-of-the-art parsers on the english penn treebank \( ptb \) show that up to 77% of input examples admit adversarial perturbations , and we also show that the robustness of parsing models can be improved by crafting high-quality adversaries and including them in the training stage , while suffering little to no performance drop on the clean input data.
