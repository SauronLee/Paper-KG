Pretrained Transformers Improve Out-of-Distribution Robustness | Dan Hendrycks | although pretrained transformers such as bert achieve high accuracy on in-distribution examples , do they generalize to new distributions \? we systematically measure out-of-distribution \( ood \) generalization for seven nlp datasets by constructing a new robustness benchmark with realistic distribution shifts. we measure the generalization of previous models including bag-of-words models , convnets , and lstms , and we show that pretrained transformersâ€™ performance declines are substantially smaller. pretrained transformers are also more effective at detecting anomalous or ood examples , while many previous models are frequently worse than chance. we examine which factors affect robustness , finding that larger models are not necessarily more robust , distillation can be harmful , and more diverse pretraining data can enhance robustness. finally , we show where future work can improve ood robustness.
