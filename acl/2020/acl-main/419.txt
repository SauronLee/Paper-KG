Information-Theoretic Probing for Linguistic Structure | Tiago Pimentel | the success of neural networks on a diverse set of nlp tasks has led researchers to question how much these networks actually “know” about natural language. probes are a natural way of assessing this. when probing , a researcher chooses a linguistic task and trains a supervised model to predict annotations in that linguistic task from the network’s learned representations. if the probe does well , the researcher may conclude that the representations encode knowledge related to the task. a commonly held belief is that using simpler models as probes is better; the logic is that simpler models will identify linguistic structure , but not learn the task itself. we propose an information-theoretic operationalization of probing as estimating mutual information that contradicts this received wisdom: one should always select the highest performing probe one can , even if it is more complex , since it will result in a tighter estimate , and thus reveal more of the linguistic information inherent in the representation. the experimental portion of our paper focuses on empirically estimating the mutual information between a linguistic property and bert , comparing these estimates to several baselines. we evaluate on a set of ten typologically diverse languages often underrepresented in nlp research—plus english—totalling eleven languages. our implementation is available in https://github.com/rycolab/info-theoretic-probing.
