Mapping Natural Language Instructions to Mobile UI Action Sequences | Yang Li | we present a new problem: grounding natural language instructions to mobile user interface actions , and create three new datasets for it. for full task evaluation , we create pixelhelp , a corpus that pairs english instructions with actions performed by people on a mobile ui emulator. to scale training , we decouple the language and action data by \( a \) annotating action phrase spans in how-to instructions and \( b \) synthesizing grounded descriptions of actions for mobile user interfaces. we use a transformer to extract action phrase tuples from long-range natural language instructions. a grounding transformer then contextually represents ui objects using both their content and screen position and connects them to object descriptions. given a starting screen and instruction , our model achieves 70.59% accuracy on predicting complete ground-truth action sequences in pixelhelp.
