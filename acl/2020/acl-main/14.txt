A Study of Non-autoregressive Model for Sequence Generation | Yi Ren | non-autoregressive \( nar \) models generate all the tokens of a sequence in parallel , resulting in faster generation speed compared to their autoregressive \( ar \) counterparts but at the cost of lower accuracy. different techniques including knowledge distillation and source-target alignment have been proposed to bridge the gap between ar and nar models in various tasks such as neural machine translation \( nmt \) , automatic speech recognition \( asr \) , and text to speech \( tts \) . with the help of those techniques , nar models can catch up with the accuracy of ar models in some tasks but not in some others. in this work , we conduct a study to understand the difficulty of nar sequence generation and try to answer: \( 1 \) why nar models can catch up with ar models in some tasks but not all \? \( 2 \) why techniques like knowledge distillation and source-target alignment can help nar models. since the main difference between ar and nar models is that nar models do not use dependency among target tokens while ar models do , intuitively the difficulty of nar sequence generation heavily depends on the strongness of dependency among target tokens. to quantify such dependency , we propose an analysis model called comma to characterize the difficulty of different nar sequence generation tasks. we have several interesting findings: 1 \) among the nmt , asr and tts tasks , asr has the most target-token dependency while tts has the least. 2 \) knowledge distillation reduces the target-token dependency in target sequence and thus improves the accuracy of nar models. 3 \) source-target alignment constraint encourages dependency of a target token on source tokens and thus eases the training of nar models.
