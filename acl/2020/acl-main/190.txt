GAN-BERT: Generative Adversarial Learning for Robust Text Classification with a Bunch of Labeled Examples | Danilo Croce | recent transformer-based architectures , e.g. , bert , provide impressive results in many natural language processing tasks. however , most of the adopted benchmarks are made of \( sometimes hundreds of \) thousands of examples. in many real scenarios , obtaining high- quality annotated data is expensive and time consuming; in contrast , unlabeled examples characterizing the target task can be , in general , easily collected. one promising method to enable semi-supervised learning has been proposed in image processing , based on semi- supervised generative adversarial networks. in this paper , we propose gan-bert that ex- tends the fine-tuning of bert-like architectures with unlabeled data in a generative adversarial setting. experimental results show that the requirement for annotated examples can be drastically reduced \( up to only 50-100 annotated examples \) , still obtaining good performances in several sentence classification tasks.
