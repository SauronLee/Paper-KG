Human Attention Maps for Text Classification: Do Humans and Neural Networks Focus on the Same Words? | Cansu Sen | motivated by human attention , computational attention mechanisms have been designed to help neural networks adjust their focus on specific parts of the input data. while attention mechanisms are claimed to achieve interpretability , little is known about the actual relationships between machine and human attention. in this work , we conduct the first quantitative assessment of human versus computational attention mechanisms for the text classification task. to achieve this , we design and conduct a large-scale crowd-sourcing study to collect human attention maps that encode the parts of a text that humans focus on when conducting text classification. based on this new resource of human attention dataset for text classification , yelp-hat , collected on the publicly available yelp dataset , we perform a quantitative comparative analysis of machine attention maps created by deep learning models and human attention maps. our analysis offers insights into the relationships between human versus machine attention maps along three dimensions: overlap in word selections , distribution over lexical categories , and context-dependency of sentiment polarity. our findings open promising future research opportunities ranging from supervised attention to the design of human-centric attention-based explanations.
