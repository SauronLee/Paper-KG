FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization | Esin Durmus | neural abstractive summarization models are prone to generate content inconsistent with the source document , i.e. unfaithful. existing automatic metrics do not capture such mistakes effectively. we tackle the problem of evaluating faithfulness of a generated summary given its source document. we first collected human annotations of faithfulness for outputs from numerous models on two datasets. we find that current models exhibit a trade-off between abstractiveness and faithfulness: outputs with less word overlap with the source document are more likely to be unfaithful. next , we propose an automatic question answering \( qa \) based metric for faithfulness , feqa , which leverages recent advances in reading comprehension. given question-answer pairs generated from the summary , a qa model extracts answers from the document; non-matched answers indicate unfaithful information in the summary. among metrics based on word overlap , embedding similarity , and learned language understanding models , our qa-based metric has significantly higher correlation with human faithfulness scores , especially on highly abstractive summaries.
