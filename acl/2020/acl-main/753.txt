Balancing Training for Multilingual Neural Machine Translation | Xinyi Wang | when training multilingual machine translation \( mt \) models that can translate to/from multiple languages , we are faced with imbalanced training sets: some languages have much more training data than others. standard practice is to up-sample less resourced languages to increase representation , and the degree of up-sampling has a large effect on the overall performance. in this paper , we propose a method that instead automatically learns how to weight training data through a data scorer that is optimized to maximize performance on all test languages. experiments on two sets of languages under both one-to-many and many-to-one mt settings show our method not only consistently outperforms heuristic baselines in terms of average performance , but also offers flexible control over the performance of which languages are optimized.
