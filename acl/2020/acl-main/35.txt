Jointly Masked Sequence-to-Sequence Model for Non-Autoregressive Neural Machine Translation | Junliang Guo | the masked language model has received remarkable attention due to its effectiveness on various natural language processing tasks. however , few works have adopted this technique in the sequence-to-sequence models. in this work , we introduce a jointly masked sequence-to-sequence model and explore its application on non-autoregressive neural machine translation~ \( nat \) . specifically , we first empirically study the functionalities of the encoder and the decoder in nat models , and find that the encoder takes a more important role than the decoder regarding the translation quality. therefore , we propose to train the encoder more rigorously by masking the encoder input while training. as for the decoder , we propose to train it based on the consecutive masking of the decoder input with an
