Beyond User Self-Reported Likert Scale Ratings: A Comparison Model for Automatic Dialog Evaluation | Weixin Liang | open domain dialog system evaluation is one of the most important challenges in dialog research. existing automatic evaluation metrics , such as bleu are mostly reference-based. they calculate the difference between the generated response and a limited number of available references. likert-score based self-reported user rating is widely adopted by social conversational systems , such as amazon alexa prize chatbots. however , self-reported user rating suffers from bias and variance among different users. to alleviate this problem , we formulate dialog evaluation as a comparison task. we also propose an automatic evaluation model cmade \( comparison model for automatic dialog evaluation \) that automatically cleans self-reported user ratings as it trains on them. specifically , we first use a self-supervised method to learn better dialog feature representation , and then use knn and shapley to remove confusing samples. our experiments show that cmade achieves 89.2% accuracy in the dialog comparison task.
