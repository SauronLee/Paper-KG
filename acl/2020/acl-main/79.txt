Modeling Code-Switch Languages Using Bilingual Parallel Corpus | Grandee Lee | language modeling is the technique to estimate the probability of a sequence of words. a bilingual language model is expected to model the sequential dependency for words across languages , which is difficult due to the inherent lack of suitable training data as well as diverse syntactic structure across languages. we propose a bilingual attention language model \( balm \) that simultaneously performs language modeling objective with a quasi-translation objective to model both the monolingual as well as the cross-lingual sequential dependency. the attention mechanism learns the bilingual context from a parallel corpus. balm achieves state-of-the-art performance on the seame code-switch database by reducing the perplexity of 20.5% over the best-reported result. we also apply balm in bilingual lexicon induction , and language normalization tasks to validate the idea.
