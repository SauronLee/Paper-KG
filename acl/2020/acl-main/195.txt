On Importance Sampling-Based Evaluation of Latent Language Models | Robert L. Logan IV | language models that use additional latent structures \( e.g. , syntax trees , coreference chains , knowledge graph links \) provide several advantages over traditional language models. however , likelihood-based evaluation of these models is often intractable as it requires marginalizing over the latent space. existing works avoid this issue by using importance sampling. although this approach has asymptotic guarantees , analysis is rarely conducted on the effect of decisions such as sample size and choice of proposal distribution on the reported estimates. in this paper , we carry out this analysis for three models: rnng , entitynlm , and kglm. in addition , we elucidate subtle differences in how importance sampling is applied in these works that can have substantial effects on the final estimates , as well as provide theoretical results which reinforce the validity of this technique.
