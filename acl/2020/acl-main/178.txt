Recurrent Neural Network Language Models Always Learn English-Like Relative Clause Attachment | Forrest Davis | a standard approach to evaluating language models analyzes how models assign probabilities to valid versus invalid syntactic constructions \( i.e. is a grammatical sentence more probable than an ungrammatical sentence \) . our work uses ambiguous relative clause attachment to extend such evaluations to cases of multiple simultaneous valid interpretations , where stark grammaticality differences are absent. we compare model performance in english and spanish to show that non-linguistic biases in rnn lms advantageously overlap with syntactic structure in english but not spanish. thus , english models may appear to acquire human-like syntactic preferences , while models trained on spanish fail to acquire comparable human-like preferences. we conclude by relating these results to broader concerns about the relationship between comprehension \( i.e. typical language model use cases \) and production \( which generates the training data for language models \) , suggesting that necessary linguistic biases are not present in the training signal at all.
