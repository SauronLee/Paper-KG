Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly | Nora Kassner | building on petroni et al. 2019 , we propose two new probing tasks analyzing factual knowledge stored in pretrained language models \( plms \) . \( 1 \) negation. we find that plms do not distinguish between negated \( ‘‘birds cannot [mask]” \) and non-negated \( ‘‘birds can [mask]” \) cloze questions. \( 2 \) mispriming. inspired by priming methods in human psychology , we add “misprimes” to cloze questions \( ‘‘talk \? birds can [mask]” \) . we find that plms are easily distracted by misprimes. these results suggest that plms still have a long way to go to adequately learn human-like factual knowledge.
