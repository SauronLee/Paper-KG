Self-Attention with Cross-Lingual Position Representation | Liang Ding | position encoding \( pe \) , an essential part of self-attention networks \( sans \) , is used to preserve the word order information for natural language processing tasks , generating fixed position indices for input sequences. however , in cross-lingual scenarios , machine translation , the pes of source and target sentences are modeled independently. due to word order divergences in different languages , modeling the cross-lingual positional relationships might help sans tackle this problem. in this paper , we augment sans with
