Semi-Supervised Dialogue Policy Learning via Stochastic Reward Estimation | Xinting Huang | dialogue policy optimization often obtains feedback until task completion in task-oriented dialogue systems. this is insufficient for training intermediate dialogue turns since supervision signals \( or rewards \) are only provided at the end of dialogues. to address this issue , reward learning has been introduced to learn from state-action pairs of an optimal policy to provide turn-by-turn rewards. this approach requires complete state-action annotations of human-to-human dialogues \( i.e. , expert demonstrations \) , which is labor intensive. to overcome this limitation , we propose a novel reward learning approach for semi-supervised policy learning. the proposed approach learns a dynamics model as the reward function which models dialogue progress \( i.e. , state-action sequences \) based on expert demonstrations , either with or without annotations. the dynamics model computes rewards by predicting whether the dialogue progress is consistent with expert demonstrations. we further propose to learn action embeddings for a better generalization of the reward function. the proposed approach outperforms competitive policy learning baselines on multiwoz , a benchmark multi-domain dataset.
