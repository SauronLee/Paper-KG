BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance | Timo Schick | pretraining deep language models has led to large performance gains in nlp. despite this success , schick and sch√ºtze \( 2020 \) recently showed that these models struggle to understand rare words. for static word embeddings , this problem has been addressed by separately learning representations for rare words. in this work , we transfer this idea to pretrained language models: we introduce bertram , a powerful architecture based on bert that is capable of inferring high-quality embeddings for rare words that are suitable as input representations for deep language models. this is achieved by enabling the surface form and contexts of a word to interact with each other in a deep architecture. integrating bertram into bert leads to large performance increases due to improved representations of rare and medium frequency words on both a rare word probing task and three downstream tasks.
