Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation | Haipeng Sun | unsupervised neural machine translation \( unmt \) has recently achieved remarkable results for several language pairs. however , it can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time. that is , research on multilingual unmt has been limited. in this paper , we empirically introduce a simple method to translate between thirteen languages using a single encoder and a single decoder , making use of multilingual data to improve unmt for all language pairs. on the basis of the empirical findings , we propose two knowledge distillation methods to further enhance multilingual unmt performance. our experiments on a dataset with english translated to and from twelve other languages \( including three language families and six language branches \) show remarkable results , surpassing strong unsupervised individual baselines while achieving promising performance between non-english language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs.
