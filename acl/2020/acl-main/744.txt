TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data | Pengcheng Yin | recent years have witnessed the burgeoning of pretrained language models \( lms \) for text-based natural language \( nl \) understanding tasks. such models are typically trained on free-form nl text , hence may not be suitable for tasks like semantic parsing over structured data , which require reasoning over both free-form nl questions and structured tabular data \( e.g. , database tables \) . in this paper we present tabert , a pretrained lm that jointly learns representations for nl sentences and \( semi- \) structured tables. tabert is trained on a large corpus of 26 million tables and their english contexts. in experiments , neural semantic parsers using tabert as feature representation layers achieve new best results on the challenging weakly-supervised semantic parsing benchmark wikitablequestions , while performing competitively on the text-to-sql dataset spider.
