Multidirectional Associative Optimization of Function-Specific Word Representations | Daniela Gerz | we present a neural framework for learning associations between interrelated groups of words such as the ones found in subject-verb-object \( svo \) structures. our model induces a joint function-specific word vector space , where vectors of e.g. plausible svo compositions lie close together. the model retains information about word group membership even in the joint space , and can thereby effectively be applied to a number of tasks reasoning over the svo structure. we show the robustness and versatility of the proposed framework by reporting state-of-the-art results on the tasks of estimating selectional preference and event similarity. the results indicate that the combinations of representations learned with our task-independent model outperform task-specific architectures from prior work , while reducing the number of parameters by up to 95%.
