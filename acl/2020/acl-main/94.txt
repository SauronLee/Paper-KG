Moving Down the Long Tail of Word Sense Disambiguation with Gloss Informed Bi-encoders | Terra Blevins | a major obstacle in word sense disambiguation \( wsd \) is that word senses are not uniformly distributed , causing existing models to generally perform poorly on senses that are either rare or unseen during training. we propose a bi-encoder model that independently embeds \( 1 \) the target word with its surrounding context and \( 2 \) the dictionary definition , or gloss , of each sense. the encoders are jointly optimized in the same representation space , so that sense disambiguation can be performed by finding the nearest sense embedding for each target word embedding. our system outperforms previous state-of-the-art models on english all-words wsd; these gains predominantly come from improved performance on rare senses , leading to a 31.1% error reduction on less frequent senses over prior work. this demonstrates that rare senses can be more effectively disambiguated by modeling their definitions.
