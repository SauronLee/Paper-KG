Beyond Accuracy: Behavioral Testing of NLP Models with CheckList | Marco Tulio Ribeiro | although measuring held-out accuracy has been the primary approach to evaluate generalization , it often overestimates the performance of nlp models , while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. inspired by principles of behavioral testing in software engineering , we introduce checklist , a task-agnostic methodology for testing nlp models. checklist includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation , as well as a software tool to generate a large and diverse number of test cases quickly. we illustrate the utility of checklist with tests for three tasks , identifying critical failures in both commercial and state-of-art models. in a user study , a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. in another user study , nlp practitioners with checklist created twice as many tests , and found almost three times as many bugs as users without it.
