Does Multi-Encoder Help? A Case Study on Context-Aware Neural Machine Translation | Bei Li | in encoder-decoder neural models , multiple encoders are in general used to represent the contextual information in addition to the individual sentence. in this paper , we investigate multi-encoder approaches in document-level neural machine translation \( nmt \) . surprisingly , we find that the context encoder does not only encode the surrounding sentences but also behaves as a noise generator. this makes us rethink the real benefits of multi-encoder in context-aware translation - some of the improvements come from robust training. we compare several methods that introduce noise and/or well-tuned dropout setup into the training of these encoders. experimental results show that noisy training plays an important role in multi-encoder-based nmt , especially when the training data is small. also , we establish a new state-of-the-art on iwslt fr-en task by careful use of noise generation and dropout methods.
