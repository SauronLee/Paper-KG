Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation | Biao Zhang | massively multilingual models for neural machine translation \( nmt \) are theoretically attractive , but often underperform bilingual models and deliver poor zero-shot translations. in this paper , we explore ways to improve them. we argue that multilingual nmt requires stronger modeling capacity to support language pairs with varying typological characteristics , and overcome this bottleneck via language-specific components and deepening nmt architectures. we identify the off-target translation issue \( i.e. translating into a wrong target language \) as the major source of the inferior zero-shot performance , and propose random online backtranslation to enforce the translation of unseen training language pairs. experiments on opus-100 \( a novel multilingual dataset with 100 languages \) show that our approach substantially narrows the performance gap with bilingual models in both one-to-many and many-to-many settings , and improves zero-shot performance by ~10 bleu , approaching conventional pivot-based methods.
