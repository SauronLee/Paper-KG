Attentive Pooling with Learnable Norms for Text Representation | Chuhan Wu | pooling is an important technique for learning text representations in many neural nlp models. in conventional pooling methods such as average , max and attentive pooling , text representations are weighted summations of the l1 or lâˆž norm of input features. however , their pooling norms are always fixed and may not be optimal for learning accurate text representations in different tasks. in addition , in many popular pooling methods such as max and attentive pooling some features may be over-emphasized , while other useful ones are not fully exploited. in this paper , we propose an attentive pooling with learnable norms \( apln \) approach for text representation. different from existing pooling methods that use a fixed pooling norm , we propose to learn the norm in an end-to-end manner to automatically find the optimal ones for text representation in different tasks. in addition , we propose two methods to ensure the numerical stability of the model training. the first one is scale limiting , which re-scales the input to ensure non-negativity and alleviate the risk of exponential explosion. the second one is re-formulation , which decomposes the exponent operation to avoid computing the real-valued powers of the input and further accelerate the pooling operation. experimental results on four benchmark datasets show that our approach can effectively improve the performance of attentive pooling.
