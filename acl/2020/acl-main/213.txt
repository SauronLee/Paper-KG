Integrating Multimodal Information in Large Pretrained Transformers | Wasifur Rahman | recent transformer-based contextual word representations , including bert and xlnet , have shown state-of-the-art performance in multiple disciplines within nlp. fine-tuning the trained contextual models on task-specific datasets has been the key to achieving superior performance downstream. while fine-tuning these pre-trained models is straightforward for lexical applications \( applications with only language modality \) , it is not trivial for multimodal language \( a growing area in nlp focused on modeling face-to-face communication \) . more specifically , this is due to the fact that pre-trained models donâ€™t have the necessary components to accept two extra modalities of vision and acoustic. in this paper , we proposed an attachment to bert and xlnet called multimodal adaptation gate \( mag \) . mag allows bert and xlnet to accept multimodal nonverbal data during fine-tuning. it does so by generating a shift to internal representation of bert and xlnet; a shift that is conditioned on the visual and acoustic modalities. in our experiments , we study the commonly used cmu-mosi and cmu-mosei datasets for multimodal sentiment analysis. fine-tuning mag-bert and mag-xlnet significantly boosts the sentiment analysis performance over previous baselines as well as language-only fine-tuning of bert and xlnet. on the cmu-mosi dataset , mag-xlnet achieves human-level multimodal sentiment analysis performance for the first time in the nlp community.
