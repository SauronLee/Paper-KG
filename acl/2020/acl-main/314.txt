Enhancing Pre-trained Chinese Character Representation with Word-aligned Attention | Yanzeng Li | most chinese pre-trained models take character as the basic unit and learn representation according to characterâ€™s external contexts , ignoring the semantics expressed in the word , which is the smallest meaningful utterance in chinese. hence , we propose a novel word-aligned attention to exploit explicit word information , which is complementary to various character-based chinese pre-trained language models. specifically , we devise a pooling mechanism to align the character-level attention to the word level and propose to alleviate the potential issue of segmentation error propagation by multi-source information fusion. as a result , word and character information are explicitly integrated at the fine-tuning procedure. experimental results on five chinese nlp benchmark tasks demonstrate that our method achieves significant improvements against bert , ernie and bert-wwm.
