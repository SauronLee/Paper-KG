Cross-modal Language Generation using Pivot Stabilization for Web-scale Language Coverage | Ashish V. Thapliyal | cross-modal language generation tasks such as image captioning are directly hurt in their ability to support non-english languages by the trend of data-hungry models combined with the lack of non-english annotations. we investigate potential solutions for combining existing language-generation annotations in english with translation capabilities in order to create solutions at web-scale in both domain and language coverage. we describe an approach called pivot-language generation stabilization \( plugs \) , which leverages directly at training time both existing english annotations \( gold data \) as well as their machine-translated versions \( silver data \) ; at run-time , it generates first an english caption and then a corresponding target-language caption. we show that plugs models outperform other candidate solutions in evaluations performed over 5 different target languages , under a large-domain testset using images from the open images dataset. furthermore , we find an interesting effect where the english captions generated by the plugs models are better than the captions generated by the original , monolingual english model.
