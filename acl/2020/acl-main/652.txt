MLQA: Evaluating Cross-lingual Extractive Question Answering | Patrick Lewis | question answering \( qa \) models have shown rapid progress enabled by the availability of large , high-quality benchmark datasets. such annotated datasets are difficult and costly to collect , and rarely exist in languages other than english , making building qa systems that work well in other languages challenging. in order to develop such systems , it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. we present mlqa , a multi-way aligned extractive qa evaluation benchmark intended to spur research in this area. mlqa contains qa instances in 7 languages , english , arabic , german , spanish , hindi , vietnamese and simplified chinese. mlqa has over 12k instances in english and 5k in each other language , with each instance parallel between 4 languages on average. we evaluate state-of-the-art cross-lingual models and machine-translation-based baselines on mlqa. in all cases , transfer results are shown to be significantly behind training-language performance.
