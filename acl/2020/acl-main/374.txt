Do Neural Language Models Show Preferences for Syntactic Formalisms? | Artur Kulmizev | recent work on the interpretability of deep neural language models has concluded that many properties of natural language syntax are encoded in their representational spaces. however , such studies often suffer from limited scope by focusing on a single language and a single linguistic formalism. in this study , we aim to investigate the extent to which the semblance of syntactic structure captured by language models adheres to a surface-syntactic or deep syntactic style of analysis , and whether the patterns are consistent across different languages. we apply a probe for extracting directed dependency trees to bert and elmo models trained on 13 different languages , probing for two different syntactic annotation styles: universal dependencies \( ud \) , prioritizing deep syntactic relations , and surface-syntactic universal dependencies \( sud \) , focusing on surface structure. we find that both models exhibit a preference for ud over sud — with interesting variations across languages and layers — and that the strength of this preference is correlated with differences in tree shape.
