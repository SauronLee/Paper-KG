Weight Poisoning Attacks on Pretrained Models | Keita Kurita | recently , nlp has seen a surge in the usage of large pre-trained models. users download weights of models pre-trained on large datasets , then fine-tune the weights on a task of their choice. this raises the question of whether downloading untrusted pre-trained weights can pose a security threat. in this paper , we show that it is possible to construct “weight poisoning” attacks where pre-trained weights are injected with vulnerabilities that expose “backdoors” after fine-tuning , enabling the attacker to manipulate the model prediction simply by injecting an arbitrary keyword. we show that by applying a regularization method which we call ripple and an initialization procedure we call embedding surgery , such attacks are possible even with limited knowledge of the dataset and fine-tuning procedure. our experiments on sentiment classification , toxicity detection , and spam detection show that this attack is widely applicable and poses a serious threat. finally , we outline practical defenses against such attacks.
