Hard-Coded Gaussian Attention for Neural Machine Translation | Weiqiu You | recent work has questioned the importance of the transformer’s multi-headed attention for achieving high translation quality. we push further in this direction by developing a “hard-coded” attention variant without any learned parameters. surprisingly , replacing all learned self-attention heads in the encoder and decoder with fixed , input-agnostic gaussian distributions minimally impacts bleu scores across four different language pairs. however , additionally , hard-coding cross attention \( which connects the decoder to the encoder \) significantly lowers bleu , suggesting that it is more important than self-attention. much of this bleu drop can be recovered by adding just a single learned cross attention head to an otherwise hard-coded transformer. taken as a whole , our results offer insight into which components of the transformer are actually important , which we hope will guide future work into the development of simpler and more efficient attention-based models.
