Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations | Oana-Maria Camburu | to increase trust in artificial intelligence systems , a promising research direction consists of designing neural models capable of generating natural language explanations for their predictions. in this work , we show that such models are nonetheless prone to generating mutually inconsistent explanations , such as ”because there is a dog in the image.” and ”because there is no dog in the [same] image.” , exposing flaws in either the decision-making process of the model or in the generation of the explanations. we introduce a simple yet effective adversarial framework for sanity checking models against the generation of inconsistent natural language explanations. moreover , as part of the framework , we address the problem of adversarial attacks with full target sequences , a scenario that was not previously addressed in sequence-to-sequence attacks. finally , we apply our framework on a state-of-the-art neural natural language inference model that provides natural language explanations for its predictions. our framework shows that this model is capable of generating a significant number of inconsistent explanations.
