Text Classification with Negative Supervision | Sora Ohashi | advanced pre-trained models for text representation have achieved state-of-the-art performance on various text classification tasks. however , the discrepancy between the semantic similarity of texts and labelling standards affects classifiers , i.e. leading to lower performance in cases where classifiers should assign different labels to semantically similar texts. to address this problem , we propose a simple multitask learning model that uses negative supervision. specifically , our model encourages texts with different labels to have distinct representations. comprehensive experiments show that our model outperforms the state-of-the-art pre-trained model on both single- and multi-label classifications , sentence and document classifications , and classifications in three different languages.
