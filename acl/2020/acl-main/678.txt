The Sensitivity of Language Models and Humans to Winograd Schema Perturbations | Mostafa Abdou | large-scale pretrained language models are the major driving force behind recent improvements in perfromance on the winograd schema challenge , a widely employed test of commonsense reasoning ability. we show , however , with a new diagnostic dataset , that these models are sensitive to linguistic perturbations of the winograd examples that minimally affect human understanding. our results highlight interesting differences between humans and language models: language models are more sensitive to number or gender alternations and synonym replacements than humans , and humans are more stable and consistent in their predictions , maintain a much higher absolute performance , and perform better on non-associative instances than associative ones.
