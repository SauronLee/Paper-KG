Injecting Numerical Reasoning Skills into Language Models | Mor Geva | large pre-trained language models \( lms \) are known to encode substantial amounts of linguistic information. however , high-level reasoning skills , such as numerical reasoning , are difficult to learn from a language-modeling objective only. consequently , existing models for numerical reasoning have used specialized architectures with limited flexibility. in this work , we show that numerical reasoning is amenable to automatic data generation , and thus one can inject this skill into pre-trained lms , by generating large amounts of data , and training in a multi-task setup. we show that pre-training our model , genbert , on this data , dramatically improves performance on drop \( 49.3 â€“> 72.3 f1 \) , reaching performance that matches state-of-the-art models of comparable size , while using a simple and general-purpose encoder-decoder architecture. moreover , genbert generalizes well to math word problem datasets , while maintaining high performance on standard rc tasks. our approach provides a general recipe for injecting skills into large pre-trained lms , whenever the skill is amenable to automatic data augmentation.
