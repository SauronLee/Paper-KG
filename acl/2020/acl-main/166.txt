GPT-too: A Language-Model-First Approach for AMR-to-Text Generation | Manuel Mager | abstract meaning representations \( amrs \) are broad-coverage sentence-level semantic graphs. existing approaches to generating text from amr have focused on training sequence-to-sequence or graph-to-sequence models on amr annotated data only. in this paper , we propose an alternative approach that combines a strong pre-trained language model with cycle consistency-based re-scoring. despite the simplicity of the approach , our experimental results show these models outperform all previous techniques on the english ldc2017t10 dataset , including the recent use of transformer architectures. in addition to the standard evaluation metrics , we provide human evaluation experiments that further substantiate the strength of our approach.
