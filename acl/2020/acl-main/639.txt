Heterogeneous Graph Transformer for Graph-to-Sequence Learning | Shaowei Yao | the graph-to-sequence \( graph2seq \) learning aims to transduce graph-structured representations to word sequences for text generation. recent studies propose various models to encode graph structure. however , most previous works ignore the indirect relations between distance nodes , or treat indirect relations and direct relations in the same way. in this paper , we propose the heterogeneous graph transformer to independently model the different relations in the individual subgraphs of the original graph , including direct relations , indirect relations and multiple possible relations between nodes. experimental results show that our model strongly outperforms the state of the art on all four standard benchmarks of amr-to-text generation and syntax-based neural machine translation.
