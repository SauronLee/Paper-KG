Discourse-Aware Neural Extractive Text Summarization | Jiacheng Xu | recently bert has been adopted for document encoding in state-of-the-art text summarization models. however , sentence-based extractive models often result in redundant or uninformative phrases in the extracted summaries. also , long-range dependencies throughout a document are not well captured by bert , which is pre-trained on sentence pairs instead of documents. to address these issues , we present a discourse-aware neural summarization model - discobert. discobert extracts sub-sentential discourse units \( instead of sentences \) as candidates for extractive selection on a finer granularity. to capture the long-range dependencies among discourse units , structural discourse graphs are constructed based on rst trees and coreference mentions , encoded with graph convolutional networks. experiments show that the proposed model outperforms state-of-the-art methods by a significant margin on popular summarization benchmarks compared to other bert-base models.
