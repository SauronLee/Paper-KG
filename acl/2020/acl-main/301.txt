Efficient Second-Order TreeCRF for Neural Dependency Parsing | Yu Zhang | in the deep learning \( dl \) era , parsing models are extremely simplified with little hurt on performance , thanks to the remarkable capability of multi-layer bilstms in context representation. as the most popular graph-based dependency parser due to its high efficiency and performance , the biaffine parser directly scores single dependencies under the arc-factorization assumption , and adopts a very simple local token-wise cross-entropy training loss. this paper for the first time presents a second-order treecrf extension to the biaffine parser. for a long time , the complexity and inefficiency of the inside-outside algorithm hinder the popularity of treecrf. to address this issue , we propose an effective way to batchify the inside and viterbi algorithms for direct large matrix operation on gpus , and to avoid the complex outside algorithm via efficient back-propagation. experiments and analysis on 27 datasets from 13 languages clearly show that techniques developed before the dl era , such as structural learning \( global treecrf loss \) and high-order modeling are still useful , and can further boost parsing performance over the state-of-the-art biaffine parser , especially for partially annotated training data. we release our code at https://github.com/yzhangcs/crfpar.
