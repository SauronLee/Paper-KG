BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension | Mike Lewis | we present bart , a denoising autoencoder for pretraining sequence-to-sequence models. bart is trained by \( 1 \) corrupting text with an arbitrary noising function , and \( 2 \) learning a model to reconstruct the original text. it uses a standard tranformer-based neural machine translation architecture which , despite its simplicity , can be seen as generalizing bert \( due to the bidirectional encoder \) , gpt \( with the left-to-right decoder \) , and other recent pretraining schemes. we evaluate a number of noising approaches , finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme , where spans of text are replaced with a single mask token. bart is particularly effective when fine tuned for text generation but also works well for comprehension tasks. it matches the performance of roberta on glue and squad , and achieves new state-of-the-art results on a range of abstractive dialogue , question answering , and summarization tasks , with gains of up to 3.5 rouge. bart also provides a 1.1 bleu increase over a back-translation system for machine translation , with only target language pretraining. we also replicate other pretraining schemes within the bart framework , to understand their effect on end-task performance.
