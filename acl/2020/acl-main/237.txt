Knowledge Graph Embedding Compression | Mrinmaya Sachan | knowledge graph \( kg \) representation learning techniques that learn continuous embeddings of entities and relations in the kg have become popular in many ai applications. with a large kg , the embeddings consume a large amount of storage and memory. this is problematic and prohibits the deployment of these techniques in many real world settings. thus , we propose an approach that compresses the kg embedding layer by representing each entity in the kg as a vector of discrete codes and then composes the embeddings from these codes. the approach can be trained end-to-end with simple modifications to any existing kg embedding technique. we evaluate the approach on various standard kg embedding evaluations and show that it achieves 50-1000x compression of embeddings with a minor loss in performance. the compressed embeddings also retain the ability to perform various reasoning tasks such as kg inference.
