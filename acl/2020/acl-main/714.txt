Exploiting the Syntax-Model Consistency for Neural Relation Extraction | Amir Pouran Ben Veyseh | this paper studies the task of relation extraction \( re \) that aims to identify the semantic relations between two entity mentions in text. in the deep learning models for re , it has been beneficial to incorporate the syntactic structures from the dependency trees of the input sentences. in such models , the dependency trees are often used to directly structure the network architectures or to obtain the dependency relations between the word pairs to inject the syntactic information into the models via multi-task learning. the major problem with these approaches is the lack of generalization beyond the syntactic structures in the training data or the failure to capture the syntactic importance of the words for re. in order to overcome these issues , we propose a novel deep learning model for re that uses the dependency trees to extract the syntax-based importance scores for the words , serving as a tree representation to introduce syntactic information into the models with greater generalization. in particular , we leverage ordered-neuron long-short term memory networks \( on-lstm \) to infer the model-based importance scores for re for every word in the sentences that are then regulated to be consistent with the syntax-based scores to enable syntactic information injection. we perform extensive experiments to demonstrate the effectiveness of the proposed method , leading to the state-of-the-art performance on three re benchmark datasets.
