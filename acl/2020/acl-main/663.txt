Improving Image Captioning with Better Use of Caption | Zhan Shi | image captioning is a multimodal problem that has drawn extensive attention in both the natural language processing and computer vision community. in this paper , we present a novel image captioning architecture to better explore semantics available in captions and leverage that to enhance both image representation and caption generation. our models first construct caption-guided visual relationship graphs that introduce beneficial inductive bias using weakly supervised multi-instance learning. the representation is then enhanced with neighbouring and contextual nodes with their textual and visual features. during generation , the model further incorporates visual relationships using multi-task learning for jointly predicting word and object/predicate tag sequences. we perform extensive experiments on the mscoco dataset , showing that the proposed framework significantly outperforms the baselines , resulting in the state-of-the-art performance under a wide range of evaluation metrics. the code of our paper has been made publicly available.
