Enhancing Machine Translation with Dependency-Aware Self-Attention | Emanuele Bugliarello | most neural machine translation models only rely on pairs of parallel sentences , assuming syntactic information is automatically learned by an attention mechanism. in this work , we investigate different approaches to incorporate syntactic knowledge in the transformer model and also propose a novel , parameter-free , dependency-aware self-attention mechanism that improves its translation quality , especially for long sentences and in low-resource scenarios. we show the efficacy of each approach on wmt english-german and english-turkish , and wat english-japanese translation tasks.
