Multi-Agent Task-Oriented Dialog Policy Learning with Role-Aware Reward Decomposition | Ryuichi Takanobu | many studies have applied reinforcement learning to train a dialog policy and show great promise these years. one common approach is to employ a user simulator to obtain a large number of simulated user experiences for reinforcement learning algorithms. however , modeling a realistic user simulator is challenging. a rule-based simulator requires heavy domain expertise for complex tasks , and a data-driven simulator requires considerable data and it is even unclear how to evaluate a simulator. to avoid explicitly building a user simulator beforehand , we propose multi-agent dialog policy learning , which regards both the system and the user as the dialog agents. two agents interact with each other and are jointly learned simultaneously. the method uses the actor-critic framework to facilitate pretraining and improve scalability. we also propose hybrid value network for the role-aware reward decomposition to integrate role-specific domain knowledge of each agent in the task-oriented dialog. results show that our method can successfully build a system policy and a user policy simultaneously , and two agents can achieve a high task success rate through conversational interaction.
