Unsupervised Multimodal Neural Machine Translation with Pseudo Visual Pivoting | Po-Yao Huang | unsupervised machine translation \( mt \) has recently achieved impressive results with monolingual corpora only. however , it is still challenging to associate source-target sentences in the latent space. as people speak different languages biologically share similar visual systems , the potential of achieving better alignment through visual content is promising yet under-explored in unsupervised multimodal mt \( mmt \) . in this paper , we investigate how to utilize visual content for disambiguation and promoting latent space alignment in unsupervised mmt. our model employs multimodal back-translation and features pseudo visual pivoting in which we learn a shared multilingual visual-semantic embedding space and incorporate visually-pivoted captioning as additional weak supervision. the experimental results on the widely used multi30k dataset show that the proposed model significantly improves over the state-of-the-art methods and generalizes well when images are not available at the testing time.
