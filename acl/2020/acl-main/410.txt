DeFormer: Decomposing Pre-trained Transformers for Faster Question Answering | Qingqing Cao | transformer-based qa models use input-wide self-attention – i.e. across both the question and the input passage – at all layers , causing them to be slow and memory-intensive. it turns out that we can get by without input-wide self-attention at all layers , especially in the lower layers. we introduce deformer , a decomposed transformer , which substitutes the full self-attention with question-wide and passage-wide self-attentions in the lower layers. this allows for question-independent processing of the input text representations , which in turn enables pre-computing passage representations reducing runtime compute drastically. furthermore , because deformer is largely similar to the original model , we can initialize deformer with the pre-training weights of a standard transformer , and directly fine-tune on the target qa dataset. we show deformer versions of bert and xlnet can be used to speed up qa by over 4.3x and with simple distillation-based losses they incur only a 1% drop in accuracy. we open source the code at https://github.com/stonybrooknlp/deformer.
