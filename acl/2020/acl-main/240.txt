Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding | Yun Tang | distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task , from transe to the latest state-of-the-art rotate. however , complex relations such as n-to-1 , 1-to-n and n-to-n still remain challenging to predict. in this work , we propose a novel distance-based approach for knowledge graph link prediction. first , we extend the rotate from 2d complex domain to high dimensional space with orthogonal transforms to model relations. the orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric , inverse and compositional relations while achieves better modeling capacity. second , the graph context is integrated into distance scoring functions directly. specifically , graph context is explicitly modeled via two directed context representations. each node embedding in knowledge graph is augmented with two context representations , which are computed from the neighboring outgoing and incoming nodes/edges respectively. the proposed approach improves prediction accuracy on the difficult n-to-1 , 1-to-n and n-to-n cases. our experimental results show that it achieves state-of-the-art results on two common benchmarks fb15k-237 and wnrr-18 , especially on fb15k-237 which has many high in-degree nodes.
