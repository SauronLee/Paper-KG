MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices | Zhiqing Sun | natural language processing \( nlp \) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters. however , these models suffer from heavy model sizes and high latency such that they cannot be deployed to resource-limited mobile devices. in this paper , we propose mobilebert for compressing and accelerating the popular bert model. like the original bert , mobilebert is task-agnostic , that is , it can be generically applied to various downstream nlp tasks via simple fine-tuning. basically , mobilebert is a thin version of bert_large , while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks. to train mobilebert , we first train a specially designed teacher model , an inverted-bottleneck incorporated bert_large model. then , we conduct knowledge transfer from this teacher to mobilebert. empirical studies show that mobilebert is 4.3x smaller and 5.5x faster than bert_base while achieving competitive results on well-known benchmarks. on the natural language inference tasks of glue , mobilebert achieves a glue score of 77.7 \( 0.6 lower than bert_base \) , and 62 ms latency on a pixel 4 phone. on the squad v1.1/v2.0 question answering task , mobilebert achieves a dev f1 score of 90.0/79.2 \( 1.5/2.1 higher than bert_base \) .
