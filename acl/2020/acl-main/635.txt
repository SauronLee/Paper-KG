Meta-Reinforced Multi-Domain State Generator for Dialogue Systems | Yi Huang | a dialogue state tracker \( dst \) is a core component of a modular task-oriented dialogue system. tremendous progress has been made in recent years. however , the major challenges remain. the state-of-the-art accuracy for dst is below 50% for a multi-domain dialogue task. a learnable dst for any new domain requires a large amount of labeled in-domain data and training from scratch. in this paper , we propose a meta-reinforced multi-domain state generator \( meret \) . our first contribution is to improve the dst accuracy. we enhance a neural model based dst generator with a reward manager , which is built on policy gradient reinforcement learning \( rl \) to fine-tune the generator. with this change , we are able to improve the joint accuracy of dst from 48.79% to 50.91% on the multiwoz corpus. second , we explore to train a dst meta-learning model with a few domains as source domains and a new domain as target domain. we apply the model-agnostic meta-learning algorithm \( maml \) to dst and the obtained meta-learning model is used for new domain adaptation. our experimental results show this solution is able to outperform the traditional training approach with extremely less training data in target domain.
