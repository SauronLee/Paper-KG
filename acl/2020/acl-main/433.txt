Spying on Your Neighbors: Fine-grained Probing of Contextual Embeddings for Information about Surrounding Words | Josef Klafka | although models using contextual word embeddings have achieved state-of-the-art results on a host of nlp tasks , little is known about exactly what information these embeddings encode about the context words that they are understood to reflect. to address this question , we introduce a suite of probing tasks that enable fine-grained testing of contextual embeddings for encoding of information about surrounding words. we apply these tasks to examine the popular bert , elmo and gpt contextual encoders , and find that each of our tested information types is indeed encoded as contextual information across tokens , often with near-perfect recoverabilityâ€”but the encoders vary in which features they distribute to which tokens , how nuanced their distributions are , and how robust the encoding of each feature is to distance. we discuss implications of these results for how different types of models break down and prioritize word-level context information when constructing token embeddings.
