SUPERT: Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization | Yang Gao | we study unsupervised multi-document summarization evaluation metrics , which require neither human-written reference summaries nor human annotations \( e.g. preferences , ratings , etc. \) . we propose supert , which rates the quality of a summary by measuring its semantic similarity with a pseudo reference summary , i.e. selected salient sentences from the source documents , using contextualized embeddings and soft token alignment techniques. compared to the state-of-the-art unsupervised evaluation metrics , supert correlates better with human ratings by 18- 39%. furthermore , we use supert as rewards to guide a neural-based reinforcement learning summarizer , yielding favorable performance compared to the state-of-the-art unsupervised summarizers. all source code is available at https://github.com/yg211/acl20-ref-free-eval.
