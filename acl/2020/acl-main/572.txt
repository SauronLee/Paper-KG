Continual Relation Learning via Episodic Memory Activation and Reconsolidation | Xu Han | continual relation learning aims to continually train a model on new data to learn incessantly emerging novel relations while avoiding catastrophically forgetting old relations. some pioneering work has proved that storing a handful of historical relation examples in episodic memory and replaying them in subsequent training is an effective solution for such a challenging problem. however , these memory-based methods usually suffer from overfitting the few memorized examples of old relations , which may gradually cause inevitable confusion among existing relations. inspired by the mechanism in human long-term memory formation , we introduce episodic memory activation and reconsolidation \( emar \) to continual relation learning. every time neural models are activated to learn both new and memorized data , emar utilizes relation prototypes for memory reconsolidation exercise to keep a stable understanding of old relations. the experimental results show that emar could get rid of catastrophically forgetting old relations and outperform the state-of-the-art continual learning models.
