Generating Hierarchical Explanations on Text Classification via Feature Interaction Detection | Hanjie Chen | generating explanations for neural networks has become crucial for their applications in real-world with respect to reliability and trustworthiness. in natural language processing , existing methods usually provide important features which are words or phrases selected from an input text as an explanation , but ignore the interactions between them. it poses challenges for humans to interpret an explanation and connect it to model prediction. in this work , we build hierarchical explanations by detecting feature interactions. such explanations visualize how words and phrases are combined at different levels of the hierarchy , which can help users understand the decision-making of black-box models. the proposed method is evaluated with three neural text classifiers \( lstm , cnn , and bert \) on two benchmark datasets , via both automatic and human evaluations. experiments show the effectiveness of the proposed method in providing explanations that are both faithful to models and interpretable to humans.
