A Batch Normalized Inference Network Keeps the KL Vanishing Away | Qile Zhu | variational autoencoder \( vae \) is widely used as a generative model to approximate a model’s posterior on latent variables by combining the amortized variational inference and deep neural networks. however , when paired with strong autoregressive decoders , vae often converges to a degenerated local optimum known as “posterior collapse”. previous approaches consider the kullback–leibler divergence \( kl \) individual for each datapoint. we propose to let the kl follow a distribution across the whole dataset , and analyze that it is sufficient to prevent posterior collapse by keeping the expectation of the kl’s distribution positive. then we propose batch normalized-vae \( bn-vae \) , a simple but effective approach to set a lower bound of the expectation by regularizing the distribution of the approximate posterior’s parameters. without introducing any new model component or modifying the objective , our approach can avoid the posterior collapse effectively and efficiently. we further show that the proposed bn-vae can be extended to conditional vae \( cvae \) . empirically , our approach surpasses strong autoregressive baselines on language modeling , text classification and dialogue generation , and rivals more complex approaches while keeping almost the same training time as vae.
