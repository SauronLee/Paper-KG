Regularized Context Gates on Transformer for Machine Translation | Xintong Li | context gates are effective to control the contributions from the source and target contexts in the recurrent neural network \( rnn \) based neural machine translation \( nmt \) . however , it is challenging to extend them into the advanced transformer architecture , which is more complicated than rnn. this paper first provides a method to identify source and target contexts and then introduce a gate mechanism to control the source and target contributions in transformer. in addition , to further reduce the bias problem in the gate mechanism , this paper proposes a regularization method to guide the learning of the gates with supervision automatically generated using pointwise mutual information. extensive experiments on 4 translation datasets demonstrate that the proposed model obtains an averaged gain of 1.0 bleu score over a strong transformer baseline.
