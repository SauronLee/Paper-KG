Large Scale Multi-Actor Generative Dialog Modeling | Alex Boyd | non-goal oriented dialog agents \( i.e. chatbots \) aim to produce varying and engaging conversations with a user; however , they typically exhibit either inconsistent personality across conversations or the average personality of all users. this paper addresses these issues by controlling an agent’s persona upon generation via conditioning on prior conversations of a target actor. in doing so , we are able to utilize more abstract patterns within a person’s speech and better emulate them in generated responses. this work introduces the generative conversation control model , an augmented and fine-tuned gpt-2 language model that conditions on past reference conversations to probabilistically model multi-turn conversations in the actor’s persona. we introduce an accompanying data collection procedure to obtain 10.3m conversations from 6 months worth of reddit comments. we demonstrate that scaling model sizes from 117m to 8.3b parameters yields an improvement from 23.14 to 13.14 perplexity on 1.7m held out reddit conversations. increasing model scale yielded similar improvements in human evaluations that measure preference of model samples to the held out target distribution in terms of realism \( 31% increased to 37% preference \) , style matching \( 37% to 42% \) , grammar and content quality \( 29% to 42% \) , and conversation coherency \( 32% to 40% \) . we find that conditionally modeling past conversations improves perplexity by 0.47 in automatic evaluations. through human trials we identify positive trends between conditional modeling and style matching and outline steps to further improve persona control.
