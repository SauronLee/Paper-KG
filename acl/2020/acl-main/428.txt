How does BERTâ€™s attention change when you fine-tune? An analysis methodology and a case study in negation scope | Yiyun Zhao | large pretrained language models like bert , after fine-tuning to a downstream task , have achieved high performance on a variety of nlp problems. yet explaining their decisions is difficult despite recent work probing their internal representations. we propose a procedure and analysis methods that take a hypothesis of how a transformer-based model might encode a linguistic phenomenon , and test the validity of that hypothesis based on a comparison between knowledge-related downstream tasks with downstream control tasks , and measurement of cross-dataset consistency. we apply this methodology to test bert and roberta on a hypothesis that some attention heads will consistently attend from a word in negation scope to the negation cue. we find that after fine-tuning bert and roberta on a negation scope task , the average attention head improves its sensitivity to negation and its attention consistency across negation datasets compared to the pre-trained models. however , only the base models \( not the large models \) improve compared to a control task , indicating there is evidence for a shallow encoding of negation only in the base models.
