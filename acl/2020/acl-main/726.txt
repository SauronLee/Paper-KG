A negative case analysis of visual grounding methods for VQA | Robik Shrestha | existing visual question answering \( vqa \) methods tend to exploit dataset biases and spurious statistical correlations , instead of producing right answers for the right reasons. to address this issue , recent bias mitigation methods for vqa propose to incorporate visual cues \( e.g. , human attention maps \) to better ground the vqa models , showcasing impressive gains. however , we show that the performance improvements are not a result of improved visual grounding , but a regularization effect which prevents over-fitting to linguistic priors. for instance , we find that it is not actually necessary to provide proper , human-based cues; random , insensible cues also result in similar improvements. based on this observation , we propose a simpler regularization scheme that does not require any external annotations and yet achieves near state-of-the-art performance on vqa-cpv2.
