Video-Grounded Dialogues with Pretrained Generation Language Models | Hung Le | pre-trained language models have shown remarkable success in improving various downstream nlp tasks due to their ability to capture dependencies in textual data and generate natural responses. in this paper , we leverage the power of pre-trained language models for improving video-grounded dialogue , which is very challenging and involves complex features of different dynamics: \( 1 \) video features which can extend across both spatial and temporal dimensions; and \( 2 \) dialogue features which involve semantic dependencies over multiple dialogue turns. we propose a framework by extending gpt-2 models to tackle these challenges by formulating video-grounded dialogue tasks as a sequence-to-sequence task , combining both visual and textual representation into a structured sequence , and fine-tuning a large pre-trained gpt-2 network. our framework allows fine-tuning language models to capture dependencies across multiple modalities over different levels of information: spatio-temporal level in video and token-sentence level in dialogue context. we achieve promising improvement on the audio-visual scene-aware dialogues \( avsd \) benchmark from dstc7 , which supports a potential direction in this line of research.
