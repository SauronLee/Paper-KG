FastBERT: a Self-distilling BERT with Adaptive Inference Time | Weijie Liu | pre-trained language models like bert have proven to be highly performant. however , they are often computationally expensive in many practical scenarios , for such heavy models can hardly be readily implemented with limited resources. to improve their efficiency with an assured model performance , we propose a novel speed-tunable fastbert with adaptive inference time. the speed at inference can be flexibly adjusted under varying demands , while redundant calculation of samples is avoided. moreover , this model adopts a unique self-distillation mechanism at fine-tuning , further enabling a greater computational efficacy with minimal loss in performance. our model achieves promising results in twelve english and chinese datasets. it is able to speed up by a wide range from 1 to 12 times than bert if given different speedup thresholds to make a speed-performance tradeoff.
