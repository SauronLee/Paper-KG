Language-aware Interlingua for Multilingual Neural Machine Translation | Changfeng Zhu | multilingual neural machine translation \( nmt \) has led to impressive accuracy improvements in low-resource scenarios by sharing common linguistic information across languages. however , the traditional multilingual model fails to capture the diversity and specificity of different languages , resulting in inferior performance compared with individual models that are sufficiently trained. in this paper , we incorporate a language-aware interlingua into the encoder-decoder architecture. the interlingual network enables the model to learn a language-independent representation from the semantic spaces of different languages , while still allowing for language-specific specialization of a particular language-pair. experiments show that our proposed method achieves remarkable improvements over state-of-the-art multilingual nmt baselines and produces comparable performance with strong individual models.
