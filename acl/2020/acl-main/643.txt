Refer360∘: A Referring Expression Recognition Dataset in 360∘ Images | Volkan Cirik | we propose a novel large-scale referring expression recognition dataset , refer360° , consisting of 17 , 137 instruction sequences and ground-truth actions for completing these instructions in 360° scenes. refer360° differs from existing related datasets in three ways. first , we propose a more realistic scenario where instructors and the followers have partial , yet dynamic , views of the scene – followers continuously modify their field-of-view \( fov \) while interpreting instructions that specify a final target location. second , instructions to find the target location consist of multiple steps for followers who will start at random fovs. as a result , intermediate instructions are strongly grounded in object references , and followers must identify intermediate fovs to find the final target location correctly. third , the target locations are neither restricted to predefined objects nor chosen by annotators; instead , they are distributed randomly across scenes. this “point anywhere” approach leads to more linguistically complex instructions , as shown in our analyses. our examination of the dataset shows that refer360° manifests linguistically rich phenomena in a language grounding task that poses novel challenges for computational modeling of language , vision , and navigation.
