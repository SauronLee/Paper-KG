Learning Efficient Dialogue Policy from Demonstrations through Shaping | Huimin Wang | training a task-oriented dialogue agent with reinforcement learning is prohibitively expensive since it requires a large volume of interactions with users. human demonstrations can be used to accelerate learning progress. however , how to effectively leverage demonstrations to learn dialogue policy remains less explored. in this paper , we present sˆ2agent that efficiently learns dialogue policy from demonstrations through policy shaping and reward shaping. we use an imitation model to distill knowledge from demonstrations , based on which policy shaping estimates feedback on how the agent should act in policy space. reward shaping is then incorporated to bonus state-actions similar to demonstrations explicitly in value space encouraging better exploration. the effectiveness of the proposed sˆ2agentt is demonstrated in three dialogue domains and a challenging domain adaptation task with both user simulator evaluation and human evaluation.
