Efficient Dialogue State Tracking by Selectively Overwriting Memory | Sungdong Kim | recent works in dialogue state tracking \( dst \) focus on an open vocabulary-based setting to resolve scalability and generalization issues of the predefined ontology-based approaches. however , they are inefficient in that they predict the dialogue state at every turn from scratch. here , we consider dialogue state as an explicit fixed-sized memory and propose a selectively overwriting mechanism for more efficient dst. this mechanism consists of two steps: \( 1 \) predicting state operation on each of the memory slots , and \( 2 \) overwriting the memory with new values , of which only a few are generated according to the predicted state operations. our method decomposes dst into two sub-tasks and guides the decoder to focus only on one of the tasks , thus reducing the burden of the decoder. this enhances the effectiveness of training and dst performance. our som-dst \( selectively overwriting memory for dialogue state tracking \) model achieves state-of-the-art joint goal accuracy with 51.72% in multiwoz 2.0 and 53.01% in multiwoz 2.1 in an open vocabulary-based dst setting. in addition , we analyze the accuracy gaps between the current and the ground truth-given situations and suggest that it is a promising direction to improve state operation prediction to boost the dst performance.
