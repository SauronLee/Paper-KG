How Can We Accelerate Progress Towards Human-like Linguistic Generalization? | Tal Linzen | this position paper describes and critiques the pretraining-agnostic identically distributed \( paid \) evaluation paradigm , which has become a central tool for measuring progress in natural language understanding. this paradigm consists of three stages: \( 1 \) pre-training of a word prediction model on a corpus of arbitrary size; \( 2 \) fine-tuning \( transfer learning \) on a training set representing a classification task; \( 3 \) evaluation on a test set drawn from the same distribution as that training set. this paradigm favors simple , low-bias architectures , which , first , can be scaled to process vast amounts of data , and second , can capture the fine-grained statistical properties of a particular data set , regardless of whether those properties are likely to generalize to examples of the task outside the data set. this contrasts with humans , who learn language from several orders of magnitude less data than the systems favored by this evaluation paradigm , and generalize to new tasks in a consistent way. we advocate for supplementing or replacing paid with paradigms that reward architectures that generalize as quickly and robustly as humans.
