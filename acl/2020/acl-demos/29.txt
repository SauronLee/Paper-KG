DIALOGPT : Large-Scale Generative Pre-training for Conversational Response Generation | Yizhe Zhang | we present a large , tunable neural conversational response generation model , dialogpt \( dialogue generative pre-trained transformer \) . trained on 147m conversation-like exchanges extracted from reddit comment chains over a period spanning from 2005 through 2017 , dialogpt extends the hugging face pytorch transformer to attain a performance close to human both in terms of automatic and human evaluation in single-turn dialogue settings. we show that conversational systems that leverage dialogpt generate more relevant , contentful and context-consistent responses than strong baseline systems. the pre-trained model and training pipeline are publicly released to facilitate research into neural response generation and the development of more intelligent open-domain dialogue systems.
