Label Noise in Context | Michael Desmond | label noise—incorrectly or ambiguously labeled training examples—can negatively impact model performance. although noise detection techniques have been around for decades , practitioners rarely apply them , as manual noise remediation is a tedious process. examples incorrectly flagged as noise waste reviewers’ time , and correcting label noise without guidance can be difficult. we propose lnic , a noise-detection method that uses an example’s neighborhood within the training set to \( a \) reduce false positives and \( b \) provide an explanation as to why the ex- ample was flagged as noise. we demonstrate on several short-text classification datasets that lnic outperforms the state of the art on measures of precision and f0.5-score. we also show how lnic’s training set context helps a reviewer to understand and correct label noise in a dataset. the lnic tool lowers the barriers to label noise remediation , increasing its utility for nlp practitioners.
