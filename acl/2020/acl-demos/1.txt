TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing | Ziqing Yang | in this paper , we introduce textbrewer , an open-source knowledge distillation toolkit designed for natural language processing. it works with different neural network models and supports various kinds of supervised learning tasks , such as text classification , reading comprehension , sequence labeling. textbrewer provides a simple and uniform workflow that enables quick setting up of distillation experiments with highly flexible configurations. it offers a set of predefined distillation methods and can be extended with custom code. as a case study , we use textbrewer to distill bert on several typical nlp tasks. with simple configurations , we achieve results that are comparable with or even higher than the public distilled bert models with similar numbers of parameters.
