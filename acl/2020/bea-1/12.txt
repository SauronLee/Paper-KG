An empirical investigation of neural methods for content scoring of science explanations | Brian Riordan | with the widespread adoption of the next generation science standards \( ngss \) , science teachers and online learning environments face the challenge of evaluating studentsâ€™ integration of different dimensions of science learning. recent advances in representation learning in natural language processing have proven effective across many natural language processing tasks , but a rigorous evaluation of the relative merits of these methods for scoring complex constructed response formative assessments has not previously been carried out. we present a detailed empirical investigation of feature-based , recurrent neural network , and pre-trained transformer models on scoring content in real-world formative assessment data. we demonstrate that recent neural methods can rival or exceed the performance of feature-based methods. we also provide evidence that different classes of neural models take advantage of different learning cues , and pre-trained transformer models may be more robust to spurious , dataset-specific learning cues , better reflecting scoring rubrics.
