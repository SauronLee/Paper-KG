Using PRMSE to evaluate automated scoring systems in the presence of label noise | Anastassia Loukina | the effect of noisy labels on the performance of nlp systems has been studied extensively for system training. in this paper , we focus on the effect that noisy labels have on system evaluation. using automated scoring as an example , we demonstrate that the quality of human ratings used for system evaluation have a substantial impact on traditional performance metrics , making it impossible to compare system evaluations on labels with different quality. we propose that a new metric , prmse , developed within the educational measurement community , can help address this issue , and provide practical guidelines on using prmse.
