Training and Inference Methods for High-Coverage Neural Machine Translation | Michael Yang | in this paper , we introduce a system built for the duolingo simultaneous translation and paraphrase for language education \( staple \) shared task at the 4th workshop on neural generation and translation \( wngt 2020 \) . we participated in the english-to-japanese track with a transformer model pretrained on the jparacrawl corpus and fine-tuned in two steps on the jesc corpus and then the \( smaller \) duolingo training corpus. first , during training , we find it is essential to deliberately expose the model to higher-quality translations more often during training for optimal translation performance. for inference , encouraging a small amount of diversity with diverse beam search to improve translation coverage yielded marginal improvement over regular beam search. finally , using an auxiliary filtering model to filter out unlikely candidates from beam search improves performance further. we achieve a weighted f1 score of 27.56% on our own test set , outperforming the staple aws translations baseline score of 4.31%.
