Growing Together: Modeling Human Language Learning With n-Best Multi-Checkpoint Machine Translation | El Moatez Billah Nagoudi | we describe our submission to the 2020 duolingo shared task on simultaneous translation and paraphrase for language education \( staple \) . we view mt models at various training stages \( i.e. , checkpoints \) as human learners at different levels. hence , we employ an ensemble of multi-checkpoints from the same model to generate translation sequences with various levels of fluency. from each checkpoint , for our best model , we sample n-best sequences \( n=10 \) with a beam width =100. we achieve an 37.57 macro f1 with a 6 checkpoint model ensemble on the official shared task test data , outperforming a baseline amazon translation system of 21.30 macro f1 and ultimately demonstrating the utility of our intuitive method.
