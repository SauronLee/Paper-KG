Edinburghâ€™s Submissions to the 2020 Machine Translation Efficiency Task | Nikolay Bogoychev | we participated in all tracks of the workshop on neural generation and translation 2020 efficiency shared task: single-core cpu , multi-core cpu , and gpu. at the model level , we use teacher-student training with a variety of student sizes , tie embeddings and sometimes layers , use the simpler simple recurrent unit , and introduce head pruning. on gpus , we used 16-bit floating-point tensor cores. on cpus , we customized 8-bit quantization and multiple processes with affinity for the multi-core setting. to reduce model size , we experimented with 4-bit log quantization but use floats at runtime. in the shared task , most of our submissions were pareto optimal with respect the trade-off between time and quality.
