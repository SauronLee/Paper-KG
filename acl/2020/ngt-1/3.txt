Compressing Neural Machine Translation Models with 4-bit Precision | Alham Fikri Aji | neural machine translation \( nmt \) is resource-intensive. we design a quantization procedure to compress fit nmt models better for devices with limited hardware capability. we use logarithmic quantization , instead of the more commonly used fixed-point quantization , based on the empirical fact that parameters distribution is not uniform. we find that biases do not take a lot of memory and show that biases can be left uncompressed to improve the overall quality without affecting the compression rate. we also propose to use an error-feedback mechanism during retraining , to preserve the compressed model as a stale gradient. we empirically show that nmt models based on transformer or rnn architecture can be compressed up to 4-bit precision without any noticeable quality degradation. models can be compressed up to binary precision , albeit with lower quality. rnn architecture seems to be more robust towards compression , compared to the transformer.
