CopyBERT: A Unified Approach to Question Generation with Self-Attention | Stalin Varanasi | contextualized word embeddings provide better initialization for neural networks that deal with various natural language understanding \( nlu \) tasks including question answering \( qa \) and more recently , question generation \( qg \) . apart from providing meaningful word representations , pre-trained transformer models \( vaswani et al. , 2017 \) , such as bert \( devlin et al. , 2019 \) also provide self-attentions which encode syntactic information that can be probed for dependency parsing \( hewitt and manning , 2019 \) and postagging \( coenen et al. , 2019 \) . in this paper , we show that the information from selfattentions of bert are useful for language modeling of questions conditioned on paragraph and answer phrases. to control the attention span , we use semi-diagonal mask and utilize a shared model for encoding and decoding , unlike sequence-to-sequence. we further employ copy-mechanism over self-attentions to acheive state-of-the-art results for question generation on squad v1.1 \( rajpurkar et al. , 2016 \) .
