Accelerating Natural Language Understanding in Task-Oriented Dialog | Ojas Ahuja | task-oriented dialog models typically leverage complex neural architectures and large-scale , pre-trained transformers to achieve state-of-the-art performance on popular natural language understanding benchmarks. however , these models frequently have in excess of tens of millions of parameters , making them impossible to deploy on-device where resource-efficiency is a major concern. in this work , we show that a simple convolutional model compressed with structured pruning achieves largely comparable results to bert on atis and snips , with under 100k parameters. moreover , we perform acceleration experiments on cpus , where we observe our multi-task model predicts intents and slots nearly 63x faster than even distilbert.
