DLGNet: A Transformer-based Model for Dialogue Response Generation | Olabiyi Oluwatobi | neural dialogue models , despite their successes , still suffer from lack of relevance , diversity , and in many cases coherence in their generated responses. on the other hand , transformer-based models such as gpt-2 have demonstrated an excellent ability to capture long-range structures in language modeling tasks. in this paper , we present dlgnet , a transformer-based model for dialogue modeling. we specifically examine the use of dlgnet for multi-turn dialogue response generation. in our experiments , we evaluate dlgnet on the open-domain movie triples dataset and the closed-domain ubuntu dialogue dataset. dlgnet models , although trained with only the maximum likelihood objective , achieve significant improvements over state-of-the-art multi-turn dialogue models. they also produce best performance to date on the two datasets based on several metrics , including bleu , rouge , and distinct n-gram. our analysis shows that the performance improvement is mostly due to the combination of \( 1 \) the long-range transformer architecture with \( 2 \) the injection of random informative paddings. other contributing factors include the joint modeling of dialogue context and response , and the 100% tokenization coverage from the byte pair encoding \( bpe \) .
