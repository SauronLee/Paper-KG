Octanove Labs’ Japanese-Chinese Open Domain Translation System | Masato Hagiwara | this paper describes octanove labs’ submission to the iwslt 2020 open domain translation challenge. in order to build a high-quality japanese-chinese neural machine translation \( nmt \) system , we use a combination of 1 \) parallel corpus filtering and 2 \) back-translation. we have shown that , by using heuristic rules and learned classifiers , the size of the parallel data can be reduced by 70% to 90% without much impact on the final mt performance. we have also shown that including the artificially generated parallel data through back-translation further boosts the metric by 17% to 27% , while self-training contributes little. aside from a small number of parallel sentences annotated for filtering , no external resources have been used to build our system.
