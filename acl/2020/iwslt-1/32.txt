Efficient Automatic Punctuation Restoration Using Bidirectional Transformers with Robust Inference | Maury Courtland | though people rarely speak in complete sentences , punctuation confers many benefits to the readers of transcribed speech. unfortunately , most asr systems do not produce punctuated output. to address this , we propose a solution for automatic punctuation that is both cost efficient and easy to train. our solution benefits from the recent trend in fine-tuning transformer-based language models. we also modify the typical framing of this task by predicting punctuation for sequences rather than individual tokens , which makes for more efficient training and inference. finally , we find that aggregating predictions across multiple context windows improves accuracy even further. our best model achieves a new state of the art on benchmark data \( ted talks \) with a combined f1 of 83.9 , representing a 48.7% relative improvement \( 15.3 absolute \) over the previous state of the art.
