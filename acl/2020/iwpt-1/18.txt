Adaptation of Multilingual Transformer Encoder for Robust Enhanced Universal Dependency Parsing | Han He | this paper presents our enhanced dependency parsing approach using transformer encoders , coupled with a simple yet powerful ensemble algorithm that takes advantage of both tree and graph dependency parsing. two types of transformer encoders are compared , a multilingual encoder and language-specific encoders. our dependency tree parsing \( dtp \) approach generates only primary dependencies to form trees whereas our dependency graph parsing \( dgp \) approach handles both primary and secondary dependencies to form graphs. since dgp does not guarantee the generated graphs are acyclic , the ensemble algorithm is designed to add secondary arcs predicted by dgp to primary arcs predicted by dtp. our results show that models using the multilingual encoder outperform ones using the language specific encoders for most languages. the ensemble models generally show higher labeled attachment score on enhanced dependencies \( elas \) than the dtp and dgp models. as the result , our best models rank the third place on the macro-average elas over 17 languages.
