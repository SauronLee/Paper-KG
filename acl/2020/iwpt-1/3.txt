Integrating Graph-Based and Transition-Based Dependency Parsers in the Deep Contextualized Era | Agnieszka Falenska | graph-based and transition-based dependency parsers used to have different strengths and weaknesses. therefore , combining the outputs of parsers from both paradigms used to be the standard approach to improve or analyze their performance. however , with the recent adoption of deep contextualized word representations , the chief weakness of graph-based models , i.e. , their limited scope of features , has been mitigated. through two popular combination techniques – blending and stacking – we demonstrate that the remaining diversity in the parsing models is reduced below the level of models trained with different random seeds. thus , an integration no longer leads to increased accuracy. when both parsers depend on bilstms , the graph-based architecture has a consistent advantage. this advantage stems from globally-trained bilstm representations , which capture more distant look-ahead syntactic relations. such representations can be exploited through multi-task learning , which improves the transition-based parser , especially on treebanks with a high ratio of right-headed dependencies.
