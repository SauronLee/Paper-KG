Distilling Neural Networks for Greener and Faster Dependency Parsing | Mark Anderson | the carbon footprint of natural language processing research has been increasing in recent years due to its reliance on large and inefficient neural network implementations. distillation is a network compression technique which attempts to impart knowledge from a large model to a smaller one. we use teacher-student distillation to improve the efficiency of the biaffine dependency parser which obtains state-of-the-art performance with respect to accuracy and parsing speed \( dozat and manning , 2017 \) . when distilling to 20% of the original model’s trainable parameters , we only observe an average decrease of ∼1 point for both uas and las across a number of diverse universal dependency treebanks while being 2.30x \( 1.19x \) faster than the baseline model on cpu \( gpu \) at inference time. we also observe a small increase in performance when compressing to 80% for some treebanks. finally , through distillation we attain a parser which is not only faster but also more accurate than the fastest modern parser on the penn treebank.
