Self-Training for Unsupervised Parsing with PRPN | Anhad Mohananey | neural unsupervised parsing \( up \) models learn to parse without access to syntactic annotations , while being optimized for another task like language modeling. in this work , we propose self-training for neural up models: we leverage aggregated annotations predicted by copies of our model as supervision for future copies. to be able to use our modelâ€™s predictions during training , we extend a recent neural up architecture , the prpn \( shen et al. , 2018a \) , such that it can be trained in a semi-supervised fashion. we then add examples with parses predicted by our model to our unlabeled up training data. our self-trained model outperforms the prpn by 8.1% f1 and the previous state of the art by 1.6% f1. in addition , we show that our architecture can also be helpful for semi-supervised parsing in ultra-low-resource settings.
