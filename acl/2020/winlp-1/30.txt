Multitask Models for Controlling the Complexity of Neural Machine Translation | Sweta Agrawal | neural language models typically employ a categorical approach to prediction and training , leading to well-known computational and numerical limitations. an under-explored alternative approach is to perform prediction directly against a continuous word embedding space , which according to recent research is more akin to how lexemes are represented in the brain. choosing this method opens the door for for large-vocabulary , language models and enables substantially smaller and simpler computational complexities. in this research we explore a different important trait - the continuous output prediction models reach low-frequency vocabulary words which we show are often ignored by the categorical model. such words are essential , as they can contribute to personalization and user vocabulary adaptation. in this work , we explore continuous-space language modeling in the context of a word prediction task over two different textual domains \( newswire text and biomedical journal articles \) . we investigate both traditional and adversarial training approaches , and report results using several different embedding spaces and decoding mechanisms. we find that our continuous-prediction approach outperforms the standard categorical approach in terms of term diversity , in particular with rare words.
