Staying True to Your Word: (How) Can Attention Become Explanation? | Martin Tutek | the attention mechanism has quickly become ubiquitous in nlp. in addition to improving performance of models , attention has been widely used as a glimpse into the inner workings of nlp models. the latter aspect has in the recent years become a common topic of discussion , most notably in recent work of jain and wallace; wiegreffe and pinter. with the shortcomings of using attention weights as a tool of transparency revealed , the attention mechanism has been stuck in a limbo without concrete proof when and whether it can be used as an explanation. in this paper , we provide an explanation as to why attention has seen rightful critique when used with recurrent networks in sequence classification tasks. we propose a remedy to these issues in the form of a word level objective and our findings give credibility for attention to provide faithful interpretations of recurrent models.
