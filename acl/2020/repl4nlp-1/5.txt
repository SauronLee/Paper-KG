Learning Geometric Word Meta-Embeddings | Pratik Jawanpuria | we propose a geometric framework for learning meta-embeddings of words from different embedding sources. our framework transforms the embeddings into a common latent space , where , for example , simple averaging or concatenation of different embeddings \( of a given word \) is more amenable. the proposed latent space arises from two particular geometric transformations - source embedding specific orthogonal rotations and a common mahalanobis metric scaling. empirical results on several word similarity and word analogy benchmarks illustrate the efficacy of the proposed framework.
