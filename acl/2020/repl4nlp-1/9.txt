Exploring the Limits of Simple Learners in Knowledge Distillation for Document Classification with DocBERT | Ashutosh Adhikari | fine-tuned variants of bert are able to achieve state-of-the-art accuracy on many natural language processing tasks , although at significant computational costs. in this paper , we verify bert’s effectiveness for document classification and investigate the extent to which bert-level effectiveness can be obtained by different baselines , combined with knowledge distillation—a popular model compression method. the results show that bert-level effectiveness can be achieved by a single-layer lstm with at least
