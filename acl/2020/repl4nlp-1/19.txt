A Cross-Task Analysis of Text Span Representations | Shubham Toshniwal | many natural language processing \( nlp \) tasks involve reasoning with textual spans , including question answering , entity recognition , and coreference resolution. while extensive research has focused on functional architectures for representing words and sentences , there is less work on representing arbitrary spans of text within sentences. in this paper , we conduct a comprehensive empirical evaluation of six span representation methods using eight pretrained language representation models across six tasks , including two tasks that we introduce. we find that , although some simple span representations are fairly reliable across tasks , in general the optimal span representation varies by task , and can also vary within different facets of individual tasks. we also find that the choice of span representation has a bigger impact with a fixed pretrained encoder than with a fine-tuned encoder.
