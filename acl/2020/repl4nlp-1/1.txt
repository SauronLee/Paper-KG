Encodings of Source Syntax: Similarities in NMT Representations Across Target Languages | Tyler A. Chang | we train neural machine translation \( nmt \) models from english to six target languages , using nmt encoder representations to predict ancestor constituent labels of source language words. we find that nmt encoders learn similar source syntax regardless of nmt target language , relying on explicit morphosyntactic cues to extract syntactic features from source sentences. furthermore , the nmt encoders outperform rnns trained directly on several of the constituent label prediction tasks , suggesting that nmt encoder representations can be used effectively for natural language tasks involving syntax. however , both the nmt encoders and the directly-trained rnns learn substantially different syntactic information from a probabilistic context-free grammar \( pcfg \) parser. despite lower overall accuracy scores , the pcfg often performs well on sentences for which the rnn-based models perform poorly , suggesting that rnn architectures are constrained in the types of syntax they can learn.
