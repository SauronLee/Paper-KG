Contextual and Non-Contextual Word Embeddings: an in-depth Linguistic Investigation | Alessio Miaschi | in this paper we present a comparison between the linguistic knowledge encoded in the internal representations of a contextual language model \( bert \) and a contextual-independent one \( word2vec \) . we use a wide set of probing tasks , each of which corresponds to a distinct sentence-level feature extracted from different levels of linguistic annotation. we show that , although bert is capable of understanding the full context of each word in an input sequence , the implicit knowledge encoded in its aggregated sentence representations is still comparable to that of a contextual-independent model. we also find that bert is able to encode sentence-level properties even within single-word embeddings , obtaining comparable or even superior results than those obtained with sentence representations.
