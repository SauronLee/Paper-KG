Zero-Resource Cross-Domain Named Entity Recognition | Zihan Liu | existing models for cross-domain named entity recognition \( ner \) rely on numerous unlabeled corpus or labeled ner training data in target domains. however , collecting data for low-resource target domains is not only expensive but also time-consuming. hence , we propose a cross-domain ner model that does not use any external resources. we first introduce a multi-task learning \( mtl \) by adding a new objective function to detect whether tokens are named entities or not. we then introduce a framework called mixture of entity experts \( moee \) to improve the robustness for zero-resource domain adaptation. finally , experimental results show that our model outperforms strong unsupervised cross-domain sequence labeling models , and the performance of our model is close to that of the state-of-the-art model which leverages extensive resources.
