Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning | Mitchell Gordon | pre-trained universal feature extractors , such as bert for natural language processing and vgg for computer vision , have become effective methods for improving deep learning models without requiring more labeled data. while effective , feature extractors like bert may be prohibitively large for some deployment scenarios. we explore weight pruning for bert and ask: how does compression during pre-training affect transfer learning \? we find that pruning affects transfer learning in three broad regimes. low levels of pruning \( 30-40% \) do not affect pre-training loss or transfer to downstream tasks at all. medium levels of pruning increase the pre-training loss and prevent useful pre-training information from being transferred to downstream tasks. high levels of pruning additionally prevent models from fitting downstream datasets , leading to further degradation. finally , we observe that fine-tuning bert on a specific task does not improve its prunability. we conclude that bert can be pruned once during pre-training rather than separately for each task without affecting performance.
