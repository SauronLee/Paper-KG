Are All Languages Created Equal in Multilingual BERT? | Shijie Wu | multilingual bert \( mbert \) trained on 104 languages has shown surprisingly good cross-lingual performance on several nlp tasks , even without explicit cross-lingual signals. however , these evaluations have focused on cross-lingual transfer with high-resource languages , covering only a third of the languages covered by mbert. we explore how mbert performs on a much wider set of languages , focusing on the quality of representation for low-resource languages , measured by within-language performance. we consider three tasks: named entity recognition \( 99 languages \) , part-of-speech tagging and dependency parsing \( 54 languages each \) . mbert does better than or comparable to baselines on high resource languages but does much worse for low resource languages. furthermore , monolingual bert models for these languages do even worse. paired with similar languages , the performance gap between monolingual bert and mbert can be narrowed. we find that better models for low resource languages require more efficient pretraining techniques or more data.
