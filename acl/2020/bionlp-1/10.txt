Evaluating the Utility of Model Configurations and Data Augmentation on Clinical Semantic Textual Similarity | Yuxia Wang | in this paper , we apply pre-trained language models to the semantic textual similarity \( sts \) task , with a specific focus on the clinical domain. in low-resource setting of clinical sts , these large models tend to be impractical and prone to overfitting. building on bert , we study the impact of a number of model design choices , namely different fine-tuning and pooling strategies. we observe that the impact of domain-specific fine-tuning on clinical sts is much less than that in the general domain , likely due to the concept richness of the domain. based on this , we propose two data augmentation techniques. experimental results on n2c2-sts 1 demonstrate substantial improvements , validating the utility of the proposed methods.
