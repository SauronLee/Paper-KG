A BERT-based One-Pass Multi-Task Model for Clinical Temporal Relation Extraction | Chen Lin | recently bert has achieved a state-of-the-art performance in temporal relation extraction from clinical electronic medical records text. however , the current approach is inefficient as it requires multiple passes through each input sequence. we extend a recently-proposed one-pass model for relation classification to a one-pass model for relation extraction. we augment this framework by introducing global embeddings to help with long-distance relation inference , and by multi-task learning to increase model performance and generalizability. our proposed model produces results on par with the state-of-the-art in temporal relation extraction on the thyme corpus and is much “greener” in computational cost.
