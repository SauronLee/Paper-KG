An Empirical Study of Multi-Task Learning on BERT for Biomedical Text Mining | Yifan Peng | multi-task learning \( mtl \) has achieved remarkable success in natural language processing applications. in this work , we study a multi-task learning model with multiple decoders on varieties of biomedical and clinical natural language processing tasks such as text similarity , relation extraction , named entity recognition , and text inference. our empirical results demonstrate that the mtl fine-tuned models outperform state-of-the-art transformer models \( e.g. , bert and its variants \) by 2.0% and 1.3% in biomedical and clinical domain adaptation , respectively. pairwise mtl further demonstrates more details about which tasks can improve or decrease others. this is particularly helpful in the context that researchers are in the hassle of choosing a suitable model for new problems. the code and models are publicly available at https://github.com/ncbi-nlp/bluebert.
