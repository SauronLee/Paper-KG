SCAR: Sentence Compression using Autoencoders for Reconstruction | Chanakya Malireddy | sentence compression is the task of shortening a sentence while retaining its meaning. most methods proposed for this task rely on labeled or paired corpora \( containing pairs of verbose and compressed sentences \) , which is often expensive to collect. to overcome this limitation , we present a novel unsupervised deep learning framework \( scar \) for deletion-based sentence compression. scar is primarily composed of two encoder-decoder pairs: a compressor and a reconstructor. the compressor masks the input , and the reconstructor tries to regenerate it. the model is entirely trained on unlabeled data and does not require additional inputs such as explicit syntactic information or optimal compression length. scarâ€™s merit lies in the novel linkage loss function , which correlates the compressor and its effect on reconstruction , guiding it to drop inferable tokens. scar achieves higher rouge scores on benchmark datasets than the existing state-of-the-art methods and baselines. we also conduct a user study to demonstrate the application of our model as a text highlighting system. using our model to underscore salient information facilitates speed-reading and reduces the time required to skim a document.
