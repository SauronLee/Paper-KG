Considering Likelihood in NLP Classification Explanations with Occlusion and Language Modeling | David Harbecke | recently , state-of-the-art nlp models gained an increasing syntactic and semantic understanding of language , and explanation methods are crucial to understand their decisions. occlusion is a well established method that provides explanations on discrete language data , e.g. by removing a language unit from an input and measuring the impact on a modelâ€™s decision. we argue that current occlusion-based methods often produce invalid or syntactically incorrect language data , neglecting the improved abilities of recent nlp models. furthermore , gradient-based explanation methods disregard the discrete distribution of data in nlp. thus , we propose olm: a novel explanation method that combines occlusion and language models to sample valid and syntactically correct replacements with high likelihood , given the context of the original input. we lay out a theoretical foundation that alleviates these weaknesses of other explanation methods in nlp and provide results that underline the importance of considering data likelihood in occlusion-based explanation.
