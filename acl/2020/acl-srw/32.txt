Unsupervised Multilingual Sentence Embeddings for Parallel Corpus Mining | Ivana Kvapilíková | long short-term memory \( lstm \) networks and their variants are capable of encapsulating long-range dependencies , which is evident from their performance on a variety of linguistic tasks. on the other hand , simple recurrent networks \( srns \) , which appear more biologically grounded in terms of synaptic connections , have generally been less successful at capturing long-range dependencies as well as the loci of grammatical errors in an unsupervised setting. in this paper , we seek to develop models that bridge the gap between biological plausibility and linguistic competence. we propose a new architecture , the decay rnn , which incorporates the decaying nature of neuronal activations and models the excitatory and inhibitory connections in a population of neurons. besides its biological inspiration , our model also shows competitive performance relative to lstms on subject-verb agreement , sentence grammaticality , and language modeling tasks. these results provide some pointers towards probing the nature of the inductive biases required for rnn architectures to model linguistic phenomena successfully.
