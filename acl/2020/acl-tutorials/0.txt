Interpretability and Analysis in Neural NLP | Yonatan Belinkov | while deep learning has transformed the natural language processing \( nlp \) field and impacted the larger computational linguistics community , the rise of neural networks is stained by their opaque nature: it is challenging to interpret the inner workings of neural network models , and explicate their behavior. therefore , in the last few years , an increasingly large body of work has been devoted to the analysis and interpretation of neural network models in nlp. this body of work is so far lacking a common framework and methodology. moreover , approaching the analysis of modern neural networks can be difficult for newcomers to the field. this tutorial aims to fill this gap and introduce the nascent field of interpretability and analysis of neural networks in nlp. the tutorial will cover the main lines of analysis work , such as structural analyses using probing classifiers , behavioral studies and test suites , and interactive visualizations. we will highlight not only the most commonly applied analysis methods , but also the specific limitations and shortcomings of current approaches , in order to inform participants where to focus future efforts.
