Language Models as Fact Checkers? | Nayeon Lee | recent work has suggested that language models \( lms \) store both common-sense and factual knowledge learned from pre-training data. in this paper , we leverage this implicit knowledge to create an effective end-to-end fact checker using a solely a language model , without any external knowledge or explicit retrieval components. while previous work on extracting knowledge from lms have focused on the task of open-domain question answering , to the best of our knowledge , this is the first work to examine the use of language models as fact checkers. in a closed-book setting , we show that our zero-shot lm approach outperforms a random baseline on the standard fever task , and that our finetuned lm compares favorably with standard baselines. though we do not ultimately outperform methods which use explicit knowledge bases , we believe our exploration shows that this method is viable and has much room for exploration.
