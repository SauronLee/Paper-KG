One-Size-Fits-All Multilingual Models | Ben Peters | this paper presents deepspin’s submissions to tasks 0 and 1 of the sigmorphon 2020 shared task. for both tasks , we present multilingual models , training jointly on data in all languages. we perform no language-specific hyperparameter tuning – each of our submissions uses the same model for all languages. our basic architecture is the sparse sequence-to-sequence model with entmax attention and loss , which allows our models to learn sparse , local alignments while still being trainable with gradient-based techniques. for task 1 , we achieve strong performance with both rnn- and transformer-based sparse models. for task 0 , we extend our rnn-based model to a multi-encoder set-up in which separate modules encode the lemma and inflection sequences. despite our models’ lack of language-specific tuning , they tie for first in task 0 and place third in task 1.
