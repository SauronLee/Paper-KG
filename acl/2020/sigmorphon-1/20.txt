Data Augmentation for Transformer-based G2P | Zach Ryan | the transformer model has been shown to outperform other neural seq2seq models in several character-level tasks. it is unclear , however , if the transformer would benefit as much as other seq2seq models from data augmentation strategies in the low-resource setting. in this paper we explore strategies for data augmentation in the g2p task together with the transformer model. our results show that a relatively simple alignment-based strategy of identifying consistent input-output subsequences in grapheme-phoneme data coupled together with a subsequent splicing together of such pieces to generate hallucinated data works well in the low-resource setting , often delivering substantial performance improvement over a standard transformer model.
