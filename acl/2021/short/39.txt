PRAL: A Tailored Pre-Training Model for Task-Oriented Dialog Generation | Jing Gu | large pre-trained language generation models such as gpt-2 have demonstrated their effectiveness as language priors by reaching state-of-the-art results in various language generation tasks. however , the performance of pre-trained models on task-oriented dialog tasks is still under-explored. we propose a pre-trainedrole alternating language model \( pral \) , explicitly designed for task-oriented conversational systems. we design several techniques: start position randomization , knowledge distillation , and history discount to improve pre-training performance. in addition , we introduce a high-quality large-scale task-oriented dialog pre-training dataset by post-prossessing13 dialog datasets. we effectively adapt pralon three downstream tasks. the results show that pral outperforms or is on par with state-of-the-art models.
