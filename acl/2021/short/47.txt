On Orthogonality Constraints for Transformers | Aston Zhang | orthogonality constraints encourage matrices to be orthogonal for numerical stability. these plug-and-play constraints , which can be conveniently incorporated into model training , have been studied for popular architectures in natural language processing , such as convolutional neural networks and recurrent neural networks. however , a dedicated study on such constraints for transformers has been absent. to fill this gap , this paper studies orthogonality constraints for transformers , showing the effectiveness with empirical evidence from ten machine translation tasks and two dialogue generation tasks. for example , on the large-scale wmt’16 en→de benchmark , simply plugging-and-playing orthogonality constraints on the original transformer model \( vaswani et al. , 2017 \) increases the bleu from 28.4 to 29.6 , coming close to the 29.7 bleu achieved by the very competitive dynamic convolution \( wu et al. , 2019 \) .
