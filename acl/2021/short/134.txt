SimCLS: A Simple Framework for Contrastive Learning of Abstractive Summarization | Yixin Liu | in this paper , we present a conceptually simple while empirically powerful framework for abstractive summarization , simcls , which can bridge the gap between the learning objective and evaluation metrics resulting from the currently dominated sequence-to-sequence learning framework by formulating text generation as a reference-free evaluation problem \( i.e. , quality estimation \) assisted by contrastive learning. experimental results show that , with minor modification over existing top-scoring systems , simcls can improve the performance of existing top-performing models by a large margin. particularly , 2.51 absolute improvement against bart and 2.50 over pegasus w.r.t rouge-1 on the cnn/dailymail dataset , driving the state-of-the-art performance to a new level. we have open-sourced our codes and results: https://github.com/yixinl7/simcls. results of our proposed models have been deployed into explainaboard platform , which allows researchers to understand our systems in a more fine-grained way.
