Towards a more Robust Evaluation for Conversational Question Answering | Wissam Siblini | with the explosion of chatbot applications , conversational question answering \( cqa \) has generated a lot of interest in recent years. among proposals , reading comprehension models which take advantage of the conversation history \( previous qa \) seem to answer better than those which only consider the current question. nevertheless , we note that the cqa evaluation protocol has a major limitation. in particular , models are allowed , at each turn of the conversation , to access the ground truth answers of the previous turns. not only does this severely prevent their applications in fully autonomous chatbots , it also leads to unsuspected biases in their behavior. in this paper , we highlight this effect and propose new tools for evaluation and training in order to guard against the noted issues. the new results that we bring come to reinforce methods of the current state of the art.
