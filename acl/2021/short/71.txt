Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking | Fangyu Liu | injecting external domain-specific knowledge \( e.g. , umls \) into pretrained language models \( lms \) advances their capability to handle specialised in-domain tasks such as biomedical entity linking \( bel \) . however , such abundant expert knowledge is available only for a handful of languages \( e.g. , english \) . in this work , by proposing a novel cross-lingual biomedical entity linking task \( xl-bel \) and establishing a new xl-bel benchmark spanning 10 typologically diverse languages , we first investigate the ability of standard knowledge-agnostic as well as knowledge-enhanced monolingual and multilingual lms beyond the standard monolingual english bel task. the scores indicate large gaps to english performance. we then address the challenge of transferring domain-specific knowledge in resource-rich languages to resource-poor ones. to this end , we propose and evaluate a series of cross-lingual transfer methods for the xl-bel task , and demonstrate that general-domain bitext helps propagate the available english knowledge to languages with little to no in-domain data. remarkably , we show that our proposed domain-specific transfer methods yield consistent gains across all target languages , sometimes up to 20 precision@1 points , without any in-domain knowledge in the target language , and without any in-domain parallel data.
