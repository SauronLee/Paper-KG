Attention Flows are Shapley Value Explanations | Kawin Ethayarajh | shapley values , a solution to the credit assignment problem in cooperative game theory , are a popular type of explanation in machine learning , having been used to explain the importance of features , embeddings , and even neurons. in nlp , however , leave-one-out and attention-based explanations still predominate. can we draw a connection between these different methods \? we formally prove that — save for the degenerate case — attention weights and leave-one-out values cannot be shapley values. attention flow is a post-processed variant of attention weights obtained by running the max-flow algorithm on the attention graph. perhaps surprisingly , we prove that attention flows are indeed shapley values , at least at the layerwise level. given the many desirable theoretical qualities of shapley values — which has driven their adoption among the ml community — we argue that nlp practitioners should , when possible , adopt attention flow explanations alongside more traditional ones.
