Constructing Multi-Modal Dialogue Dataset by Replacing Text with Semantically Relevant Images | Nyoungwoo Lee | in multi-modal dialogue systems , it is important to allow the use of images as part of a multi-turn conversation. training such dialogue systems generally requires a large-scale dataset consisting of multi-turn dialogues that involve images , but such datasets rarely exist. in response , this paper proposes a 45k multi-modal dialogue dataset created with minimal human intervention. our method to create such a dataset consists of \( 1 \) preparing and pre-processing text dialogue datasets , \( 2 \) creating image-mixed dialogues by using a text-to-image replacement technique , and \( 3 \) employing a contextual-similarity-based filtering step to ensure the contextual coherence of the dataset. to evaluate the validity of our dataset , we devise a simple retrieval model for dialogue sentence prediction tasks. automatic metrics and human evaluation results on such tasks show that our dataset can be effectively used as training data for multi-modal dialogue systems which require an understanding of images and text in a context-aware manner. our dataset and generation code is available at https://github.com/shh1574/multi-modal-dialogue-dataset.
