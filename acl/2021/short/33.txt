A Semantics-aware Transformer Model of Relation Linking for Knowledge Base Question Answering | Tahira Naseem | relation linking is a crucial component of knowledge base question answering systems. existing systems use a wide variety of heuristics , or ensembles of multiple systems , heavily relying on the surface question text. however , the explicit semantic parse of the question is a rich source of relation information that is not taken advantage of. we propose a simple transformer-based neural model for relation linking that leverages the amr semantic parse of a sentence. our system significantly outperforms the state-of-the-art on 4 popular benchmark datasets. these are based on either dbpedia or wikidata , demonstrating that our approach is effective across kgs.
