Lightweight Adapter Tuning for Multilingual Speech Translation | Hang Le | adapter modules were recently introduced as an efficient alternative to fine-tuning in nlp. adapter tuning consists in freezing pre-trained parameters of a model and injecting lightweight modules between layers , resulting in the addition of only a small number of task-specific trainable parameters. while adapter tuning was investigated for multilingual neural machine translation , this paper proposes a comprehensive analysis of adapters for multilingual speech translation \( st \) . starting from different pre-trained models \( a multilingual st trained on parallel data or a multilingual bart \( mbart \) trained on non parallel multilingual data \) , we show that adapters can be used to: \( a \) efficiently specialize st to specific language pairs with a low extra cost in terms of parameters , and \( b \) transfer from an automatic speech recognition \( asr \) task and an mbart pre-trained model to a multilingual st task. experiments show that adapter tuning offer competitive results to full fine-tuning , while being much more parameter-efficient.
