A Simple Recipe for Multilingual Grammatical Error Correction | Sascha Rothe | this paper presents a simple recipe to trainstate-of-the-art multilingual grammatical error correction \( gec \) models. we achieve this by first proposing a language-agnostic method to generate a large number of synthetic examples. the second ingredient is to use large-scale multilingual language models \( up to 11b parameters \) . once fine-tuned on language-specific supervised sets we surpass the previous state-of-the-art results on gec benchmarks in four languages: english , czech , german and russian. having established a new set of baselines for gec , we make our results easily reproducible and accessible by releasing a clang-8 dataset. it is produced by using our best model , which we call gt5 , to clean the targets of a widely used yet noisy lang-8 dataset. clang-8 greatly simplifies typical gec training pipelines composed of multiple fine-tuning stages â€“ we demonstrate that performing a single fine-tuning stepon clang-8 with the off-the-shelf language models yields further accuracy improvements over an already top-performing gt5 model for english.
