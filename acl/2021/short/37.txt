eMLM: A New Pre-training Objective for Emotion Related Tasks | Tiberiu Sosea | bert has been shown to be extremely effective on a wide variety of natural language processing tasks , including sentiment analysis and emotion detection. however , the proposed pretraining objectives of bert do not induce any sentiment or emotion-specific biases into the model. in this paper , we present emotion masked language modelling , a variation of masked language modelling aimed at improving the bert language representation model for emotion detection and sentiment analysis tasks. using the same pre-training corpora as the original model , wikipedia and bookcorpus , our bert variation manages to improve the downstream performance on 4 tasks from emotion detection and sentiment analysis by an average of 1.2% f-1. moreover , our approach shows an increased performance in our task-specific robustness tests.
