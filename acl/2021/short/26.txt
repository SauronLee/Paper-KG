How effective is BERT without word ordering? Implications for language understanding and data privacy | Jack Hessel | ordered word sequences contain the rich structures that define language. however , itâ€™s often not clear if or how modern pretrained language models utilize these structures. we show that the token representations and self-attention activations within bert are surprisingly resilient to shuffling the order of input tokens , and that for several glue language understanding tasks , shuffling only minimally degrades performance , e.g. , by 4% for qnli. while bleak from the perspective of language understanding , our results have positive implications for cases where copyright or ethics necessitates the consideration of bag-of-words data \( vs. full documents \) . we simulate such a scenario for three sensitive classification tasks , demonstrating minimal performance degradation vs. releasing full language sequences.
