Learning to Solve NLP Tasks in an Incremental Number of Languages | Giuseppe Castellucci | in real scenarios , a multilingual model trained to solve nlp tasks on a set of languages can be required to support new languages over time. unfortunately , the straightforward retraining on a dataset containing annotated examples for all the languages is both expensive and time-consuming , especially when the number of target languages grows. moreover , the original annotated material may no longer be available due to storage or business constraints. re-training only with the new language data will inevitably result in catastrophic forgetting of previously acquired knowledge. we propose a continual learning strategy that updates a model to support new languages over time , while maintaining consistent results on previously learned languages. we define a teacher-student framework where the existing model “teaches” to a student model its knowledge about the languages it supports , while the student is also trained on a new language. we report an experimental evaluation in several tasks including sentence classification , relational learning and sequence labeling.
