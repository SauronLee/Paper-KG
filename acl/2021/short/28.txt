UMIC: An Unreferenced Metric for Image Captioning via Contrastive Learning | Hwanhee Lee | despite the success of various text generation metrics such as bertscore , it is still difficult to evaluate the image captions without enough reference captions due to the diversity of the descriptions. in this paper , we introduce a new metric umic , an unreferenced metric for image captioning which does not require reference captions to evaluate image captions. based on vision-and-language bert , we train umic to discriminate negative captions via contrastive learning. also , we observe critical problems of the previous benchmark dataset \( i.e. , human annotations \) on image captioning metric , and introduce a new collection of human annotations on the generated captions. we validate umic on four datasets , including our new dataset , and show that umic has a higher correlation than all previous metrics that require multiple references.
