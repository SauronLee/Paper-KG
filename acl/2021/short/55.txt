Exploration and Exploitation: Two Ways to Improve Chinese Spelling Correction Models | Chong Li | a sequence-to-sequence learning with neural networks has empirically proven to be an effective framework for chinese spelling correction \( csc \) , which takes a sentence with some spelling errors as input and outputs the corrected one. however , csc models may fail to correct spelling errors covered by the confusion sets , and also will encounter unseen ones. we propose a method , which continually identifies the weak spots of a model to generate more valuable training instances , and apply a task-specific pre-training strategy to enhance the model. the generated adversarial examples are gradually added to the training set. experimental results show that such an adversarial training method combined with the pre-training strategy can improve both the generalization and robustness of multiple csc models across three different datasets , achieving state-of-the-art performance for csc task.
