Can Transformer Models Measure Coherence In Text: Re-Thinking the Shuffle Test | Philippe Laban | the shuffle test is the most common task to evaluate whether nlp models can measure coherence in text. most recent work uses direct supervision on the task; we show that by simply finetuning a roberta model , we can achieve a near perfect accuracy of 97.8% , a state-of-the-art. we argue that this outstanding performance is unlikely to lead to a good model of text coherence , and suggest that the shuffle test should be approached in a zero-shot setting: models should be evaluated without being trained on the task itself. we evaluate common models in this setting , such as generative and bi-directional transformers , and find that larger architectures achieve high-performance out-of-the-box. finally , we suggest the k-block shuffle test , a modification of the original by increasing the size of blocks shuffled. even though human reader performance remains high \( around 95% accuracy \) , model performance drops from 94% to 78% as block size increases , creating a conceptually simple challenge to benchmark nlp models.
