Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling | Chuhan Wu | transformer is important for text modeling. however , it has difficulty in handling long documents due to the quadratic complexity with input text length. in order to handle this problem , we propose a hierarchical interactive transformer \( hi-transformer \) for efficient and effective long document modeling. hi-transformer models documents in a hierarchical way , i.e. , first learns sentence representations and then learns document representations. it can effectively reduce the complexity and meanwhile capture global document context in the modeling of each sentence. more specifically , we first use a sentence transformer to learn the representations of each sentence. then we use a document transformer to model the global document context from these sentence representations. next , we use another sentence transformer to enhance sentence modeling using the global document context. finally , we use hierarchical pooling method to obtain document embedding. extensive experiments on three benchmark datasets validate the efficiency and effectiveness of hi-transformer in long document modeling.
