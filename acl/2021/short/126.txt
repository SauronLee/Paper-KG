Unsupervised Cross-Domain Prerequisite Chain Learning using Variational Graph Autoencoders | Irene Li | learning prerequisite chains is an important task for one to pick up knowledge efficiently in both known and unknown domains. for example , one may be an expert in the natural language processing \( nlp \) domain , but want to determine the best order in which to learn new concepts in an unfamiliar computer vision domain \( cv \) . both domains share some common concepts , such as machine learning basics and deep learning models. in this paper , we solve the task of unsupervised cross-domain concept prerequisite chain learning , using an optimized variational graph autoencoder. our model learns to transfer concept prerequisite relations from an information-rich domain \( source domain \) to an information-poor domain \( target domain \) , substantially surpassing other baseline models. in addition , we expand an existing dataset by introducing two new domainsâ€”-cv and bioinformatics \( bio \) . the annotated data and resources as well as the code will be made publicly available.
