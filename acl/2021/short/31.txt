Higher-order Derivatives of Weighted Finite-state Machines | Ran Zmigrod | weighted finite-state machines are a fundamental building block of nlp systems. they have withstood the test of time—from their early use in noisy channel models in the 1990s up to modern-day neurally parameterized conditional random fields. this work examines the computation of higher-order derivatives with respect to the normalization constant for weighted finite-state machines. we provide a general algorithm for evaluating derivatives of all orders , which has not been previously described in the literature. in the case of second-order derivatives , our scheme runs in the optimal o \( aˆ2 nˆ4 \) time where a is the alphabet size and n is the number of states. our algorithm is significantly faster than prior algorithms. additionally , our approach leads to a significantly faster algorithm for computing second-order expectations , such as covariance matrices and gradients of first-order expectations.
