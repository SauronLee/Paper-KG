Cross-lingual Text Classification with Heterogeneous Graph Neural Network | Ziyun Wang | cross-lingual text classification aims at training a classifier on the source language and transferring the knowledge to target languages , which is very useful for low-resource languages. recent multilingual pretrained language models \( mplm \) achieve impressive results in cross-lingual classification tasks , but rarely consider factors beyond semantic similarity , causing performance degradation between some language pairs. in this paper we propose a simple yet effective method to incorporate heterogeneous information within and across languages for cross-lingual text classification using graph convolutional networks \( gcn \) . in particular , we construct a heterogeneous graph by treating documents and words as nodes , and linking nodes with different relations , which include part-of-speech roles , semantic similarity , and document translations. extensive experiments show that our graph-based method significantly outperforms state-of-the-art models on all tasks , and also achieves consistent performance gain over baselines in low-resource settings where external tools like translators are unavailable.
