The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models | Ulme Wennberg | mechanisms for encoding positional information are central for transformer-based language models. in this paper , we analyze the position embeddings of existing language models , finding strong evidence of translation invariance , both for the embeddings themselves and for their effect on self-attention. the degree of translation invariance increases during training and correlates positively with model performance. our findings lead us to propose translation-invariant self-attention \( tisa \) , which accounts for the relative position between tokens in an interpretable fashion without needing conventional position embeddings. our proposal has several theoretical advantages over existing position-representation approaches. proof-of-concept experiments show that it improves on regular albert on glue tasks , while only adding orders of magnitude less positional parameters.
