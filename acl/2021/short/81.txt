Learning to Generate Task-Specific Adapters from Task Description | Qinyuan Ye | pre-trained text-to-text transformers such as bart have achieved impressive performance across a range of nlp tasks. recent study further shows that they can learn to generalize to novel tasks , by including task descriptions as part of the source sequence and training the model with \( source , target \) examples. at test time , these fine-tuned models can make inferences on new tasks using the new task descriptions as part of the input. however , this approach has potential limitations , as the model learns to solve individual \( source , target \) examples \( i.e. , at the instance level \) , instead of learning to solve tasks by taking all examples within a task as a whole \( i.e. , at the task level \) . to this end , we introduce hypter , a framework that improves text-to-text transformerâ€™s generalization ability to unseen tasks by training a hypernetwork to generate task-specific , light-weight adapters from task descriptions. experiments on zest dataset and a synthetic squad dataset demonstrate that hypter improves upon fine-tuning baselines. notably , when using bart-large as the main network , hypter brings 11.3% comparative improvement on zest dataset.
