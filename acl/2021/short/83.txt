Domain-Adaptive Pretraining Methods for Dialogue Understanding | Han Wu | language models like bert and spanbert pretrained on open-domain data have obtained impressive gains on various nlp tasks. in this paper , we probe the effectiveness of domain-adaptive pretraining objectives on downstream tasks. in particular , three objectives , including a novel objective focusing on modeling predicate-argument relations , are evaluated on two challenging dialogue understanding tasks. experimental results demonstrate that domain-adaptive pretraining with proper objectives can significantly improve the performance of a strong baseline on these tasks , achieving the new state-of-the-art performances.
