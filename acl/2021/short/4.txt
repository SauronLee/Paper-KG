Difficulty-Aware Machine Translation Evaluation | Runzhe Zhan | the high-quality translation results produced by machine translation \( mt \) systems still pose a huge challenge for automatic evaluation. current mt evaluation pays the same attention to each sentence component , while the questions of real-world examinations \( e.g. , university examinations \) have different difficulties and weightings. in this paper , we propose a novel difficulty-aware mt evaluation metric , expanding the evaluation dimension by taking translation difficulty into consideration. a translation that fails to be predicted by most mt systems will be treated as a difficult one and assigned a large weight in the final score function , and conversely. experimental results on the wmt19 english-german metrics shared tasks show that our proposed method outperforms commonly used mt metrics in terms of human correlation. in particular , our proposed method performs well even when all the mt systems are very competitive , which is when most existing metrics fail to distinguish between them. the source code is freely available at https://github.com/nlp2ct/difficulty-aware-mt-evaluation.
