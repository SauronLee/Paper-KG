Using Adversarial Attacks to Reveal the Statistical Bias in Machine Reading Comprehension Models | Jieyu Lin | pre-trained language models have achieved human-level performance on many machine reading comprehension \( mrc \) tasks , but it remains unclear whether these models truly understand language or answer questions by exploiting statistical biases in datasets. here , we demonstrate a simple yet effective method to attack mrc models and reveal the statistical biases in these models. we apply the method to the race dataset , for which the answer to each mrc question is selected from 4 options. it is found that several pre-trained language models , including bert , albert , and roberta , show consistent preference to some options , even when these options are irrelevant to the question. when interfered by these irrelevant options , the performance of mrc models can be reduced from human-level performance to the chance-level performance. human readers , however , are not clearly affected by these irrelevant options. finally , we propose an augmented training method that can greatly reduce modelsâ€™ statistical biases.
