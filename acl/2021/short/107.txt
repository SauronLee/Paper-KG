Robust Transfer Learning with Pretrained Language Models through Adapters | Wenjuan Han | transfer learning with large pretrained transformer-based language models like bert has become a dominating approach for most nlp tasks. simply fine-tuning those large language models on downstream tasks or combining it with task-specific pretraining is often not robust. in particular , the performance considerably varies as the random seed changes or the number of pretraining and/or fine-tuning iterations varies , and the fine-tuned model is vulnerable to adversarial attack. we propose a simple yet effective adapter-based approach to mitigate these issues. specifically , we insert small bottleneck layers \( i.e. , adapter \) within each layer of a pretrained model , then fix the pretrained layers and train the adapter layers on the downstream task data , with \( 1 \) task-specific unsupervised pretraining and then \( 2 \) task-specific supervised training \( e.g. , classification , sequence labeling \) . our experiments demonstrate that such a training scheme leads to improved stability and adversarial robustness in transfer learning to various downstream tasks.
