VAULT: VAriable Unified Long Text Representation for Machine Reading Comprehension | Haoyang Wen | existing models on machine reading comprehension \( mrc \) require complex model architecture for effectively modeling long texts with paragraph representation and classification , thereby making inference computationally inefficient for production use. in this work , we propose vault: a light-weight and parallel-efficient paragraph representation for mrc based on contextualized representation from long document input , trained using a new gaussian distribution-based objective that pays close attention to the partially correct instances that are close to the ground-truth. we validate our vault architecture showing experimental results on two benchmark mrc datasets that require long context modeling; one wikipedia-based \( natural questions \( nq \) \) and the other on technotes \( techqa \) . vault can achieve comparable performance on nq with a state-of-the-art \( sota \) complex document modeling approach while being 16 times faster , demonstrating the efficiency of our proposed model. we also demonstrate that our model can also be effectively adapted to a completely different domain – techqa – with large improvement over a model fine-tuned on a previously published large plm.
