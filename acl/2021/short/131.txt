Avoiding Overlap in Data Augmentation for AMR-to-Text Generation | Wenchao Du | leveraging additional unlabeled data to boost model performance is common practice in machine learning and natural language processing. for generation tasks , if there is overlap between the additional data and the target text evaluation data , then training on the additional data is training on answers of the test set. this leads to overly-inflated scores with the additional data compared to real-world testing scenarios and problems when comparing models. we study the amr dataset and gigaword , which is popularly used for improving amr-to-text generators , and find significant overlap between gigaword and a subset of the amr dataset. we propose methods for excluding parts of gigaword to remove this overlap , and show that our approach leads to a more realistic evaluation of the task of amr-to-text generation. going forward , we give simple best-practice recommendations for leveraging additional data in amr-to-text generation.
