Strength in Numbers: Averaging and Clustering Effects in Mixture of Experts for Graph-Based Dependency Parsing | Xudong Zhang | we review two features of mixture of experts \( moe \) models which we call averaging and clustering effects in the context of graph-based dependency parsers learned in a supervised probabilistic framework. averaging corresponds to the ensemble combination of parsers and is responsible for variance reduction which helps stabilizing and improving parsing accuracy. clustering describes the capacity of moe models to give more credit to experts believed to be more accurate given an input. although promising , this is difficult to achieve , especially without additional data. we design an experimental set-up to study the impact of these effects. whereas averaging is always beneficial , clustering requires good initialization and stabilization techniques , but its advantages over mere averaging seem to eventually vanish when enough experts are present. as a by product , we show how this leads to state-of-the-art results on the ptb and the conll09 chinese treebank , with low variance across experiments.
