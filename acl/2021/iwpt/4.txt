Levi Graph AMR Parser using Heterogeneous Attention | Han He | coupled with biaffine decoders , transformers have been effectively adapted to text-to-graph transduction and achieved state-of-the-art performance on amr parsing. many prior works , however , rely on the biaffine decoder for either or both arc and label predictions although most features used by the decoder may be learned by the transformer already. this paper presents a novel approach to amr parsing by combining heterogeneous data \( tokens , concepts , labels \) as one input to a transformer to learn attention , and use only attention matrices from the transformer to predict all elements in amr graphs \( concepts , arcs , labels \) . although our models use significantly fewer parameters than the previous state-of-the-art graph parser , they show similar or better accuracy on amr 2.0 and 3.0.
