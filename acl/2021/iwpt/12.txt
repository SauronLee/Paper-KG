Applying Occam’s Razor to Transformer-Based Dependency Parsing: What Works, What Doesn’t, and What is Really Necessary | Stefan Grünewald | the introduction of pre-trained transformer-based contextualized word embeddings has led to considerable improvements in the accuracy of graph-based parsers for frameworks such as universal dependencies \( ud \) . however , previous works differ in various dimensions , including their choice of pre-trained language models and whether they use lstm layers. with the aims of disentangling the effects of these choices and identifying a simple yet widely applicable architecture , we introduce steps , a new modular graph-based dependency parser. using steps , we perform a series of analyses on the ud corpora of a diverse set of languages. we find that the choice of pre-trained embeddings has by far the greatest impact on parser performance and identify xlm-r as a robust choice across the languages in our study. adding lstm layers provides no benefits when using transformer-based embeddings. a multi-task training setup outputting additional ud features may contort results. taking these insights together , we propose a simple but widely applicable parser architecture and configuration , achieving new state-of-the-art results \( in terms of las \) for 10 out of 12 diverse languages.
