TGIF: Tree-Graph Integrated-Format Parser for Enhanced UD with Two-Stage Generic- to Individual-Language Finetuning | Tianze Shi | we present our contribution to the iwpt 2021 shared task on parsing into enhanced universal dependencies. our main system component is a hybrid tree-graph parser that integrates \( a \) predictions of spanning trees for the enhanced graphs with \( b \) additional graph edges not present in the spanning trees. we also adopt a finetuning strategy where we first train a language-generic parser on the concatenation of data from all available languages , and then , in a second step , finetune on each individual language separately. additionally , we develop our own complete set of pre-processing modules relevant to the shared task , including tokenization , sentence segmentation , and multiword token expansion , based on pre-trained xlm-r models and our own pre-training of character-level language models. our submission reaches a macro-average elas of 89.24 on the test set. it ranks top among all teams , with a margin of more than 2 absolute elas over the next best-performing submission , and best score on 16 out of 17 languages.
