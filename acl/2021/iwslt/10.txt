End-to-End Speech Translation with Pre-trained Models and Adapters: UPC at IWSLT 2021 | Gerard I. GÃ¡llego | this paper describes the submission to the iwslt 2021 offline speech translation task by the upc machine translation group. the task consists of building a system capable of translating english audio recordings extracted from ted talks into german text. submitted systems can be either cascade or end-to-end and use a custom or given segmentation. our submission is an end-to-end speech translation system , which combines pre-trained models \( wav2vec 2.0 and mbart \) with coupling modules between the encoder and decoder , and uses an efficient fine-tuning technique , which trains only 20% of its total parameters. we show that adding an adapter to the system and pre-training it , can increase the convergence speed and the final result , with which we achieve a bleu score of 27.3 on the must-c test set. our final model is an ensemble that obtains 28.22 bleu score on the same set. our submission also uses a custom segmentation algorithm that employs pre-trained wav2vec 2.0 for identifying periods of untranscribable text and can bring improvements of 2.5 to 3 bleu score on the iwslt 2019 test set , as compared to the result with the given segmentation.
