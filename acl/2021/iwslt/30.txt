A Statistical Extension of Byte-Pair Encoding | David Vilar | sub-word segmentation is currently a standard tool for training neural machine translation \( mt \) systems and other nlp tasks. the goal is to split words \( both in the source and target languages \) into smaller units which then constitute the input and output vocabularies of the mt system. the aim of reducing the size of the input and output vocabularies is to increase the generalization capabilities of the translation model , enabling the system to translate and generate infrequent and new \( unseen \) words at inference time by combining previously seen sub-word units. ideally , we would expect the created units to have some linguistic meaning , so that words are created in a compositional way. however , the most popular word-splitting method , byte-pair encoding \( bpe \) , which originates from the data compression literature , does not include explicit criteria to favor linguistic splittings nor to find the optimal sub-word granularity for the given training data. in this paper , we propose a statistically motivated extension of the bpe algorithm and an effective convergence criterion that avoids the costly experimentation cycle needed to select the best sub-word vocabulary size. experimental results with morphologically rich languages show that our model achieves nearly-optimal bleu scores and produces morphologically better word segmentations , which allows to outperform bpeâ€™s generalization in the translation of sentences containing new words , as shown via human evaluation.
