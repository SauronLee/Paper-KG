Integrated Training for Sequence-to-Sequence Models Using Non-Autoregressive Transformer | Evgeniia Tokarchuk | complex natural language applications such as speech translation or pivot translation traditionally rely on cascaded models. however , cascaded models are known to be prone to error propagation and model discrepancy problems. furthermore , there is no possibility of using end-to-end training data in conventional cascaded systems , meaning that the training data most suited for the task cannot be used.previous studies suggested several approaches for integrated end-to-end training to overcome those problems , however they mostly rely on \( synthetic or natural \) three-way data. we propose a cascaded model based on the non-autoregressive transformer that enables end-to-end training without the need for an explicit intermediate representation. this new architecture \( i \) avoids unnecessary early decisions that can cause errors which are then propagated throughout the cascaded models and \( ii \) utilizes the end-to-end training data directly. we conduct an evaluation on two pivot-based machine translation tasks , namely french→german and german→czech. our experimental results show that the proposed architecture yields an improvement of more than 2 bleu for french→german over the cascaded baseline.
