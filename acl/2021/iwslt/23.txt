On Knowledge Distillation for Translating Erroneous Speech Transcriptions | Ryo Fukuda | recent studies argue that knowledge distillation is promising for speech translation \( st \) using end-to-end models. in this work , we investigate the effect of knowledge distillation with a cascade st using automatic speech recognition \( asr \) and machine translation \( mt \) models. we distill knowledge from a teacher model based on human transcripts to a student model based on erroneous transcriptions. our experimental results demonstrated that knowledge distillation is beneficial for a cascade st. further investigation that combined knowledge distillation and fine-tuning revealed that the combination consistently improved two language pairs: english-italian and spanish-english.
