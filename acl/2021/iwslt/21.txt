The USYD-JD Speech Translation System for IWSLT2021 | Liang Ding | this paper describes the university of sydney & jdâ€™s joint submission of the iwslt 2021 low resource speech translation task. we participated in the swahili->english direction and got the best scarebleu \( 25.3 \) score among all the participants. our constrained system is based on a pipeline framework , i.e. asr and nmt. we trained our models with the officially provided asr and mt datasets. the asr system is based on the open-sourced tool kaldi and this work mainly explores how to make the most of the nmt models. to reduce the punctuation errors generated by the asr model , we employ our previous work slotrefine to train a punctuation correction model. to achieve better translation performance , we explored the most recent effective strategies , including back translation , knowledge distillation , multi-feature reranking , and transductive finetuning. for model structure , we tried auto-regressive and non-autoregressive models , respectively. in addition , we proposed two novel pre-train approaches , i.e. de-noising training and bidirectional training to fully exploit the data. extensive experiments show that adding the above techniques consistently improves the bleu scores , and the final submission system outperforms the baseline \( transformer ensemble model trained with the original parallel data \) by approximately 10.8 bleu score , achieving the sota performance.
