Multilingual Speech Translation with Unified Transformer: Huawei Noah’s Ark Lab at IWSLT 2021 | Xingshan Zeng | this paper describes the system submitted to the iwslt 2021 multilingual speech translation \( multist \) task from huawei noah’s ark lab. we use a unified transformer architecture for our multist model , so that the data from different modalities \( i.e. , speech and text \) and different tasks \( i.e. , speech recognition , machine translation , and speech translation \) can be exploited to enhance the model’s ability. specifically , speech and text inputs are firstly fed to different feature extractors to extract acoustic and textual features , respectively. then , these features are processed by a shared encoder–decoder architecture. we apply several training techniques to improve the performance , including multi-task learning , task-level curriculum learning , data augmentation , etc. our final system achieves significantly better results than bilingual baselines on supervised language pairs and yields reasonable results on zero-shot language pairs.
