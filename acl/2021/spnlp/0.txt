RewardsOfSum: Exploring Reinforcement Learning Rewards for Summarisation | Jacob Parnell | to date , most abstractive summarisation models have relied on variants of the negative log-likelihood \( nll \) as their training objective. in some cases , reinforcement learning has been added to train the models with an objective that is closer to their evaluation measures \( e.g. rouge \) . however , the reward function to be used within the reinforcement learning approach can play a key role for performance and is still partially unexplored. for this reason , in this paper , we propose two reward functions for the task of abstractive summarisation: the first function , referred to as rwb-hinge , dynamically selects the samples for the gradient update. the second function , nicknamed risk , leverages a small pool of strong candidates to inform the reward. in the experiments , we probe the proposed approach by fine-tuning an nll pre-trained model over nine summarisation datasets of diverse size and nature. the experimental results show a consistent improvement over the negative log-likelihood baselines.
