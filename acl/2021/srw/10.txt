“I’ve Seen Things You People Wouldn’t Believe”: Hallucinating Entities in GuessWhat?! | Alberto Testoni | natural language generation systems have witnessed important progress in the last years , but they are shown to generate tokens that are unrelated to the source input. this problem affects computational models in many nlp tasks , and it is particularly unpleasant in multimodal systems. in this work , we assess the rate of object hallucination in multimodal conversational agents playing the guesswhat \? ! referential game. better visual processing has been shown to mitigate this issue in image captioning; hence , we adapt to the guesswhat \? ! task the best visual processing models at disposal , and propose two new models to play the questioner agent. we show that the new models generate few hallucinations compared to other renowned models available in the literature. moreover , their hallucinations are less severe \( affect task-accuracy less \) and are more human-like. we also analyse where hallucinations tend to occur more often through the dialogue: hallucinations are less frequent in earlier turns , cause a cascade hallucination effect , and are often preceded by negative answers , which have been shown to be harder to ground.
