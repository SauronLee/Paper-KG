Synchronous Syntactic Attention for Transformer Neural Machine Translation | Hiroyuki Deguchi | this paper proposes a novel attention mechanism for transformer neural machine translation , “synchronous syntactic attention , ” inspired by synchronous dependency grammars. the mechanism synchronizes source-side and target-side syntactic self-attentions by minimizing the difference between target-side self-attentions and the source-side self-attentions mapped by the encoder-decoder attention matrix. the experiments show that the proposed method improves the translation performance on wmt14 en-de , wmt16 en-ro , and aspec ja-en \( up to +0.38 points in bleu \) .
