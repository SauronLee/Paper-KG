Stage-wise Fine-tuning for Graph-to-Text Generation | Qingyun Wang | graph-to-text generation has benefited from pre-trained language models \( plms \) in achieving better performance than structured graph encoders. however , they fail to fully utilize the structure information of the input graph. in this paper , we aim to further improve the performance of the pre-trained language model by proposing a structured graph-to-text model with a two-step fine-tuning mechanism which first fine-tunes model on wikipedia before adapting to the graph-to-text generation. in addition to using the traditional token and position embeddings to encode the knowledge graph \( kg \) , we propose a novel tree-level embedding method to capture the inter-dependency structures of the input graph. this new approach has significantly improved the performance of all text generation metrics for the english webnlg 2017 dataset.
