MVP-BERT: Multi-Vocab Pre-training for Chinese BERT | Wei Zhu | despite the development of pre-trained language models \( plms \) significantly raise the performances of various chinese natural language processing \( nlp \) tasks , the vocabulary \( vocab \) for these chinese plms remains to be the one provided by google chinese bert \( citation \) , which is based on chinese characters \( chars \) . second , the masked language model pre-training is based on a single vocab , limiting its downstream task performances. in this work , we first experimentally demonstrate that building a vocab via chinese word segmentation \( cws \) guided sub-word tokenization \( sgt \) can improve the performances of chinese plms. then we propose two versions of multi-vocab pre-training \( mvp \) , hi-mvp and al-mvp , to improve the models’ expressiveness. experiments show that: \( a \) mvp training strategies improve plms’ downstream performances , especially it can improve the plm’s performances on span-level tasks; \( b \) our al-mvp outperforms the recent ambert \( citation \) after large-scale pre-training , and it is more robust against adversarial attacks.
