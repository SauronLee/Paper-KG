Transformer-Based Direct Hidden Markov Model for Machine Translation | Weiyue Wang | the neural hidden markov model has been proposed as an alternative to attention mechanism in machine translation with recurrent neural networks. however , since the introduction of the transformer models , its performance has been surpassed. this work proposes to introduce the concept of the hidden markov model to the transformer architecture , which outperforms the transformer baseline. interestingly , we find that the zero-order model already provides promising performance , giving it an edge compared to a model with first-order dependency , which performs similarly but is significantly slower in training and decoding.
