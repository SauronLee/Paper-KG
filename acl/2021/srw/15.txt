Semantics of the Unwritten: The Effect of End of Paragraph and Sequence Tokens on Text Generation with GPT2 | He Bai | the semantics of a text is manifested not only by what is read but also by what is not read. in this article , we will study how those implicit “not read” information such as end-of-paragraph \( \) and end-of-sequence \( \) affect the quality of text generation. specifically , we find that the pre-trained language model gpt2 can generate better continuations by learning to generate the in the fine-tuning stage. experimental results on english story generation show that can lead to higher bleu scores and lower perplexity. we also conduct experiments on a self-collected chinese essay dataset with chinese-gpt2 , a character level lm without and during pre-training. experimental results show that the chinese gpt2 can generate better essay endings with .
