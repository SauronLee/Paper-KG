Modeling Text using the Continuous Space Topic Model with Pre-Trained Word Embeddings | Seiichi Inoue | in this study , we propose a model that extends the continuous space topic model \( cstm \) , which flexibly controls word probability in a document , using pre-trained word embeddings. to develop the proposed model , we pre-train word embeddings , which capture the semantics of words and plug them into the cstm. intrinsic experimental results show that the proposed model exhibits a superior performance over the cstm in terms of perplexity and convergence speed. furthermore , extrinsic experimental results show that the proposed model is useful for a document classification task when compared with the baseline model. we qualitatively show that the latent coordinates obtained by training the proposed model are better than those of the baseline model.
