WeaSuL: Weakly Supervised Dialogue Policy Learning: Reward Estimation for Multi-turn Dialogue | Anant Khandelwal | an intelligent dialogue system in a multi-turn setting should not only generate the responses which are of good quality , but it should also generate the responses which can lead to long-term success of the dialogue. although , the current approaches improved the response quality , but they over-look the training signals present in the dialogue data. we can leverage these signals to generate the weakly supervised training data for learning dialog policy and reward estimator , and make the policy take actions \( generates responses \) which can foresee the future direction for a successful \( rewarding \) conversation. we simulate the dialogue between an agent and a user \( modelled similar to an agent with supervised learning objective \) to interact with each other. the agent uses dynamic blocking to generate ranked diverse responses and exploration-exploitation to select among the top-k responses. each simulated state-action pair is evaluated \( works as a weak annotation \) with three quality modules: semantic relevant , semantic coherence and consistent flow. empirical studies with two benchmarks indicate that our model can significantly out-perform the response quality and lead to a successful conversation on both automatic evaluation and human judgment.
