Alignment Rationale for Natural Language Inference | Zhongtao Jiang | deep learning models have achieved great success on the task of natural language inference \( nli \) , though only a few attempts try to explain their behaviors. existing explanation methods usually pick prominent features such as words or phrases from the input text. however , for nli , alignments among words or phrases are more enlightening clues to explain the model. to this end , this paper presents arec , a post-hoc approach to generate alignment rationale explanations for co-attention based models in nli. the explanation is based on feature selection , which keeps few but sufficient alignments while maintaining the same prediction of the target model. experimental results show that our method is more faithful and human-readable compared with many existing approaches. we further study and re-evaluate three typical models through our explanation beyond accuracy , and propose a simple method that greatly improves the model robustness.
