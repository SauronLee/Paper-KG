Marginal Utility Diminishes: Exploring the Minimum Knowledge for BERT Knowledge Distillation | Yuanxin Liu | recently , knowledge distillation \( kd \) has shown great success in bert compression. instead of only learning from the teacher’s soft label as in conventional kd , researchers find that the rich information contained in the hidden layers of bert is conducive to the student’s performance. to better exploit the hidden knowledge , a common practice is to force the student to deeply mimic the teacher’s hidden states of all the tokens in a layer-wise manner. in this paper , however , we observe that although distilling the teacher’s hidden state knowledge \( hsk \) is helpful , the performance gain \( marginal utility \) diminishes quickly as more hsk is distilled. to understand this effect , we conduct a series of analysis. specifically , we divide the hsk of bert into three dimensions , namely depth , length and width. we first investigate a variety of strategies to extract crucial knowledge for each single dimension and then jointly compress the three dimensions. in this way , we show that 1 \) the student’s performance can be improved by extracting and distilling the crucial hsk , and 2 \) using a tiny fraction of hsk can achieve the same performance as extensive hsk distillation. based on the second finding , we further propose an efficient kd paradigm to compress bert , which does not require loading the teacher during the training of student. for two kinds of student models and computing devices , the proposed kd paradigm gives rise to training speedup of 2.7x 3.4x.
