Evaluation Examples are not Equally Informative: How should that change NLP Leaderboards? | Pedro Rodriguez | leaderboards are widely used in nlp and push the field forward. while leaderboards are a straightforward ranking of nlp models , this simplicity can mask nuances in evaluation items \( examples \) and subjects \( nlp models \) . rather than replace leaderboards , we advocate a re-imagining so that they better highlight if and where progress is made. building on educational testing , we create a bayesian leaderboard model where latent subject skill and latent item difficulty predict correct responses. using this model , we analyze the ranking reliability of leaderboards. afterwards , we show the model can guide what to annotate , identify annotation errors , detect overfitting , and identify informative examples. we conclude with recommendations for future benchmark tasks.
