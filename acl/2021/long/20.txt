Contrastive Learning for Many-to-many Multilingual Neural Machine Translation | Xiao Pan | existing multilingual machine translation approaches mainly focus on english-centric directions , while the non-english directions still lag behind. in this work , we aim to build a many-to-many translation system with an emphasis on the quality of non-english language directions. our intuition is based on the hypothesis that a universal cross-language representation leads to better multilingual translation performance. to this end , we propose mrasp2 , a training method to obtain a single unified multilingual translation model. mrasp2 is empowered by two techniques: a \) a contrastive learning scheme to close the gap among representations of different languages , and b \) data augmentation on both multiple parallel and monolingual data to further align token representations. for english-centric directions , mrasp2 achieves competitive or even better performance than a strong pre-trained model mbart on tens of wmt benchmarks. for non-english directions , mrasp2 achieves an improvement of average 10+ bleu compared with the multilingual baseline
