Language Model as an Annotator: Exploring DialoGPT for Dialogue Summarization | Xiachong Feng | current dialogue summarization systems usually encode the text with a number of general semantic features \( e.g. , keywords and topics \) to gain more powerful dialogue modeling capabilities. however , these features are obtained via open-domain toolkits that are dialog-agnostic or heavily relied on human annotations. in this paper , we show how dialogpt , a pre-trained model for conversational response generation , can be developed as an unsupervised dialogue annotator , which takes advantage of dialogue background knowledge encoded in dialogpt. we apply dialogpt to label three types of features on two dialogue summarization datasets , samsum and ami , and employ pre-trained and non pre-trained models as our summarizers. experimental results show that our proposed method can obtain remarkable improvements on both datasets and achieves new state-of-the-art performance on the samsum dataset.
