Few-Shot Question Answering by Pretraining Span Selection | Ori Ram | in several question answering benchmarks , pretrained models have reached human parity through fine-tuning on an order of 100 , 000 annotated questions and answers. we explore the more realistic few-shot setting , where only a few hundred training examples are available , and observe that standard models perform poorly , highlighting the discrepancy between current pretraining objectives and question answering. we propose a new pretraining scheme tailored for question answering: recurring span selection. given a passage with multiple sets of recurring spans , we mask in each set all recurring spans but one , and ask the model to select the correct span in the passage for each masked span. masked spans are replaced with a special token , viewed as a question representation , that is later used during fine-tuning to select the answer span. the resulting model obtains surprisingly good results on multiple benchmarks \( e.g. , 72.7 f1 on squad with only 128 training examples \) , while maintaining competitive performance in the high-resource setting.
