BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models Identify Analogies? | Asahi Ushio | analogies play a central role in human commonsense reasoning. the ability to recognize analogies such as “eye is to seeing what ear is to hearing” , sometimes referred to as analogical proportions , shape how we structure knowledge and understand language. surprisingly , however , the task of identifying such analogies has not yet received much attention in the language model era. in this paper , we analyze the capabilities of transformer-based language models on this unsupervised task , using benchmarks obtained from educational settings , as well as more commonly used datasets. we find that off-the-shelf language models can identify analogies to a certain extent , but struggle with abstract and complex relations , and results are highly sensitive to model architecture and hyperparameters. overall the best results were obtained with gpt-2 and roberta , while configurations using bert were not able to outperform word embedding models. our results raise important questions for future work about how , and to what extent , pre-trained language models capture knowledge about abstract semantic relations.
