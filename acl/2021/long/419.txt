Syntax-Enhanced Pre-trained Model | Zenan Xu | we study the problem of leveraging the syntactic structure of text to enhance pre-trained models such as bert and roberta. existing methods utilize syntax of text either in the pre-training stage or in the fine-tuning stage , so that they suffer from discrepancy between the two stages. such a problem would lead to the necessity of having human-annotated syntactic information , which limits the application of existing methods to broader scenarios. to address this , we present a model that utilizes the syntax of text in both pre-training and fine-tuning stages. our model is based on transformer with a syntax-aware attention layer that considers the dependency tree of the text. we further introduce a new pre-training task of predicting the syntactic distance among tokens in the dependency tree. we evaluate the model on three downstream tasks , including relation classification , entity typing , and question answering. results show that our model achieves state-of-the-art performance on six public benchmark datasets. we have two major findings. first , we demonstrate that infusing automatically produced syntax of text improves pre-trained models. second , global syntactic distances among tokens bring larger performance gains compared to local head relations between contiguous tokens.
