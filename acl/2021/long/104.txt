Exploiting Language Relatedness for Low Web-Resource Language Model Adaptation: An Indic Languages Study | Yash Khemchandani | recent research in multilingual language models \( lm \) has demonstrated their ability to effectively handle multiple languages in a single model. this holds promise for low web-resource languages \( lrl \) as multilingual models can enable transfer of supervision from high resource languages to lrls. however , incorporating a new language in an lm still remains a challenge , particularly for languages with limited corpora and in unseen scripts. in this paper we argue that relatedness among languages in a language family may be exploited to overcome some of the corpora limitations of lrls , and propose relatelm. we focus on indian languages , and exploit relatedness along two dimensions: \( 1 \) script \( since many indic scripts originated from the brahmic script \) , and \( 2 \) sentence structure. relatelm uses transliteration to convert the unseen script of limited lrl text into the script of a related prominent language \( rpl \) \( hindi in our case \) . while exploiting similar sentence structures , relatelm utilizes readily available bilingual dictionaries to pseudo translate rpl text into lrl corpora. experiments on multiple real-world benchmark datasets provide validation to our hypothesis that using a related language as pivot , along with transliteration and pseudo translation based data augmentation , can be an effective way to adapt lms for lrls , rather than direct training or pivoting through english.
