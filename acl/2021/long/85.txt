MATE-KD: Masked Adversarial TExt, a Companion to Knowledge Distillation | Ahmad Rashid | the advent of large pre-trained language models has given rise to rapid progress in the field of natural language processing \( nlp \) . while the performance of these models on standard benchmarks has scaled with size , compression techniques such as knowledge distillation have been key in making them practical. we present mate-kd , a novel text-based adversarial training algorithm which improves the performance of knowledge distillation. mate-kd first trains a masked language model-based generator to perturb text by maximizing the divergence between teacher and student logits. then using knowledge distillation a student is trained on both the original and the perturbed training samples. we evaluate our algorithm , using bert-based models , on the glue benchmark and demonstrate that mate-kd outperforms competitive adversarial learning and data augmentation baselines. on the glue test set our 6 layer roberta based model outperforms bert-large.
