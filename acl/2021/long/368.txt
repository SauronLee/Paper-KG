Mask-Align: Self-Supervised Neural Word Alignment | Chi Chen | word alignment , which aims to align translationally equivalent words between source and target sentences , plays an important role in many natural language processing tasks. current unsupervised neural alignment methods focus on inducing alignments from neural machine translation models , which does not leverage the full context in the target sequence. in this paper , we propose mask-align , a self-supervised word alignment model that takes advantage of the full context on the target side. our model masks out each target token and predicts it conditioned on both source and the remaining target tokens. this two-step process is based on the assumption that the source token contributing most to recovering the masked target token should be aligned. we also introduce an attention variant called leaky attention , which alleviates the problem of unexpected high cross-attention weights on special tokens such as periods. experiments on four language pairs show that our model outperforms previous unsupervised neural aligners and obtains new state-of-the-art results.
