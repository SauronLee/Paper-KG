Selective Knowledge Distillation for Neural Machine Translation | Fusheng Wang | neural machine translation \( nmt \) models achieve state-of-the-art performance on many translation benchmarks. as an active research field in nmt , knowledge distillation is widely applied to enhance the model’s performance by transferring teacher model’s knowledge on each training sample. however , previous work rarely discusses the different impacts and connections among these samples , which serve as the medium for transferring teacher knowledge. in this paper , we design a novel protocol that can effectively analyze the different impacts of samples by comparing various samples’ partitions. based on above protocol , we conduct extensive experiments and find that the teacher’s knowledge is not the more , the better. knowledge over specific samples may even hurt the whole performance of knowledge distillation. finally , to address these issues , we propose two simple yet effective strategies , i.e. , batch-level and global-level selections , to pick suitable samples for distillation. we evaluate our approaches on two large-scale machine translation tasks , wmt’14 english-german and wmt’19 chinese-english. experimental results show that our approaches yield up to +1.28 and +0.89 bleu points improvements over the transformer baseline , respectively.
