SemFace: Pre-training Encoder and Decoder with a Semantic Interface for Neural Machine Translation | Shuo Ren | while pre-training techniques are working very well in natural language processing , how to pre-train a decoder and effectively use it for neural machine translation \( nmt \) still remains a tricky issue. the main reason is that the cross-attention module between the encoder and decoder cannot be pre-trained , and the combined encoder-decoder model cannot work well in the fine-tuning stage because the inputs of the decoder cross-attention come from unknown encoder outputs. in this paper , we propose a better pre-training method for nmt by defining a semantic interface \( semface \) between the pre-trained encoder and the pre-trained decoder. specifically , we propose two types of semantic interfaces , including cl-semface which regards cross-lingual embeddings as an interface , and vq-semface which employs vector quantized embeddings to constrain the encoder outputs and decoder inputs in the same language-independent space. we conduct massive experiments on six supervised translation pairs and three unsupervised pairs. experimental results demonstrate that our proposed semface can effectively connect the pre-trained encoder and decoder , and achieves significant improvement by 3.7 and 1.5 bleu points on the two tasks respectively compared with previous pre-training-based nmt models.
