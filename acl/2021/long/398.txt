Structural Pre-training for Dialogue Comprehension | Zhuosheng Zhang | pre-trained language models \( prlms \) have demonstrated superior performance due to their strong ability to learn universal language representations from self-supervised pre-training. however , even with the help of the powerful prlms , it is still challenging to effectively capture task-related knowledge from dialogue texts which are enriched by correlations among speaker-aware utterances. in this work , we present spider , structural pre-trained dialogue reader , to capture dialogue exclusive features. to simulate the dialogue-like features , we propose two training objectives in addition to the original lm objectives: 1 \) utterance order restoration , which predicts the order of the permuted utterances in dialogue context; 2 \) sentence backbone regularization , which regularizes the model to improve the factual correctness of summarized subject-verb-object triplets. experimental results on widely used dialogue benchmarks verify the effectiveness of the newly introduced self-supervised tasks.
