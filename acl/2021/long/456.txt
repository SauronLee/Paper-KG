SMedBERT: A Knowledge-Enhanced Pre-trained Language Model with Structured Semantics for Medical Text Mining | Taolin Zhang | recently , the performance of pre-trained language models \( plms \) has been significantly improved by injecting knowledge facts to enhance their abilities of language understanding. for medical domains , the background knowledge sources are especially useful , due to the massive medical terms and their complicated relations are difficult to understand in text. in this work , we introduce smedbert , a medical plm trained on large-scale medical corpora , incorporating deep structured semantic knowledge from neighbours of linked-entity. in smedbert , the mention-neighbour hybrid attention is proposed to learn heterogeneous-entity information , which infuses the semantic representations of entity types into the homogeneous neighbouring entity structure. apart from knowledge integration as external features , we propose to employ the neighbors of linked-entities in the knowledge graph as additional global contexts of text mentions , allowing them to communicate via shared neighbors , thus enrich their semantic representations. experiments demonstrate that smedbert significantly outperforms strong baselines in various knowledge-intensive chinese medical tasks. it also improves the performance of other tasks such as question answering , question matching and natural language inference.
