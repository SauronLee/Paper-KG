LexFit: Lexical Fine-Tuning of Pretrained Language Models | Ivan Vulić | transformer-based language models \( lms \) pretrained on large text collections implicitly store a wealth of lexical semantic knowledge , but it is non-trivial to extract that knowledge effectively from their parameters. inspired by prior work on semantic specialization of static word embedding \( we \) models , we show that it is possible to expose and enrich lexical knowledge from the lms , that is , to specialize them to serve as effective and universal “decontextualized” word encoders even when fed input words “in isolation” \( i.e. , without any context \) . their transformation into such word encoders is achieved through a simple and efficient lexical fine-tuning procedure \( termed lexfit \) based on dual-encoder network structures. further , we show that lexfit can yield effective word encoders even with limited lexical supervision and , via cross-lingual transfer , in different languages without any readily available external knowledge. our evaluation over four established , structurally different lexical-level tasks in 8 languages indicates the superiority of lexfit-based wes over standard static wes \( e.g. , fasttext \) and wes from vanilla lms. other extensive experiments and ablation studies further profile the lexfit framework , and indicate best practices and performance variations across lexfit variants , languages , and lexical tasks , also directly questioning the usefulness of traditional we models in the era of large neural models.
