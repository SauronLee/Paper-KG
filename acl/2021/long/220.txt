Self-Training Sampling with Monolingual Data Uncertainty for Neural Machine Translation | Wenxiang Jiao | self-training has proven effective for improving nmt performance by augmenting model training with synthetic parallel data. the common practice is to construct synthetic data based on a randomly sampled subset of large-scale monolingual data , which we empirically show is sub-optimal. in this work , we propose to improve the sampling procedure by selecting the most informative monolingual sentences to complement the parallel data. to this end , we compute the uncertainty of monolingual sentences using the bilingual dictionary extracted from the parallel data. intuitively , monolingual sentences with lower uncertainty generally correspond to easy-to-translate patterns which may not provide additional gains. accordingly , we design an uncertainty-based sampling strategy to efficiently exploit the monolingual data for self-training , in which monolingual sentences with higher uncertainty would be sampled with higher probability. experimental results on large-scale wmt english⇒german and english⇒chinese datasets demonstrate the effectiveness of the proposed approach. extensive analyses suggest that emphasizing the learning on uncertain monolingual sentences by our approach does improve the translation quality of high-uncertainty sentences and also benefits the prediction of low-frequency words at the target side.
