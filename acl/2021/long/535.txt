Improving Factual Consistency of Abstractive Summarization via Question Answering | Feng Nan | a commonly observed problem with the state-of-the art abstractive summarization models is that the generated summaries can be factually inconsistent with the input documents. the fact that automatic summarization may produce plausible-sounding yet inaccurate summaries is a major concern that limits its wide application. in this paper we present an approach to address factual consistency in summarization. we first propose an efficient automatic evaluation metric to measure factual consistency; next , we propose a novel learning algorithm that maximizes the proposed metric during model training. through extensive experiments , we confirm that our method is effective in improving factual consistency and even overall quality of the summaries , as judged by both automatic metrics and human evaluation.
