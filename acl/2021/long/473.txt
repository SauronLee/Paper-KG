Focus Attention: Promoting Faithfulness and Diversity in Summarization | Rahul Aralikatte | professional summaries are written with document-level information , such as the theme of the document , in mind. this is in contrast with most seq2seq decoders which simultaneously learn to focus on salient content , while deciding what to generate , at each decoding step. with the motivation to narrow this gap , we introduce focus attention mechanism , a simple yet effective method to encourage decoders to proactively generate tokens that are similar or topical to the input document. further , we propose a focus sampling method to enable generation of diverse summaries , an area currently understudied in summarization. when evaluated on the bbc extreme summarization task , two state-of-the-art models augmented with focus attention generate summaries that are closer to the target and more faithful to their input documents , outperforming their vanilla counterparts on rouge and multiple faithfulness measures. we also empirically demonstrate that focus sampling is more effective in generating diverse and faithful summaries than top-k or nucleus sampling-based decoding methods.
