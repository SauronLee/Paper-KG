Cross-Lingual Abstractive Summarization with Limited Parallel Resources | Yu Bai | parallel cross-lingual summarization data is scarce , requiring models to better use the limited available cross-lingual resources. existing methods to do so often adopt sequence-to-sequence networks with multi-task frameworks. such approaches apply multiple decoders , each of which is utilized for a specific task. however , these independent decoders share no parameters , hence fail to capture the relationships between the discrete phrases of summaries in different languages , breaking the connections in order to transfer the knowledge of the high-resource languages to low-resource languages. to bridge these connections , we propose a novel multi-task framework for cross-lingual abstractive summarization \( mclas \) in a low-resource setting. employing one unified decoder to generate the sequential concatenation of monolingual and cross-lingual summaries , mclas makes the monolingual summarization task a prerequisite of the cls task. in this way , the shared decoder learns interactions involving alignments and summary patterns across languages , which encourages attaining knowledge transfer. experiments on two cls datasets demonstrate that our model significantly outperforms three baseline models in both low-resource and full-dataset scenarios. moreover , in-depth analysis on the generated summaries and attention heads verifies that interactions are learned well using mclas , which benefits the cls task under limited parallel resources.
