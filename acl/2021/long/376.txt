Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word Substitution | Fanchao Qi | recent studies show that neural natural language processing \( nlp \) models are vulnerable to backdoor attacks. injected with backdoors , models perform normally on benign examples but produce attacker-specified predictions when the backdoor is activated , presenting serious security threats to real-world applications. since existing textual backdoor attacks pay little attention to the invisibility of backdoors , they can be easily detected and blocked. in this work , we present invisible backdoors that are activated by a learnable combination of word substitution. we show that nlp models can be injected with backdoors that lead to a nearly 100% attack success rate , whereas being highly invisible to existing defense strategies and even human inspections. the results raise a serious alarm to the security of nlp models , which requires further research to be resolved. all the data and code of this paper are released at https://github.com/thunlp/bkdatk-lws.
