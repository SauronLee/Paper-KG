Pre-training Universal Language Representation | Yian Li | despite the well-developed cut-edge representation learning for language , most language representation models usually focus on specific levels of linguistic units. this work introduces universal language representation learning , i.e. , embeddings of different levels of linguistic units or text with quite diverse lengths in a uniform vector space. we propose the training objective misad that utilizes meaningful n-grams extracted from large unlabeled corpus by a simple but effective algorithm for pre-trained language models. then we empirically verify that well designed pre-training scheme may effectively yield universal language representation , which will bring great convenience when handling multiple layers of linguistic objects in a unified way. especially , our model achieves the highest accuracy on analogy tasks in different language levels and significantly improves the performance on downstream tasks in the glue benchmark and a question answering dataset.
