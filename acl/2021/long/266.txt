G-Transformer for Document-Level Machine Translation | Guangsheng Bao | document-level mt models are still far from satisfactory. existing work extend translation unit from single sentence to multiple sentences. however , study shows that when we further enlarge the translation unit to a whole document , supervised training of transformer can fail. in this paper , we find such failure is not caused by overfitting , but by sticking around local minima during training. our analysis shows that the increased complexity of target-to-source attention is a reason for the failure. as a solution , we propose g-transformer , introducing locality assumption as an inductive bias into transformer , reducing the hypothesis space of the attention from target to source. experiments show that g-transformer converges faster and more stably than transformer , achieving new state-of-the-art bleu scores for both nonpretraining and pre-training settings on three benchmark datasets.
