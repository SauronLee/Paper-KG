ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning | Yujia Qin | pre-trained language models \( plms \) have shown superior performance on various downstream natural language processing \( nlp \) tasks. however , conventional pre-training objectives do not explicitly model relational facts in text , which are crucial for textual understanding. to address this issue , we propose a novel contrastive learning framework erica to obtain a deep understanding of the entities and their relations in text. specifically , we define two novel pre-training tasks to better understand entities and relations: \( 1 \) the entity discrimination task to distinguish which tail entity can be inferred by the given head entity and relation; \( 2 \) the relation discrimination task to distinguish whether two relations are close or not semantically , which involves complex relational reasoning. experimental results demonstrate that erica can improve typical plms \( bert and roberta \) on several language understanding tasks , including relation extraction , entity typing and question answering , especially under low-resource settings.
