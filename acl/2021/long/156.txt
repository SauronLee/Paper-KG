Control Image Captioning Spatially and Temporally | Kun Yan | generating image captions with user intention is an emerging need. the recently published localized narratives dataset takes mouse traces as another input to the image captioning task , which is an intuitive and efficient way for a user to control what to describe in the image. however , how to effectively employ traces to improve generation quality and controllability is still under exploration. this paper aims to solve this problem by proposing a novel model called loopcag , which connects contrastive constraints and attention guidance in a loop manner , engaged explicit spatial and temporal constraints to the generating process. precisely , each generated sentence is temporally aligned to the corresponding trace sequence through a contrastive learning strategy. besides , each generated text token is supervised to attend to the correct visual objects under heuristic spatial attention guidance. comprehensive experimental results demonstrate that our loopcag model learns better correspondence among the three modalities \( vision , language , and traces \) and achieves sota performance on trace-controlled image captioning task. moreover , the controllability and explainability of loopcag are validated by analyzing spatial and temporal sensitivity during the generation process.
