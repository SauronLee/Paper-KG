Shortformer: Better Language Modeling using Shorter Inputs | Ofir Press | increasing the input length has been a driver of progress in language modeling with transformers. we identify conditions where shorter inputs are not harmful , and achieve perplexity and efficiency improvements through two new methods that decrease input length. first , we show that initially training a model on short subsequences before moving on to longer ones both reduces overall training time and , surprisingly , substantially improves perplexity. second , we show how to improve the efficiency of recurrence methods in transformers , which let models condition on previously processed tokens when generating sequences that exceed the maximal length the transformer can handle at once. existing methods require computationally expensive relative position embeddings; we introduce a simple alternative of adding absolute position embeddings to queries and keys instead of to word embeddings , which efficiently produces superior results. we show that these recurrent models also benefit from short input lengths. combining these techniques speeds up training by a factor of 1.65 , reduces memory usage , and substantially improves perplexity on wikitext-103 , without adding any parameters.
