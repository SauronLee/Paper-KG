DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts | Alisa Liu | despite recent advances in natural language generation , it remains challenging to control attributes of generated text. we propose dexperts: decoding-time experts , a decoding-time method for controlled text generation that combines a pretrained language model with “expert” lms and/or “anti-expert” lms in a product of experts. intuitively , under the ensemble , tokens only get high probability if they are considered likely by the experts , and unlikely by the anti-experts. we apply dexperts to language detoxification and sentiment-controlled generation , where we outperform existing controllable generation methods on both automatic and human evaluations. moreover , because dexperts operates only on the output of the pretrained lm , it is effective with \( anti- \) experts of smaller size , including when operating on gpt-3. our work highlights the promise of tuning small lms on text with \( un \) desirable attributes for efficient decoding-time steering.
