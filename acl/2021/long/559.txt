Language Embeddings for Typology and Cross-lingual Transfer Learning | Dian Yu | cross-lingual language tasks typically require a substantial amount of annotated data or parallel translation data. we explore whether language representations that capture relationships among languages can be learned and subsequently leveraged in cross-lingual tasks without the use of parallel data. we generate dense embeddings for 29 languages using a denoising autoencoder , and evaluate the embeddings using the world atlas of language structures \( wals \) and two extrinsic tasks in a zero-shot setting: cross-lingual dependency parsing and cross-lingual natural language inference.
