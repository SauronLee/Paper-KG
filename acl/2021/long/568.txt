UnNatural Language Inference | Koustuv Sinha | recent investigations into the inner-workings of state-of-the-art large-scale pre-trained transformer-based natural language understanding \( nlu \) models indicate that they appear to understand human-like syntax , at least to some extent. we provide novel evidence that complicates this claim: we find that state-of-the-art natural language inference \( nli \) models assign the same labels to permuted examples as they do to the original , i.e. they are invariant to random word-order permutations. this behavior notably differs from that of humans; we struggle to understand the meaning of ungrammatical sentences. to measure the severity of this issue , we propose a suite of metrics and investigate which properties of particular permutations lead models to be word order invariant. for example , in mnli dataset we find almost all \( 98.7% \) examples contain at least one permutation which elicits the gold label. models are even able to assign gold labels to permutations that they originally failed to predict correctly. we provide a comprehensive empirical evaluation of this phenomenon , and further show that this issue exists in pre-transformer rnn / convnet based encoders , as well as across multiple languages \( english and chinese \) . our code and data are available at https://github.com/facebookresearch/unlu.
