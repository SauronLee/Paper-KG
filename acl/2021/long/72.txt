XLPT-AMR: Cross-Lingual Pre-Training via Multi-Task Learning for Zero-Shot AMR Parsing and Text Generation | Dongqin Xu | due to the scarcity of annotated data , abstract meaning representation \( amr \) research is relatively limited and challenging for languages other than english. upon the availability of english amr dataset and english-to- x parallel datasets , in this paper we propose a novel cross-lingual pre-training approach via multi-task learning \( mtl \) for both zeroshot amr parsing and amr-to-text generation. specifically , we consider three types of relevant tasks , including amr parsing , amr-to-text generation , and machine translation. we hope that knowledge gained while learning for english amr parsing and text generation can be transferred to the counterparts of other languages. with properly pretrained models , we explore four different finetuning methods , i.e. , vanilla fine-tuning with a single task , one-for-all mtl fine-tuning , targeted mtl fine-tuning , and teacher-studentbased mtl fine-tuning. experimental results on amr parsing and text generation of multiple non-english languages demonstrate that our approach significantly outperforms a strong baseline of pre-training approach , and greatly advances the state of the art. in detail , on ldc2020t07 we have achieved 70.45% , 71.76% , and 70.80% in smatch f1 for amr parsing of german , spanish , and italian , respectively , while for amr-to-text generation of the languages , we have obtained 25.69 , 31.36 , and 28.42 in bleu respectively. we make our code available on github https://github.com/xdqkid/xlpt-amr.
