Fast and Accurate Neural Machine Translation with Translation Memory | Qiuxiang He | it is generally believed that a translation memory \( tm \) should be beneficial for machine translation tasks. unfortunately , existing wisdom demonstrates the superiority of tm-based neural machine translation \( nmt \) only on the tm-specialized translation tasks rather than general tasks , with a non-negligible computational overhead. in this paper , we propose a fast and accurate approach to tm-based nmt within the transformer framework: the model architecture is simple and employs a single bilingual sentence as its tm , leading to efficient training and inference; and its parameters are effectively optimized through a novel training criterion. extensive experiments on six tm-specialized tasks show that the proposed approach substantially surpasses several strong baselines that use multiple tms , in terms of bleu and running time. in particular , the proposed approach also advances the strong baselines on two general tasks \( wmt news zh->en and en->de \) .
