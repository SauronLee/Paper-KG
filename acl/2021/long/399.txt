AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models | Yichun Yin | pre-trained language models \( plms \) have achieved great success in natural language processing. most of plms follow the default setting of architecture hyper-parameters \( e.g. , the hidden dimension is a quarter of the intermediate dimension in feed-forward sub-networks \) in bert. few studies have been conducted to explore the design of architecture hyper-parameters in bert , especially for the more efficient plms with tiny sizes , which are essential for practical deployment on resource-constrained devices. in this paper , we adopt the one-shot neural architecture search \( nas \) to automatically search architecture hyper-parameters. specifically , we carefully design the techniques of one-shot learning and the search space to provide an adaptive and efficient development way of tiny plms for various latency constraints. we name our method autotinybert and evaluate its effectiveness on the glue and squad benchmarks. the extensive experiments show that our method outperforms both the sota search-based baseline \( nas-bert \) and the sota distillation-based methods \( such as distilbert , tinybert , minilm , and mobilebert \) . in addition , based on the obtained architectures , we propose a more efficient development method that is even faster than the development of a single plm. the source code and models will be publicly available upon publication.
