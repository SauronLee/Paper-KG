Cascaded Head-colliding Attention | Lin Zheng | transformers have advanced the field of natural language processing \( nlp \) on a variety of important tasks. at the cornerstone of the transformer architecture is the multi-head attention \( mha \) mechanism which models pairwise interactions between the elements of the sequence. despite its massive success , the current framework ignores interactions among different heads , leading to the problem that many of the heads are redundant in practice , which greatly wastes the capacity of the model. to improve parameter efficiency , we re-formulate the mha as a latent variable model from a probabilistic perspective. we present
