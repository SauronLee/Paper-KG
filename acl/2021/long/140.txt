Ultra-Fine Entity Typing with Weak Supervision from a Masked Language Model | Hongliang Dai | recently , there is an effort to extend fine-grained entity typing by using a richer and ultra-fine set of types , and labeling noun phrases including pronouns and nominal nouns instead of just named entity mentions. a key challenge for this ultra-fine entity typing task is that human annotated data are extremely scarce , and the annotation ability of existing distant or weak supervision approaches is very limited. to remedy this problem , in this paper , we propose to obtain training data for ultra-fine entity typing by using a bert masked language model \( mlm \) . given a mention in a sentence , our approach constructs an input for the bert mlm so that it predicts context dependent hypernyms of the mention , which can be used as type labels. experimental results demonstrate that , with the help of these automatically generated labels , the performance of an ultra-fine entity typing model can be improved substantially. we also show that our approach can be applied to improve traditional fine-grained entity typing after performing simple type mapping.
