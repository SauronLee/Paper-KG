A Cognitive Regularizer for Language Modeling | Jason Wei | the uniform information density \( uid \) hypothesis , which posits that speakers behaving optimally tend to distribute information uniformly across a linguistic signal , has gained traction in psycholinguistics as an explanation for certain syntactic , morphological , and prosodic choices. in this work , we explore whether the uid hypothesis can be operationalized as an inductive bias for statistical language modeling. specifically , we augment the canonical mle objective for training language models with a regularizer that encodes uid. in experiments on ten languages spanning five language families , we find that using uid regularization consistently improves perplexity in language models , having a larger effect when training data is limited. moreover , via an analysis of generated sequences , we find that uid-regularized language models have other desirable properties , e.g. , they generate text that is more lexically diverse. our results not only suggest that uid is a reasonable inductive bias for language modeling , but also provide an alternative validation of the uid hypothesis using modern-day nlp tools.
