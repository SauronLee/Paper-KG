How to Adapt Your Pretrained Multilingual Model to 1600 Languages | Abteen Ebrahimi | pretrained multilingual models \( pmms \) enable zero-shot learning via cross-lingual transfer , performing best for languages seen during pretraining. while methods exist to improve performance for unseen languages , they have almost exclusively been evaluated using amounts of raw text only available for a small fraction of the worldâ€™s languages. in this paper , we evaluate the performance of existing methods to adapt pmms to new languages using a resource available for close to 1600 languages: the new testament. this is challenging for two reasons: \( 1 \) the small corpus size , and \( 2 \) the narrow domain. while performance drops for all approaches , we surprisingly still see gains of up to 17.69% accuracy for part-of-speech tagging and 6.29 f1 for ner on average over all languages as compared to xlm-r. another unexpected finding is that continued pretraining , the simplest approach , performs best. finally , we perform a case study to disentangle the effects of domain and size and to shed light on the influence of the finetuning source language.
