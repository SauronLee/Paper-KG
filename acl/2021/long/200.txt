LayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understanding | Yang Xu | pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to its effective model architecture and the advantage of large-scale unlabeled scanned/digital-born documents. we propose layoutlmv2 architecture with new pre-training tasks to model the interaction among text , layout , and image in a single multi-modal framework. specifically , with a two-stream multi-modal transformer encoder , layoutlmv2 uses not only the existing masked visual-language modeling task but also the new text-image alignment and text-image matching tasks , which make it better capture the cross-modality interaction in the pre-training stage. meanwhile , it also integrates a spatial-aware self-attention mechanism into the transformer architecture so that the model can fully understand the relative positional relationship among different text blocks. experiment results show that layoutlmv2 outperforms layoutlm by a large margin and achieves new state-of-the-art results on a wide variety of downstream visually-rich document understanding tasks , including funsd \( 0.7895 to 0.8420 \) , cord \( 0.9493 to 0.9601 \) , sroie \( 0.9524 to 0.9781 \) , kleister-nda \( 0.8340 to 0.8520 \) , rvl-cdip \( 0.9443 to 0.9564 \) , and docvqa \( 0.7295 to 0.8672 \) .
