Learning Relation Alignment for Calibrated Cross-modal Retrieval | Shuhuai Ren | despite the achievements of large-scale multimodal pre-training approaches , cross-modal retrieval , e.g. , image-text retrieval , remains a challenging task. to bridge the semantic gap between the two modalities , previous studies mainly focus on word-region alignment at the object level , lacking the matching between the linguistic relation among the words and the visual relation among the regions. the neglect of such relation consistency impairs the contextualized representation of image-text pairs and hinders the model performance and the interpretability. in this paper , we first propose a novel metric , intra-modal self-attention distance \( isd \) , to quantify the relation consistency by measuring the semantic distance between linguistic and visual relations. in response , we present inter-modal alignment on intra-modal self-attentions \( iais \) , a regularized training method to optimize the isd and calibrate intra-modal self-attentions from the two modalities mutually via inter-modal alignment. the iais regularizer boosts the performance of prevailing models on flickr30k and ms coco datasets by a considerable margin , which demonstrates the superiority of our approach.
