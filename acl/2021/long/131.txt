Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection | Bertie Vidgen | we present a human-and-model-in-the-loop process for dynamically generating datasets and training better performing and more robust hate detection models. we provide a new dataset of 40 , 000 entries , generated and labelled by trained annotators over four rounds of dynamic data creation. it includes 15 , 000 challenging perturbations and each hateful entry has fine-grained labels for the type and target of hate. hateful entries make up 54% of the dataset , which is substantially higher than comparable datasets. we show that model performance is substantially improved using this approach. models trained on later rounds of data collection perform better on test sets and are harder for annotators to trick. they also have better performance on hatecheck , a suite of functional tests for online hate detection. we provide the code , dataset and annotation guidelines for other researchers to use.
