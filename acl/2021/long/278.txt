Superbizarre Is Not Superb: Derivational Morphology Improves BERTâ€™s Interpretation of Complex Words | Valentin Hofmann | how does the input segmentation of pretrained language models \( plms \) affect their interpretations of complex words \? we present the first study investigating this question , taking bert as the example plm and focusing on its semantic representations of english derivatives. we show that plms can be interpreted as serial dual-route models , i.e. , the meanings of complex words are either stored or else need to be computed from the subwords , which implies that maximally meaningful input tokens should allow for the best generalization on new words. this hypothesis is confirmed by a series of semantic probing tasks on which delbert \( derivation leveraging bert \) , a model with derivational input segmentation , substantially outperforms bert with wordpiece segmentation. our results suggest that the generalization capabilities of plms could be further improved if a morphologically-informed vocabulary of input tokens were used.
