Stereotyping Norwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets | Su Lin Blodgett | auditing nlp systems for computational harms like surfacing stereotypes is an elusive goal. several recent efforts have focused on benchmark datasets consisting of pairs of contrastive sentences , which are often accompanied by metrics that aggregate an nlp system’s behavior on these pairs into measurements of harms. we examine four such benchmarks constructed for two nlp tasks: language modeling and coreference resolution. we apply a measurement modeling lens—originating from the social sciences—to inventory a range of pitfalls that threaten these benchmarks’ validity as measurement models for stereotyping. we find that these benchmarks frequently lack clear articulations of what is being measured , and we highlight a range of ambiguities and unstated assumptions that affect how these benchmarks conceptualize and operationalize stereotyping.
