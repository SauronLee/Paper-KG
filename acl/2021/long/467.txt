Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation | Xin Liu | a well-known limitation in pretrain-finetune paradigm lies in its inflexibility caused by the one-size-fits-all vocabulary.this potentially weakens the effect when applying pretrained models into natural language generation \( nlg \) tasks , especially for the subword distributions between upstream and downstream tasks with significant discrepancy. towards approaching this problem , we extend the vanilla pretrain-finetune pipeline with an extra embedding transfer step. specifically , a plug-and-play embedding generator is introduced to produce the representation of any input token , according to pre-trained embeddings of its morphologically similar ones.thus , embeddings of mismatch tokens in downstream tasks can also be efficiently initialized.we conduct experiments on a variety of nlg tasks under the pretrain-finetune fashion. experimental results and extensive analyses show that the proposed strategy offers us opportunities to feel free to transfer the vocabulary , leading to more efficient and better performed downstream nlg models.
