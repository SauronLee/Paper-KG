Meta-Learning with Variational Semantic Memory for Word Sense Disambiguation | Yingjun Du | a critical challenge faced by supervised word sense disambiguation \( wsd \) is the lack of large annotated datasets with sufficient coverage of words in their diversity of senses. this inspired recent research on few-shot wsd using meta-learning. while such work has successfully applied meta-learning to learn new word senses from very few examples , its performance still lags behind its fully-supervised counterpart. aiming to further close this gap , we propose a model of semantic memory for wsd in a meta-learning setting. semantic memory encapsulates prior experiences seen throughout the lifetime of the model , which aids better generalization in limited data settings. our model is based on hierarchical variational inference and incorporates an adaptive memory update rule via a hypernetwork. we show our model advances the state of the art in few-shot wsd , supports effective learning in extremely data scarce \( e.g. one-shot \) scenarios and produces meaning prototypes that capture similar senses of distinct words.
