CLEVE: Contrastive Pre-training for Event Extraction | Ziqi Wang | event extraction \( ee \) has considerably benefited from pre-trained language models \( plms \) by fine-tuning. however , existing pre-training methods have not involved modeling event characteristics , resulting in the developed ee models cannot take full advantage of large-scale unsupervised data. to this end , we propose cleve , a contrastive pre-training framework for ee to better learn event knowledge from large unsupervised data and their semantic structures \( e.g. amr \) obtained with automatic parsers. cleve contains a text encoder to learn event semantics and a graph encoder to learn event structures respectively. specifically , the text encoder learns event semantic representations by self-supervised contrastive learning to represent the words of the same events closer than those unrelated words; the graph encoder learns event structure representations by graph contrastive pre-training on parsed event-related semantic structures. the two complementary representations then work together to improve both the conventional supervised ee and the unsupervised “liberal” ee , which requires jointly extracting events and discovering event schemata without any annotated data. experiments on ace 2005 and maven datasets show that cleve achieves significant improvements , especially in the challenging unsupervised setting. the source code and pre-trained checkpoints can be obtained from https://github.com/thu-keg/cleve.
