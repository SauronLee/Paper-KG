Vocabulary Learning via Optimal Transport for Neural Machine Translation | Jingjing Xu | the choice of token vocabulary affects the performance of machine translation. this paper aims to figure out what is a good vocabulary and whether we can find the optimal vocabulary without trial training. to answer these questions , we first provide an alternative understanding of vocabulary from the perspective of information theory. it motivates us to formulate the quest of vocabularization – finding the best token dictionary with a proper size – as an optimal transport \( ot \) problem. we propose volt , a simple and efficient solution without trial training. empirical results show that volt beats widely-used vocabularies in diverse scenarios , including wmt-14 english-german translation , ted bilingual translation , and ted multilingual translation. for example , volt achieves 70% vocabulary size reduction and 0.5 bleu gain on english-german translation. also , compared to bpe-search , volt reduces the search time from 384 gpu hours to 30 gpu hours on english-german translation. codes are available at https://github.com/jingjing-nlp/volt.
