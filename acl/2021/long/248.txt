MultiMET: A Multimodal Dataset for Metaphor Understanding | Dongyu Zhang | metaphor involves not only a linguistic phenomenon , but also a cognitive phenomenon structuring human thought , which makes understanding it challenging. as a means of cognition , metaphor is rendered by more than texts alone , and multimodal information in which vision/audio content is integrated with the text can play an important role in expressing and understanding metaphor. however , previous metaphor processing and understanding has focused on texts , partly due to the unavailability of large-scale datasets with ground truth labels of multimodal metaphor. in this paper , we introduce multimet , a novel multimodal metaphor dataset to facilitate understanding metaphorical information from multimodal text and image. it contains 10 , 437 text-image pairs from a range of sources with multimodal annotations of the occurrence of metaphors , domain relations , sentiments metaphors convey , and author intents. multimet opens the door to automatic metaphor understanding by investigating multimodal cues and their interplay. moreover , we propose a range of strong baselines and show the importance of combining multimodal cues for metaphor understanding. multimet will be released publicly for research.
