EnsLM: Ensemble Language Model for Data Diversity by Semantic Clustering | Zhibin Duan | natural language processing \( nlp \) often faces the problem of data diversity such as different domains , themes , styles , and so on. therefore , a single language model \( lm \) is insufficient to learn all knowledge from diverse samples. to solve this problem , we firstly propose an autoencoding topic model with a mixture prior \( matm \) to perform clustering for the data , where the clusters defined in semantic space describes the data diversity. having obtained the clustering assignment for each sample , we develop the ensemble lm \( enslm \) with the technique of weight modulation. specifically , enslm contains a backbone that is adjusted by a few modulated weights to fit for different sample clusters. as a result , the backbone learns the shared knowledge among all clusters while modulated weights extract the cluster-specific features. enslm can be trained jointly with matm with a flexible lm backbone. we evaluate the effectiveness of both matm and enslm on various tasks.
