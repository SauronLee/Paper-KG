Generalising Multilingual Concept-to-Text NLG with Language Agnostic Delexicalisation | Giulio Zhou | concept-to-text natural language generation is the task of expressing an input meaning representation in natural language. previous approaches in this task have been able to generalise to rare or unseen instances by relying on a delexicalisation of the input. however , this often requires that the input appears verbatim in the output text. this poses challenges in multilingual settings , where the task expands to generate the output text in multiple languages given the same input. in this paper , we explore the application of multilingual models in concept-to-text and propose language agnostic delexicalisation , a novel delexicalisation method that uses multilingual pretrained embeddings , and employs a character-level post-editing model to inflect words in their correct form during relexicalisation. our experiments across five datasets and five languages show that multilingual models outperform monolingual models in concept-to-text and that our framework outperforms previous approaches , especially in low resource conditions.
