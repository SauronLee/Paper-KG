Taming Pre-trained Language Models with N-gram Representations for Low-Resource Domain Adaptation | Shizhe Diao | large pre-trained models such as bert are known to improve different downstream nlp tasks , even when such a model is trained on a generic domain. moreover , recent studies have shown that when large domain-specific corpora are available , continued pre-training on domain-specific data can further improve the performance of in-domain tasks. however , this practice requires significant domain-specific data and computational resources which may not always be available. in this paper , we aim to adapt a generic pretrained model with a relatively small amount of domain-specific data. we demonstrate that by explicitly incorporating multi-granularity information of unseen and domain-specific words via the adaptation of \( word based \) n-grams , the performance of a generic pretrained model can be greatly improved. specifically , we introduce a transformer-based domain-aware n-gram adaptor , t-dna , to effectively learn and incorporate the semantic representation of different combinations of words in the new domain. experimental results illustrate the effectiveness of t-dna on eight low-resource downstream tasks from four domains. we show that t-dna is able to achieve significant improvements compared to existing methods on most tasks using limited data with lower computational costs. moreover , further analyses demonstrate the importance and effectiveness of both unseen words and the information of different granularities. our code is available at https://github.com/shizhediao/t-dna.
