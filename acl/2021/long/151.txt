Contributions of Transformer Attention Heads in Multi- and Cross-lingual Tasks | Weicheng Ma | this paper studies the relative importance of attention heads in transformer-based models to aid their interpretability in cross-lingual and multi-lingual tasks. prior research has found that only a few attention heads are important in each mono-lingual natural language processing \( nlp \) task and pruning the remaining heads leads to comparable or improved performance of the model. however , the impact of pruning attention heads is not yet clear in cross-lingual and multi-lingual tasks. through extensive experiments , we show that \( 1 \) pruning a number of attention heads in a multi-lingual transformer-based model has , in general , positive effects on its performance in cross-lingual and multi-lingual tasks and \( 2 \) the attention heads to be pruned can be ranked using gradients and identified with a few trial experiments. our experiments focus on sequence labeling tasks , with potential applicability on other cross-lingual and multi-lingual tasks. for comprehensiveness , we examine two pre-trained multi-lingual models , namely multi-lingual bert \( mbert \) and xlm-r , on three tasks across 9 languages each. we also discuss the validity of our findings and their extensibility to truly resource-scarce languages and other task settings.
