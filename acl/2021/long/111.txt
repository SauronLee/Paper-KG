Factorising Meaning and Form for Intent-Preserving Paraphrasing | Tom Hosking | we propose a method for generating paraphrases of english questions that retain the original intent but use a different surface form. our model combines a careful choice of training objective with a principled information bottleneck , to induce a latent encoding space that disentangles meaning and form. we train an encoder-decoder model to reconstruct a question from a paraphrase with the same meaning and an exemplar with the same surface form , leading to separated encoding spaces. we use a vector-quantized variational autoencoder to represent the surface form as a set of discrete latent variables , allowing us to use a classifier to select a different surface form at test time. crucially , our method does not require access to an external source of target exemplars. extensive experiments and a human evaluation show that we are able to generate paraphrases with a better tradeoff between semantic preservation and syntactic novelty compared to previous methods.
