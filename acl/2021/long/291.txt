Self-Attention Networks Can Process Bounded Hierarchical Languages | Shunyu Yao | despite their impressive performance in nlp , self-attention networks were recently proved to be limited for processing formal languages with hierarchical structure , such as dyck-k , the language consisting of well-nested parentheses of k types. this suggested that natural language can be approximated well with models that are too weak for formal languages , or that the role of hierarchy and recursion in natural language might be limited. we qualify this implication by proving that self-attention networks can process dyck- \( k , d \) , the subset of dyck-k with depth bounded by d , which arguably better captures the bounded hierarchical structure of natural language. specifically , we construct a hard-attention network with d+1 layers and o \( log k \) memory size \( per token per layer \) that recognizes dyck- \( k , d \) , and a soft-attention network with two layers and o \( log k \) memory size that generates dyck- \( k , d \) . experiments show that self-attention networks trained on dyck- \( k , d \) generalize to longer inputs with near-perfect accuracy , and also verify the theoretical memory advantage of self-attention networks over recurrent networks.
