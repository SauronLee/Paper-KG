BERTAC: Enhancing Transformer-based Language Models with Adversarially Pretrained Convolutional Neural Networks | Jong-Hoon Oh | transformer-based language models \( tlms \) , such as bert , albert and gpt-3 , have shown strong performance in a wide range of nlp tasks and currently dominate the field of nlp. however , many researchers wonder whether these models can maintain their dominance forever. of course , we do not have answers now , but , as an attempt to find better neural architectures and training schemes , we pretrain a simple cnn using a gan-style learning scheme and wikipedia data , and then integrate it with standard tlms. we show that on the glue tasks , the combination of our pretrained cnn with albert outperforms the original albert and achieves a similar performance to that of sota. furthermore , on open-domain qa \( quasar-t and searchqa \) , the combination of the cnn with albert or roberta achieved stronger performance than sota and the original tlms. we hope that this work provides a hint for developing a novel strong network architecture along with its training scheme. our source code and models are available at https://github.com/nict-wisdom/bertac.
