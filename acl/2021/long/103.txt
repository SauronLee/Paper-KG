Diverse Pretrained Context Encodings Improve Document Translation | Domenic Donato | we propose a new architecture for adapting a sentence-level sequence-to-sequence transformer by incorporating multiple pre-trained document context signals and assess the impact on translation performance of \( 1 \) different pretraining approaches for generating these signals , \( 2 \) the quantity of parallel data for which document context is available , and \( 3 \) conditioning on source , target , or source and target contexts. experiments on the nist chinese-english , and iwslt and wmt english-german tasks support four general conclusions: that using pre-trained context representations markedly improves sample efficiency , that adequate parallel data resources are crucial for learning to use document context , that jointly conditioning on multiple context representations outperforms any single representation , and that source context is more valuable for translation performance than target side context. our best multi-context model consistently outperforms the best existing context-aware transformers.
