ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer | Yuanmeng Yan | learning high-quality sentence representations benefits a wide range of natural language processing tasks. though bert-based pre-trained language models achieve high performance on many downstream tasks , the native derived sentence representations are proved to be collapsed and thus produce a poor performance on the semantic textual similarity \( sts \) tasks. in this paper , we present consert , a contrastive framework for self-supervised sentence representation transfer , that adopts contrastive learning to fine-tune bert in an unsupervised and effective way. by making use of unlabeled texts , consert solves the collapse issue of bert-derived sentence representations and make them more applicable for downstream tasks. experiments on sts datasets demonstrate that consert achieves an 8% relative improvement over the previous state-of-the-art , even comparable to the supervised sbert-nli. and when further incorporating nli supervision , we achieve new state-of-the-art performance on sts tasks. moreover , consert obtains comparable results with only 1000 samples available , showing its robustness in data scarcity scenarios.
