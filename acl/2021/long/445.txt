Transfer Learning for Sequence Generation: from Single-source to Multi-source | Xuancheng Huang | multi-source sequence generation \( msg \) is an important kind of sequence generation tasks that takes multiple sources , including automatic post-editing , multi-source translation , multi-document summarization , etc. as msg tasks suffer from the data scarcity problem and recent pretrained models have been proven to be effective for low-resource downstream tasks , transferring pretrained sequence-to-sequence models to msg tasks is essential. although directly finetuning pretrained models on msg tasks and concatenating multiple sources into a single long sequence is regarded as a simple method to transfer pretrained models to msg tasks , we conjecture that the direct finetuning method leads to catastrophic forgetting and solely relying on pretrained self-attention layers to capture cross-source information is not sufficient. therefore , we propose a two-stage finetuning method to alleviate the pretrain-finetune discrepancy and introduce a novel msg model with a fine encoder to learn better representations in msg tasks. experiments show that our approach achieves new state-of-the-art results on the wmt17 ape task and multi-source translation task using the wmt14 test set. when adapted to document-level translation , our framework outperforms strong baselines significantly.
