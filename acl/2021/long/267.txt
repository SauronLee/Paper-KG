Prevent the Language Model from being Overconfident in Neural Machine Translation | Mengqi Miao | the neural machine translation \( nmt \) model is essentially a joint language model conditioned on both the source sentence and partial translation. therefore , the nmt model naturally involves the mechanism of the language model \( lm \) that predicts the next token only based on partial translation. despite its success , nmt still suffers from the hallucination problem , generating fluent but inadequate translations. the main reason is that nmt pays excessive attention to the partial translation while neglecting the source sentence to some extent , namely overconfidence of the lm. accordingly , we define the margin between the nmt and the lm , calculated by subtracting the predicted probability of the lm from that of the nmt model for each token. the margin is negatively correlated to the overconfidence degree of the lm. based on the property , we propose a margin-based token-level objective \( mto \) and a margin-based sentence-level objective \( mso \) to maximize the margin for preventing the lm from being overconfident. experiments on wmt14 english-to-german , wmt19 chinese-to-english , and wmt14 english-to-french translation tasks demonstrate the effectiveness of our approach , with 1.36 , 1.50 , and 0.63 bleu improvements , respectively , compared to the transformer baseline. the human evaluation further verifies that our approaches improve translation adequacy as well as fluency.
