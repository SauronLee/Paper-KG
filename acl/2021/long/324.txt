How is BERT surprised? Layerwise detection of linguistic anomalies | Bai Li | transformer language models have shown remarkable ability in detecting when a word is anomalous in context , but likelihood scores offer no information about the cause of the anomaly. in this work , we use gaussian models for density estimation at intermediate layers of three language models \( bert , roberta , and xlnet \) , and evaluate our method on blimp , a grammaticality judgement benchmark. in lower layers , surprisal is highly correlated to low token frequency , but this correlation diminishes in upper layers. next , we gather datasets of morphosyntactic , semantic , and commonsense anomalies from psycholinguistic studies; we find that the best performing model roberta exhibits surprisal in earlier layers when the anomaly is morphosyntactic than when it is semantic , while commonsense anomalies do not exhibit surprisal at any intermediate layer. these results suggest that language models employ separate mechanisms to detect different types of linguistic anomalies.
