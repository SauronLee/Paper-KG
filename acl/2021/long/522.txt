Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and Improving Models | Tongshuang Wu | while counterfactual examples are useful for analysis and training of nlp models , current generation methods either rely on manual labor to create very few counterfactuals , or only instantiate limited types of perturbations such as paraphrases or word substitutions. we present polyjuice , a general-purpose counterfactual generator that allows for control over perturbation types and locations , trained by finetuning gpt-2 on multiple datasets of paired sentences. we show that polyjuice produces diverse sets of realistic counterfactuals , which in turn are useful in various distinct applications: improving training and evaluation on three different tasks \( with around 70% less annotation effort than manual generation \) , augmenting state-of-the-art explanation techniques , and supporting systematic counterfactual error analysis by revealing behaviors easily missed by human experts.
