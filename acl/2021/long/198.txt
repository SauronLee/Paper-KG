Multi-stage Pre-training over Simplified Multimodal Pre-training Models | Tongtong Liu | multimodal pre-training models , such as lxmert , have achieved excellent results in downstream tasks. however , current pre-trained models require large amounts of training data and have huge model sizes , which make them impossible to apply in low-resource situations. how to obtain similar or even better performance than a larger model under the premise of less pre-training data and smaller model size has become an important problem. in this paper , we propose a new multi-stage pre-training \( msp \) method , which uses information at different granularities from word , phrase to sentence in both texts and images to pre-train a model in stages. we also design several different pre-training tasks suitable for the information granularity in different stage in order to efficiently capture the diverse knowledge from a limited corpus. we take a simplified lxmert \( lxmert-s \) which is with 45.9% parameters of the original lxmert model and only 11.44% of the original pre-training data as the testbed of our msp method. experimental results show that our method achieves comparable performance to the original lxmert model in all downstream tasks , and even outperforms the original model in image-text retrieval task.
