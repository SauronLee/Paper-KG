POS-Constrained Parallel Decoding for Non-autoregressive Generation | Kexin Yang | the multimodality problem has become a major challenge of existing non-autoregressive generation \( nag \) systems. a common solution often resorts to sequence-level knowledge distillation by rebuilding the training dataset through autoregressive generation \( hereinafter known as “teacher ag” \) . the success of such methods may largely depend on a latent assumption , i.e. , the teacher ag is superior to the nag model. however , in this work , we experimentally reveal that this assumption does not always hold for the text generation tasks like text summarization and story ending generation. to provide a feasible solution to the multimodality problem of nag , we propose incorporating linguistic structure \( part-of-speech sequence in particular \) into nag inference instead of relying on teacher ag. more specifically , the proposed pos-constrained parallel decoding \( pospd \) method aims at providing a specific pos sequence to constrain the nag model during decoding. our experiments demonstrate that pospd consistently improves nag models on four text generation tasks to a greater extent compared to knowledge distillation. this observation validates the necessity of exploring the alternatives for sequence-level knowledge distillation.
