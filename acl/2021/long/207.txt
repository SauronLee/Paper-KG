The Limitations of Limited Context for Constituency Parsing | Yuchen Li | incorporating syntax into neural approaches in nlp has a multitude of practical and scientific benefits. for instance , a language model that is syntax-aware is likely to be able to produce better samples; even a discriminative model like bert with a syntax module could be used for core nlp tasks like unsupervised syntactic parsing. rapid progress in recent years was arguably spurred on by the empirical success of the parsing-reading-predict architecture of \( shen et al. , 2018a \) , later simplified by the order neuron lstm of \( shen et al. , 2019 \) . most notably , this is the first time neural approaches were able to successfully perform unsupervised syntactic parsing \( evaluated by various metrics like f-1 score \) . however , even heuristic \( much less fully mathematical \) understanding of why and when these architectures work is lagging severely behind. in this work , we answer representational questions raised by the architectures in \( shen et al. , 2018a , 2019 \) , as well as some transition-based syntax-aware language models \( dyer et al. , 2016 \) : what kind of syntactic structure can current neural approaches to syntax represent \? concretely , we ground this question in the sandbox of probabilistic context-free-grammars \( pcfgs \) , and identify a key aspect of the representational power of these approaches: the amount and directionality of context that the predictor has access to when forced to make parsing decision. we show that with limited context \( either bounded , or unidirectional \) , there are pcfgs , for which these approaches cannot represent the max-likelihood parse; conversely , if the context is unlimited , they can represent the max-likelihood parse of any pcfg.
