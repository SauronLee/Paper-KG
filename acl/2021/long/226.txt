ERNIE-Doc: A Retrospective Long-Document Modeling Transformer | SiYu Ding | transformers are not suited for processing long documents , due to their quadratically increasing memory and time consumption. simply truncating a long document or applying the sparse attention mechanism will incur the context fragmentation problem or lead to an inferior modeling capability against comparable model sizes. in this paper , we propose ernie-doc , a document-level language pretraining model based on recurrence transformers. two well-designed techniques , namely the retrospective feed mechanism and the enhanced recurrence mechanism , enable ernie-doc , which has a much longer effective context length , to capture the contextual information of a complete document. we pretrain ernie-doc to explicitly learn the relationships among segments with an additional document-aware segment-reordering objective. various experiments were conducted on both english and chinese document-level tasks. ernie-doc improved the state-of-the-art language modeling result of perplexity to 16.8 on wikitext-103. moreover , it outperformed competitive pretraining models by a large margin on most language understanding tasks , such as text classification and question answering.
