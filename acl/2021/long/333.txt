BinaryBERT: Pushing the Limit of BERT Quantization | Haoli Bai | the rapid development of large pre-trained language models has greatly increased the demand for model compression techniques , among which quantization is a popular solution. in this paper , we propose binarybert , which pushes bert quantization to the limit by weight binarization. we find that a binary bert is hard to be trained directly than a ternary counterpart due to its complex and irregular loss landscape. therefore , we propose ternary weight splitting , which initializes binarybert by equivalently splitting from a half-sized ternary network. the binary model thus inherits the good performance of the ternary one , and can be further enhanced by fine-tuning the new architecture after splitting. empirical results show that our binarybert has only a slight performance drop compared with the full-precision model while being 24x smaller , achieving the state-of-the-art compression results on the glue and squad benchmarks. code will be released.
