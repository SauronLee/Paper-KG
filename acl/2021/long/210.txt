Towards Quantifiable Dialogue Coherence Evaluation | Zheng Ye | automatic dialogue coherence evaluation has attracted increasing attention and is crucial for developing promising dialogue systems. however , existing metrics have two major limitations: \( a \) they are mostly trained in a simplified two-level setting \( coherent vs. incoherent \) , while humans give likert-type multi-level coherence scores , dubbed as “quantifiable”; \( b \) their predicted coherence scores cannot align with the actual human rating standards due to the absence of human guidance during training. to address these limitations , we propose quantifiable dialogue coherence evaluation \( quantidce \) , a novel framework aiming to train a quantifiable dialogue coherence metric that can reflect the actual human rating standards. specifically , quantidce includes two training stages , multi-level ranking \( mlr \) pre-training and knowledge distillation \( kd \) fine-tuning. during mlr pre-training , a new mlr loss is proposed for enabling the model to learn the coarse judgement of coherence degrees. then , during kd fine-tuning , the pretrained model is further finetuned to learn the actual human rating standards with only very few human-annotated data. to advocate the generalizability even with limited fine-tuning data , a novel kd regularization is introduced to retain the knowledge learned at the pre-training stage. experimental results show that the model trained by quantidce presents stronger correlations with human judgements than the other state-of-the-art metrics.
