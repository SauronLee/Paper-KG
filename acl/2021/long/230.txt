LeeBERT: Learned Early Exit for BERT with cross-level optimization | Wei Zhu | pre-trained language models like bert are performant in a wide range of natural language tasks. however , they are resource exhaustive and computationally expensive for industrial scenarios. thus , early exits are adopted at each layer of bert to perform adaptive computation by predicting easier samples with the first few layers to speed up the inference. in this work , to improve efficiency without performance drop , we propose a novel training scheme called learned early exit for bert \( leebert \) . first , we ask each exit to learn from each other , rather than learning only from the last layer. second , the weights of different loss terms are learned , thus balancing off different objectives. we formulate the optimization of leebert as a bi-level optimization problem , and we propose a novel cross-level optimization \( clo \) algorithm to improve the optimization results. experiments on the glue benchmark show that our proposed methods improve the performance of the state-of-the-art \( sota \) early exit methods for pre-trained models.
