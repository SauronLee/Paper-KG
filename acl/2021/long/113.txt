Reflective Decoding: Beyond Unidirectional Generation with Off-the-Shelf Language Models | Peter West | publicly available , large pretrained language models \( lms \) generate text with remarkable quality , but only sequentially from left to right. as a result , they are not immediately applicable to generation tasks that break the unidirectional assumption , such as paraphrasing or text-infilling , necessitating task-specific supervision. in this paper , we present reflective decoding , a novel unsupervised algorithm that allows for direct application of unidirectional lms to non-sequential tasks. our 2-step approach requires no supervision or even parallel corpora , only two off-the-shelf pretrained lms in opposite directions: forward and backward. first , in the contextualization step , we use lms to generate ensembles of past and future contexts which collectively capture the input \( e.g. the source sentence for paraphrasing \) . second , in the reflection step , we condition on these “context ensembles” , generating outputs that are compatible with them. comprehensive empirical results demonstrate that reflective decoding outperforms strong unsupervised baselines on both paraphrasing and abductive text infilling , significantly narrowing the gap between unsupervised and supervised methods. reflective decoding surpasses multiple supervised baselines on various metrics including human evaluation.
