A Novel Estimator of Mutual Information for Learning to Disentangle Textual Representations | Pierre Colombo | learning disentangled representations of textual data is essential for many natural language tasks such as fair classification , style transfer and sentence generation , among others. the existent dominant approaches in the context of text data either rely on training an adversary \( discriminator \) that aims at making attribute values difficult to be inferred from the latent code or rely on minimising variational bounds of the mutual information between latent code and the value attribute. however , the available methods suffer of the impossibility to provide a fine-grained control of the degree \( or force \) of disentanglement. in contrast to adversarial methods , which are remarkably simple , although the adversary seems to be performing perfectly well during the training phase , after it is completed a fair amount of information about the undesired attribute still remains. this paper introduces a novel variational upper bound to the mutual information between an attribute and the latent code of an encoder. our bound aims at controlling the approximation error via the renyiâ€™s divergence , leading to both better disentangled representations and in particular , a precise control of the desirable degree of disentanglement than state-of-the-art methods proposed for textual data. furthermore , it does not suffer from the degeneracy of other losses in multi-class scenarios. we show the superiority of this method on fair classification and on textual style transfer tasks. additionally , we provide new insights illustrating various trade-offs in style transfer when attempting to learn disentangled representations and quality of the generated sentence.
