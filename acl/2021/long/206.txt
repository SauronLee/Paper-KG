Multi-View Cross-Lingual Structured Prediction with Minimum Supervision | Zechuan Hu | in structured prediction problems , cross-lingual transfer learning is an efficient way to train quality models for low-resource languages , and further improvement can be obtained by learning from multiple source languages. however , not all source models are created equal and some may hurt performance on the target language. previous work has explored the similarity between source and target sentences as an approximate measure of strength for different source models. in this paper , we propose a multi-view framework , by leveraging a small number of labeled target sentences , to effectively combine multiple source models into an aggregated source view at different granularity levels \( language , sentence , or sub-structure \) , and transfer it to a target view based on a task-specific model. by encouraging the two views to interact with each other , our framework can dynamically adjust the confidence level of each source model and improve the performance of both views during training. experiments for three structured prediction tasks on sixteen data sets show that our framework achieves significant improvement over all existing approaches , including these with access to additional source language data.
