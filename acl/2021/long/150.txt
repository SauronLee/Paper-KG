RedditBias: A Real-World Resource for Bias Evaluation and Debiasing of Conversational Language Models | Soumya Barikeri | text representation models are prone to exhibit a range of societal biases , reflecting the non-controlled and biased nature of the underlying pretraining data , which consequently leads to severe ethical issues and even bias amplification. recent work has predominantly focused on measuring and mitigating bias in pretrained language models. surprisingly , the landscape of bias measurements and mitigation resources and methods for conversational language models is still very scarce: it is limited to only a few types of bias , artificially constructed resources , and completely ignores the impact that debiasing methods may have on the final perfor mance in dialog tasks , e.g. , conversational response generation. in this work , we present redditbias , the first conversational data set grounded in the actual human conversations from reddit , allowing for bias measurement and mitigation across four important bias dimensions: gender , race , religion , and queerness. further , we develop an evaluation framework which simultaneously 1 \) measures bias on the developed redditbias resource , and 2 \) evaluates model capability in dialog tasks after model debiasing. we use the evaluation framework to benchmark the widely used conversational dialogpt model along with the adaptations of four debiasing methods. our results indicate that dialogpt is biased with respect to religious groups and that some debiasing techniques can remove this bias while preserving downstream task performance.
