The Art of Abstention: Selective Prediction and Error Regularization for Natural Language Processing | Ji Xin | in selective prediction , a classifier is allowed to abstain from making predictions on low-confidence examples. though this setting is interesting and important , selective prediction has rarely been examined in natural language processing \( nlp \) tasks. to fill this void in the literature , we study in this paper selective prediction for nlp , comparing different models and confidence estimators. we further propose a simple error regularization trick that improves confidence estimation without substantially increasing the computation budget. we show that recent pre-trained transformer models simultaneously improve both model accuracy and confidence estimation effectiveness. we also find that our proposed regularization improves confidence estimation and can be applied to other relevant scenarios , such as using classifier cascades for accuracyâ€“efficiency trade-offs. source code for this paper can be found at https://github.com/castorini/transformers-selective.
