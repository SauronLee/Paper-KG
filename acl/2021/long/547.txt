Cross-replication Reliability - An Empirical Approach to Interpreting Inter-rater Reliability | Ka Wong | when collecting annotations and labeled data from humans , a standard practice is to use inter-rater reliability \( irr \) as a measure of data goodness \( hallgren , 2012 \) . metrics such as krippendorff’s alpha or cohen’s kappa are typically required to be above a threshold of 0.6 \( landis and koch , 1977 \) . these absolute thresholds are unreasonable for crowdsourced data from annotators with high cultural and training variances , especially on subjective topics. we present a new alternative to interpreting irr that is more empirical and contextualized. it is based upon benchmarking irr against baseline measures in a replication , one of which is a novel cross-replication reliability \( xrr \) measure based on cohen’s \( 1960 \) kappa. we call this approach the xrr framework. we opensource a replication dataset of 4 million human judgements of facial expressions and analyze it with the proposed framework. we argue this framework can be used to measure the quality of crowdsourced datasets.
