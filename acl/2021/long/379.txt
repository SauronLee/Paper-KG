Risk Minimization for Zero-shot Sequence Labeling | Zechuan Hu | zero-shot sequence labeling aims to build a sequence labeler without human-annotated datasets. one straightforward approach is utilizing existing systems \( source models \) to generate pseudo-labeled datasets and train a target sequence labeler accordingly. however , due to the gap between the source and the target languages/domains , this approach may fail to recover the true labels. in this paper , we propose a novel unified framework for zero-shot sequence labeling with minimum risk training and design a new decomposable risk function that models the relations between the predicted labels from the source models and the true labels. by making the risk function trainable , we draw a connection between minimum risk training and latent variable model learning. we propose a unified learning algorithm based on the expectation maximization \( em \) algorithm. we extensively evaluate our proposed approaches on cross-lingual/domain sequence labeling tasks over twenty-one datasets. the results show that our approaches outperform state-of-the-art baseline systems.
