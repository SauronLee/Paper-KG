Beyond Noise: Mitigating the Impact of Fine-grained Semantic Divergences on Neural Machine Translation | Eleftheria Briakou | while it has been shown that neural machine translation \( nmt \) is highly sensitive to noisy parallel training samples , prior work treats all types of mismatches between source and target as noise. as a result , it remains unclear how samples that are mostly equivalent but contain a small number of semantically divergent tokens impact nmt training. to close this gap , we analyze the impact of different types of fine-grained semantic divergences on transformer models. we show that models trained on synthetic divergences output degenerated text more frequently and are less confident in their predictions. based on these findings , we introduce a divergent-aware nmt framework that uses factors to help nmt recover from the degradation caused by naturally occurring divergences , improving both translation quality and model calibration on en-fr tasks.
