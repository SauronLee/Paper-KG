Glancing Transformer for Non-Autoregressive Neural Machine Translation | Lihua Qian | recent work on non-autoregressive neural machine translation \( nat \) aims at improving the efficiency by parallel decoding without sacrificing the quality. however , existing nat methods are either inferior to transformer or require multiple decoding passes , leading to reduced speedup. we propose the glancing language model \( glm \) for single-pass parallel generation models. with glm , we develop glancing transformer \( glat \) for machine translation. with only single-pass parallel decoding , glat is able to generate high-quality translation with 8×-15× speedup. note that glat does not modify the network architecture , which is a training method to learn word interdependency. experiments on multiple wmt language directions show that glat outperforms all previous single pass non-autoregressive methods , and is nearly comparable to transformer , reducing the gap to 0.25-0.9 bleu points.
