Online Learning Meets Machine Translation Evaluation: Finding the Best Systems with the Least Human Effort | Vânia Mendonça | in machine translation , assessing the quality of a large amount of automatic translations can be challenging. automatic metrics are not reliable when it comes to high performing systems. in addition , resorting to human evaluators can be expensive , especially when evaluating multiple systems. to overcome the latter challenge , we propose a novel application of online learning that , given an ensemble of machine translation systems , dynamically converges to the best systems , by taking advantage of the human feedback available. our experiments on wmt’19 datasets show that our online approach quickly converges to the top-3 ranked systems for the language pairs considered , despite the lack of human feedback for many translations.
