Rethinking Stealthiness of Backdoor Attack against NLP Models | Wenkai Yang | recent researches have shown that large natural language processing \( nlp \) models are vulnerable to a kind of security threat called the backdoor attack. backdoor attacked models can achieve good performance on clean test sets but perform badly on those input sentences injected with designed trigger words. in this work , we point out a potential problem of current backdoor attacking research: its evaluation ignores the stealthiness of backdoor attacks , and most of existing backdoor attacking methods are not stealthy either to system deployers or to system users. to address this issue , we first propose two additional stealthiness-based metrics to make the backdoor attacking evaluation more credible. we further propose a novel word-based backdoor attacking method based on negative data augmentation and modifying word embeddings , making an important step towards achieving stealthy backdoor attacking. experiments on sentiment analysis and toxic detection tasks show that our method is much stealthier while maintaining pretty good attacking performance. our code is available at https://github.com/lancopku/sos.
