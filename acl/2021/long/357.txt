Recursive Tree-Structured Self-Attention for Answer Sentence Selection | Khalil Mrini | syntactic structure is an important component of natural language text. recent top-performing models in answer sentence selection \( as2 \) use self-attention and transfer learning , but not syntactic structure. tree structures have shown strong performance in tasks with sentence pair input like semantic relatedness. we investigate whether tree structures can boost performance in as2. we introduce the tree aggregation transformer: a novel recursive , tree-structured self-attention model for as2. the recursive nature of our model is able to represent all levels of syntactic parse trees with only one additional self-attention layer. without transfer learning , we establish a new state of the art on the popular trecqa and wikiqa benchmark datasets. additionally , we evaluate our method on four community question answering datasets , and find that tree-structured representations have limitations with noisy user-generated text. we conduct probing experiments to evaluate how our models leverage tree structures across datasets. our findings show that the ability of tree-structured models to successfully absorb syntactic information is strongly correlated with a higher performance in as2.
