ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic | Muhammad Abdul-Mageed | pre-trained language models \( lms \) are currently integral to many natural language processing systems. although multilingual lms were also introduced to serve many languages , these have limitations such as being costly at inference time and the size and diversity of non-english data involved in their pre-training. we remedy these issues for a collection of diverse arabic varieties by introducing two powerful deep bidirectional transformer-based models , arbert and marbert. to evaluate our models , we also introduce arlue , a new benchmark for multi-dialectal arabic language understanding evaluation. arlue is built using 42 datasets targeting six different task clusters , allowing us to offer a series of standardized experiments under rich conditions. when fine-tuned on arlue , our models collectively achieve new state-of-the-art results across the majority of tasks \( 37 out of 48 classification tasks , on the 42 datasets \) . our best model acquires the highest arlue score \( 77.40 \) across all six task clusters , outperforming all other models including xlm-r large \( 3.4x larger size \) . our models are publicly available at https://github.com/ubc-nlp/marbert and arlue will be released through the same repository.
