T1	Titles 0 78	Contributions of Transformer Attention Heads in Multi- and Cross-lingual Tasks
T2	Persons 81 92	Weicheng Ma
R1	Author Arg1:T2 Arg2:T1	
T3	Issues 257 587	prior research has found that only a few attention heads are important in each mono-lingual natural language processing \( nlp \) task and pruning the remaining heads leads to comparable or improved performance of the model. however , the impact of pruning attention heads is not yet clear in cross-lingual and multi-lingual tasks
T4	Models 1139 1164	namely multi-lingual bert
T5	Models 1168 1173	mbert
R2	Acronym Arg1:T5 Arg2:T4	
T6	Models 1181 1186	xlm-r
T7	Evaluations 1189 1227	on three tasks across 9 languages each
R3	Base Arg1:T6 Arg2:T7	
R4	Base Arg1:T4 Arg2:T7	
T8	Definitions 1104 1136	pre-trained multi-lingual models
R5	Attributes Arg1:T8 Arg2:T4	
R6	Attributes Arg1:T8 Arg2:T6	
T9	Theories 642 816	pruning a number of attention heads in a multi-lingual transformer-based model has , in general , positive effects on its performance in cross-lingual and multi-lingual tasks
R7	Result Arg1:T7 Arg2:T9	
R8	Solution Arg1:T9 Arg2:T3	
R9	Contributions Arg1:T9 Arg2:T1	
T10	Models 141 184	attention heads in transformer-based models
R10	Challenges Arg1:T3 Arg2:T10	
