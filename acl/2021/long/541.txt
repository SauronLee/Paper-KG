Dynamic Contextualized Word Embeddings | Valentin Hofmann | static word embeddings that represent words by a single vector cannot capture the variability of word meaning in different linguistic and extralinguistic contexts. building on prior work on contextualized and dynamic word embeddings , we introduce dynamic contextualized word embeddings that represent words as a function of both linguistic and extralinguistic context. based on a pretrained language model \( plm \) , dynamic contextualized word embeddings model time and social space jointly , which makes them attractive for a range of nlp tasks involving semantic variability. we highlight potential application scenarios by means of qualitative and quantitative analyses on four english datasets.
