Best of Both Worlds: Making High Accuracy Non-incremental Transformer-based Disfluency Detection Incremental | Morteza Rohanian | while transformer-based text classifiers pre-trained on large volumes of text have yielded significant improvements on a wide range of computational linguistics tasks , their implementations have been unsuitable for live incremental processing thus far , operating only on the level of complete sentence inputs. we address the challenge of introducing methods for word-by-word left-to-right incremental processing to transformers such as bert , models without an intrinsic sense of linear order. we modify the training method and live decoding of non-incremental models to detect speech disfluencies with minimum latency and without pre-segmentation of dialogue acts. we experiment with several decoding methods to predict the rightward context of the word currently being processed using a gpt-2 language model and apply a bert-based disfluency detector to sequences , including predicted words. we show our method of incrementalising transformers maintains most of their high non-incremental performance while operating strictly incrementally. we also evaluate our modelsâ€™ incremental performance to establish the trade-off between incremental performance and final performance , using different prediction strategies. we apply our system to incremental speech recognition results as they arrive into a live system and achieve state-of-the-art results in this setting.
