How Knowledge Graph and Attention Help? A Qualitative Analysis into Bag-level Relation Extraction | Zikun Hu | knowledge graph \( kg \) and attention mechanism have been demonstrated effective in introducing and selecting useful information for weakly supervised methods. however , only qualitative analysis and ablation study are provided as evidence. in this paper , we contribute a dataset and propose a paradigm to quantitatively evaluate the effect of attention and kg on bag-level relation extraction \( re \) . we find that \( 1 \) higher attention accuracy may lead to worse performance as it may harm the modelâ€™s ability to extract entity mention features; \( 2 \) the performance of attention is largely influenced by various noise distribution patterns , which is closely related to real-world datasets; \( 3 \) kg-enhanced attention indeed improves re performance , while not through enhanced attention but by incorporating entity prior; and \( 4 \) attention mechanism may exacerbate the issue of insufficient training data. based on these findings , we show that a straightforward variant of re model can achieve significant improvements \( 6% auc on average \) on two real-world datasets as compared with three state-of-the-art baselines. our codes and datasets are available at https://github.com/zig-kwin-hu/how-kg-att-help.
