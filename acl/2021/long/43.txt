KM-BART: Knowledge Enhanced Multimodal BART for Visual Commonsense Generation | Yiran Xing | we present knowledge enhanced multimodal bart \( km-bart \) , which is a transformer-based sequence-to-sequence model capable of reasoning about commonsense knowledge from multimodal inputs of images and texts. we adapt the generative bart architecture \( lewis et al. , 2020 \) to a multimodal model with visual and textual inputs. we further develop novel pretraining tasks to improve the model performance on the visual commonsense generation \( vcg \) task. in particular , our pretraining task of knowledge-based commonsense generation \( kcg \) boosts model performance on the vcg task by leveraging commonsense knowledge from a large language model pretrained on external commonsense knowledge graphs. to the best of our knowledge , we are the first to propose a dedicated task for improving model performance on the vcg task. experimental results show that our model reaches state-of-the-art performance on the vcg task \( park et al. , 2020 \) by applying these novel pretraining tasks.
