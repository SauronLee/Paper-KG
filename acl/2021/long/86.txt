Selecting Informative Contexts Improves Language Model Fine-tuning | Richard Antonello | language model fine-tuning is essential for modern natural language processing , but is computationally expensive and time-consuming. further , the effectiveness of fine-tuning is limited by the inclusion of training examples that negatively affect performance. here we present a general fine-tuning method that we call information gain filtration for improving the overall training efficiency and final performance of language model fine-tuning. we define the information gain of an example as the improvement on a validation metric after training on that example. a secondary learner is then trained to approximate this quantity. during fine-tuning , this learner selects informative examples and skips uninformative ones. we show that our method has consistent improvement across datasets , fine-tuning tasks , and language model architectures. for example , we achieve a median perplexity of 54.0 on a books dataset compared to 57.3 for standard fine-tuning. we present statistical evidence that offers insight into the improvements of our method over standard fine-tuning. the generality of our method leads us to propose a new paradigm for language model fine-tuning â€” we encourage researchers to release pretrained secondary learners on common corpora to promote efficient and effective fine-tuning , thereby improving the performance and reducing the overall energy footprint of language model fine-tuning.
