Meta-KD: A Meta Knowledge Distillation Framework for Language Model Compression across Domains | Haojie Pan | pre-trained language models have been applied to various nlp tasks with considerable performance gains. however , the large model sizes , together with the long inference time , limit the deployment of such models in real-time applications. one line of model compression approaches considers knowledge distillation to distill large teacher models into small student models. most of these studies focus on single-domain only , which ignores the transferable knowledge from other domains. we notice that training a teacher with transferable knowledge digested across domains can achieve better generalization capability to help knowledge distillation. hence we propose a meta-knowledge distillation \( meta-kd \) framework to build a meta-teacher model that captures transferable knowledge across domains and passes such knowledge to students. specifically , we explicitly force the meta-teacher to capture transferable knowledge at both instance-level and feature-level from multiple domains , and then propose a meta-distillation algorithm to learn single-domain student models with guidance from the meta-teacher. experiments on public multi-domain nlp tasks show the effectiveness and superiority of the proposed meta-kd framework. further , we also demonstrate the capability of meta-kd in the settings where the training data is scarce.
