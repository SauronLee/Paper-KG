A Pre-training Strategy for Zero-Resource Response Selection in Knowledge-Grounded Conversations | Chongyang Tao | recently , many studies are emerging towards building a retrieval-based dialogue system that is able to effectively leverage background knowledge \( e.g. , documents \) when conversing with humans. however , it is non-trivial to collect large-scale dialogues that are naturally grounded on the background documents , which hinders the effective and adequate training of knowledge selection and response matching. to overcome the challenge , we consider decomposing the training of the knowledge-grounded response selection into three tasks including: 1 \) query-passage matching task; 2 \) query-dialogue history matching task; 3 \) multi-turn response matching task , and joint learning all these tasks in a unified pre-trained language model. the former two tasks could help the model in knowledge selection and comprehension , while the last task is designed for matching the proper response with the given query and background knowledge \( dialogue history \) . by this means , the model can be learned to select relevant knowledge and distinguish proper response , with the help of ad-hoc retrieval corpora and a large number of ungrounded multi-turn dialogues. experimental results on two benchmarks of knowledge-grounded response selection indicate that our model can achieve comparable performance with several existing methods that rely on crowd-sourced data for training.
