Enhancing the generalization for Intent Classification and Out-of-Domain Detection in SLU | Yilin Shen | intent classification is a major task in spoken language understanding \( slu \) . since most models are built with pre-collected in-domain \( ind \) training utterances , their ability to detect unsupported out-of-domain \( ood \) utterances has a critical effect in practical use. recent works have shown that using extra data and labels can improve the ood detection performance , yet it could be costly to collect such data. this paper proposes to train a model with only ind data while supporting both ind intent classification and ood detection. our method designs a novel domain-regularized module \( drm \) to reduce the overconfident phenomenon of a vanilla classifier , achieving a better generalization in both cases. besides , drm can be used as a drop-in replacement for the last layer in any neural network-based intent classifier , providing a low-cost strategy for a significant improvement. the evaluation on four datasets shows that our method built on bert and roberta models achieves state-of-the-art performance against existing approaches and the strong baselines we created for the comparisons.
