Lexicon Enhanced Chinese Sequence Labeling Using BERT Adapter | Wei Liu | lexicon information and pre-trained models , such as bert , have been combined to explore chinese sequence labeling tasks due to their respective strengths. however , existing methods solely fuse lexicon features via a shallow and random initialized sequence layer and do not integrate them into the bottom layers of bert. in this paper , we propose lexicon enhanced bert \( lebert \) for chinese sequence labeling , which integrates external lexicon knowledge into bert layers directly by a lexicon adapter layer. compared with existing methods , our model facilitates deep lexicon knowledge fusion at the lower layers of bert. experiments on ten chinese datasets of three tasks including named entity recognition , word segmentation , and part-of-speech tagging , show that lebert achieves state-of-the-art results.
