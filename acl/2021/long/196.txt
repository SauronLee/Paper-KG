Self-Guided Contrastive Learning for BERT Sentence Representations | Taeuk Kim | although bert and its variants have reshaped the nlp landscape , it still remains unclear how best to derive sentence embeddings from such pre-trained transformers. in this work , we propose a contrastive learning method that utilizes self-guidance for improving the quality of bert sentence representations. our method fine-tunes bert in a self-supervised fashion , does not rely on data augmentation , and enables the usual [cls] token embeddings to function as sentence vectors. moreover , we redesign the contrastive learning objective \( nt-xent \) and apply it to sentence representation learning. we demonstrate with extensive experiments that our approach is more effective than competitive baselines on diverse sentence-related tasks. we also show it is efficient at inference and robust to domain shifts.
