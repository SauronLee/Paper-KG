CIL: Contrastive Instance Learning Framework for Distantly Supervised Relation Extraction | Tao Chen | the journey of reducing noise from distant supervision \( ds \) generated training data has been started since the ds was first introduced into the relation extraction \( re \) task. for the past decade , researchers apply the multi-instance learning \( mil \) framework to find the most reliable feature from a bag of sentences. although the pattern of mil bags can greatly reduce ds noise , it fails to represent many other useful sentence features in the datasets. in many cases , these sentence features can only be acquired by extra sentence-level human annotation with heavy costs. therefore , the performance of distantly supervised re models is bounded. in this paper , we go beyond typical mil framework and propose a novel contrastive instance learning \( cil \) framework. specifically , we regard the initial mil as the relational triple encoder and constraint positive pairs against negative pairs for each instance. experiments demonstrate the effectiveness of our proposed framework , with significant improvements over the previous methods on nyt10 , gds and kbp.
