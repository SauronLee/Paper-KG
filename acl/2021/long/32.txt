Self-Supervised Multimodal Opinion Summarization | Jinbae Im | recently , opinion summarization , which is the generation of a summary from multiple reviews , has been conducted in a self-supervised manner by considering a sampled review as a pseudo summary. however , non-text data such as image and metadata related to reviews have been considered less often. to use the abundant information contained in non-text data , we propose a self-supervised multimodal opinion summarization framework called multimodalsum. our framework obtains a representation of each modality using a separate encoder for each modality , and the text decoder generates a summary. to resolve the inherent heterogeneity of multimodal data , we propose a multimodal training pipeline. we first pretrain the text encoderâ€“decoder based solely on text modality data. subsequently , we pretrain the non-text modality encoders by considering the pretrained text decoder as a pivot for the homogeneous representation of multimodal data. finally , to fuse multimodal representations , we train the entire framework in an end-to-end manner. we demonstrate the superiority of multimodalsum by conducting experiments on yelp and amazon datasets.
