Stacked Acoustic-and-Textual Encoding: Integrating the Pre-trained Models into Speech Translation Encoders | Chen Xu | encoder pre-training is promising in end-to-end speech translation \( st \) , given the fact that speech-to-translation data is scarce. but st encoders are not simple instances of automatic speech recognition \( asr \) or machine translation \( mt \) encoders. for example , we find that asr encoders lack the global context representation , which is necessary for translation , whereas mt encoders are not designed to deal with long but locally attentive acoustic sequences. in this work , we propose a stacked acoustic-and-textual encoding \( sate \) method for speech translation. our encoder begins with processing the acoustic sequence as usual , but later behaves more like an mt encoder for a global representation of the input sequence. in this way , it is straightforward to incorporate the pre-trained models into the system. also , we develop an adaptor module to alleviate the representation inconsistency between the pre-trained asr encoder and mt encoder , and develop a multi-teacher knowledge distillation method to preserve the pre-training knowledge. experimental results on the librispeech en-fr and must-c en-de st tasks show that our method achieves state-of-the-art bleu scores of 18.3 and 25.2. to our knowledge , we are the first to develop an end-to-end st system that achieves comparable or even better bleu performance than the cascaded st counterpart when large-scale asr and mt data is available.
