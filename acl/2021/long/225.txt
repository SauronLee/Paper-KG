Lightweight Cross-Lingual Sentence Representation Learning | Zhuoyuan Mao | large-scale models for learning fixed-dimensional cross-lingual sentence representations like laser \( artetxe and schwenk , 2019b \) lead to significant improvement in performance on downstream tasks. however , further increases and modifications based on such large-scale models are usually impractical due to memory limitations. in this work , we introduce a lightweight dual-transformer architecture with just 2 layers for generating memory-efficient cross-lingual sentence representations. we explore different training tasks and observe that current cross-lingual training tasks leave a lot to be desired for this shallow architecture. to ameliorate this , we propose a novel cross-lingual language model , which combines the existing single-word masked language model with the newly proposed cross-lingual token-level reconstruction task. we further augment the training task by the introduction of two computationally-lite sentence-level contrastive learning tasks to enhance the alignment of cross-lingual sentence representation space , which compensates for the learning bottleneck of the lightweight transformer for generative tasks. our comparisons with competing models on cross-lingual sentence retrieval and multilingual document classification confirm the effectiveness of the newly proposed training tasks for a shallow model.
