A Human-machine Collaborative Framework for Evaluating Malevolence in Dialogues | Yangjun Zhang | conversational dialogue systems \( cdss \) are hard to evaluate due to the complexity of natural language. automatic evaluation of dialogues often shows insufficient correlation with human judgements. human evaluation is reliable but labor-intensive. we introduce a human-machine collaborative framework , hmceval , that can guarantee reliability of the evaluation outcomes with reduced human effort. hmceval casts dialogue evaluation as a sample assignment problem , where we need to decide to assign a sample to a human or a machine for evaluation. hmceval includes a model confidence estimation module to estimate the confidence of the predicted sample assignment , and a human effort estimation module to estimate the human effort should the sample be assigned to human evaluation , as well as a sample assignment execution module that finds the optimum assignment solution based on the estimated confidence and effort. we assess the performance of hmceval on the task of evaluating malevolence in dialogues. the experimental results show that hmceval achieves around 99% evaluation accuracy with half of the human effort spared , showing that hmceval provides reliable evaluation outcomes while reducing human effort by a large amount.
