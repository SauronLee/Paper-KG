Robustness Testing of Language Understanding in Task-Oriented Dialog | Jiexi Liu | most language understanding models in task-oriented dialog systems are trained on a small amount of annotated training data , and evaluated in a small set from the same distribution. however , these models can lead to system failure or undesirable output when being exposed to natural language perturbation or variation in practice. in this paper , we conduct comprehensive evaluation and analysis with respect to the robustness of natural language understanding models , and introduce three important aspects related to language understanding in real-world dialog systems , namely , language variety , speech characteristics , and noise perturbation. we propose a model-agnostic toolkit laug to approximate natural language perturbations for testing the robustness issues in task-oriented dialog. four data augmentation approaches covering the three aspects are assembled in laug , which reveals critical robustness issues in state-of-the-art models. the augmented dataset through laug can be used to facilitate future research on the robustness testing of language understanding in task-oriented dialog.
