On the Efficacy of Adversarial Data Collection for Question Answering: Results from a Large-Scale Randomized Study | Divyansh Kaushik | in adversarial data collection \( adc \) , a human workforce interacts with a model in real time , attempting to produce examples that elicit incorrect predictions. researchers hope that models trained on these more challenging datasets will rely less on superficial patterns , and thus be less brittle. however , despite adcâ€™s intuitive appeal , it remains unclear when training on adversarial datasets produces more robust models. in this paper , we conduct a large-scale controlled study focused on question answering , assigning workers at random to compose questions either \( i \) adversarially \( with a model in the loop \) ; or \( ii \) in the standard fashion \( without a model \) . across a variety of models and datasets , we find that models trained on adversarial data usually perform better on other adversarial datasets but worse on a diverse collection of out-of-domain evaluation sets. finally , we provide a qualitative analysis of adversarial \( vs standard \) data , identifying key differences and offering guidance for future research.
