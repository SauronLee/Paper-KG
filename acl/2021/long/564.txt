All That’s ‘Human’ Is Not Gold: Evaluating Human Evaluation of Generated Text | Elizabeth Clark | human evaluations are typically considered the gold standard in natural language generation , but as models’ fluency improves , how well can evaluators detect and judge machine-generated text \? we run a study assessing non-experts’ ability to distinguish between human- and machine-authored text \( gpt2 and gpt3 \) in three domains \( stories , news articles , and recipes \) . we find that , without training , evaluators distinguished between gpt3- and human-authored text at random chance level. we explore three approaches for quickly training evaluators to better identify gpt3-authored text \( detailed instructions , annotated examples , and paired examples \) and find that while evaluators’ accuracy improved up to 55% , it did not significantly improve across the three domains. given the inconsistent results across text domains and the often contradictory reasons evaluators gave for their judgments , we examine the role untrained human evaluations play in nlg evaluation and provide recommendations to nlg researchers for improving human evaluations of text generated from state-of-the-art models.
