BERTifying the Hidden Markov Model for Multi-Source Weakly Supervised Named Entity Recognition | Yinghao Li | we study the problem of learning a named entity recognition \( ner \) tagger using noisy labels from multiple weak supervision sources. though cheap to obtain , the labels from weak supervision sources are often incomplete , inaccurate , and contradictory , making it difficult to learn an accurate ner model. to address this challenge , we propose a conditional hidden markov model \( chmm \) , which can effectively infer true labels from multi-source noisy labels in an unsupervised way. chmm enhances the classic hidden markov model with the contextual representation power of pre-trained language models. specifically , chmm learns token-wise transition and emission probabilities from the bert embeddings of the input tokens to infer the latent true labels from noisy observations. we further refine chmm with an alternate-training approach \( chmm-alt \) . it fine-tunes a bert-ner model with the labels inferred by chmm , and this bert-nerâ€™s output is regarded as an additional weak source to train the chmm in return. experiments on four ner benchmarks from various domains show that our method outperforms state-of-the-art weakly supervised ner models by wide margins.
