Bootstrapped Unsupervised Sentence Representation Learning | Yan Zhang | as high-quality labeled data is scarce , unsupervised sentence representation learning has attracted much attention. in this paper , we propose a new framework with a two-branch siamese network which maximizes the similarity between two augmented views of each sentence. specifically , given one augmented view of the input sentence , the online network branch is trained by predicting the representation yielded by the target network of the same sentence under another augmented view. meanwhile , the target network branch is bootstrapped with a moving average of the online network. the proposed method significantly outperforms other state-of-the-art unsupervised methods on semantic textual similarity \( sts \) and classification tasks. it can be adopted as a post-training procedure to boost the performance of the supervised methods. we further extend our method for learning multilingual sentence representations and demonstrate its effectiveness on cross-lingual sts tasks. our code is available at
