Parameter-Efficient Transfer Learning with Diff Pruning | Demi Guo | the large size of pretrained networks makes them difficult to deploy for multiple tasks in storage-constrained settings. diff pruning enables parameter-efficient transfer learning that scales well with new tasks. the approach learns a task-specific “diff” vector that extends the original pretrained parameters. this diff vector is adaptively pruned during training with a differentiable approximation to the l0-norm penalty to encourage sparsity. as the number of tasks increases , diff pruning remains parameter-efficient , as it requires storing only a small diff vector for each task. since it does not require access to all tasks during training , it is attractive in on-device deployment settings where tasks arrive in stream or even from different providers. diff pruning can match the performance of finetuned baselines on the glue benchmark while only modifying 0.5% of the pretrained model’s parameters per task and scales favorably in comparison to popular pruning approaches.
