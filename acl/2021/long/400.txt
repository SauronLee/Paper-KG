Data Augmentation with Adversarial Training for Cross-Lingual NLI | Xin Dong | due to recent pretrained multilingual representation models , it has become feasible to exploit labeled data from one language to train a cross-lingual model that can then be applied to multiple new languages. in practice , however , we still face the problem of scarce labeled data , leading to subpar results. in this paper , we propose a novel data augmentation strategy for better cross-lingual natural language inference by enriching the data to reflect more diversity in a semantically faithful way. to this end , we propose two methods of training a generative model to induce synthesized examples , and then leverage the resulting data using an adversarial training regimen for more robustness. in a series of detailed experiments , we show that this fruitful combination leads to substantial gains in cross-lingual inference.
