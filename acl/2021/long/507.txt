Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search | Gyuwan Kim | despite transformersâ€™ impressive accuracy , their computational cost is often prohibitive to use with limited computational resources. most previous approaches to improve inference efficiency require a separate model for each possible computational budget. in this paper , we extend power-bert \( goyal et al. , 2020 \) and propose length-adaptive transformer that can be used for various inference scenarios after one-shot training. we train a transformer with lengthdrop , a structural variant of dropout , which stochastically determines a sequence length at each layer. we then conduct a multi-objective evolutionary search to find a length configuration that maximizes the accuracy and minimizes the efficiency metric under any given computational budget. additionally , we significantly extend the applicability of power-bert beyond sequence-level classification into token-level classification with drop-and-restore process that drops word-vectors temporarily in intermediate layers and restores at the last layer if necessary. we empirically verify the utility of the proposed approach by demonstrating the superior accuracy-efficiency trade-off under various setups , including span-based question answering and text classification. code is available at https://github.com/clovaai/lengthadaptive-transformer.
