OpenMEVA: A Benchmark for Evaluating Open-ended Story Generation Metrics | Jian Guan | automatic metrics are essential for developing natural language generation \( nlg \) models , particularly for open-ended language generation tasks such as story generation. however , existing automatic metrics are observed to correlate poorly with human evaluation. the lack of standardized benchmark datasets makes it difficult to fully evaluate the capabilities of a metric and fairly compare different metrics. therefore , we propose openmeva , a benchmark for evaluating open-ended story generation metrics. openmeva provides a comprehensive test suite to assess the capabilities of metrics , including \( a \) the correlation with human judgments , \( b \) the generalization to different model outputs and datasets , \( c \) the ability to judge story coherence , and \( d \) the robustness to perturbations. to this end , openmeva includes both manually annotated stories and auto-constructed test examples. we evaluate existing metrics on openmeva and observe that they have poor correlation with human judgments , fail to recognize discourse-level incoherence , and lack inferential knowledge \( e.g. , causal order between events \) , the generalization ability and robustness. our study presents insights for developing nlg models and metrics in further research.
