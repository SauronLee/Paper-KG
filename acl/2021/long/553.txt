ReadOnce Transformers: Reusable Representations of Text for Transformers | Shih-Ting Lin | we present readonce transformers , an approach to convert a transformer-based model into one that can build an information-capturing , task-independent , and compressed representation of text. the resulting representation is reusable across different examples and tasks , thereby requiring a document shared across many examples or tasks to only be read once. this leads to faster training and evaluation of models. additionally , we extend standard text-to-text transformer models to representation+text-to-text models , and evaluate on multiple downstream tasks: multi-hop qa , abstractive qa , and long-document summarization. our one-time computed representation results in a 2x-5x speedup compared to standard text-to-text models , while the compression also allows existing language models to handle longer documents without the need for designing new pre-trained models.
