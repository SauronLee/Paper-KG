MMGCN: Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation | Jingwen Hu | emotion recognition in conversation \( erc \) is a crucial component in affective dialogue systems , which helps the system understand usersâ€™ emotions and generate empathetic responses. however , most works focus on modeling speaker and contextual information primarily on the textual modality or simply leveraging multimodal information through feature concatenation. in order to explore a more effective way of utilizing both multimodal and long-distance contextual information , we propose a new model based on multimodal fused graph convolutional network , mmgcn , in this work. mmgcn can not only make use of multimodal dependencies effectively , but also leverage speaker information to model inter-speaker and intra-speaker dependency. we evaluate our proposed model on two public benchmark datasets , iemocap and meld , and the results prove the effectiveness of mmgcn , which outperforms other sota methods by a significant margin under the multimodal conversation setting.
