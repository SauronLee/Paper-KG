Continuous Language Generative Flow | Zineng Tang | recent years have witnessed various types of generative models for natural language generation \( nlg \) , especially rnns or transformer based sequence-to-sequence models , as well as variational autoencoder \( vae \) and generative adversarial network \( gan \) based models. however , flow-based generative models , which achieve strong performance in image generation due to their invertibility and exact density estimation properties , have been less explored for nlg. in this paper , we propose a flow-based language generation model by adapting previous flow generative models to language generation via continuous input embeddings , adapted affine coupling structures , and a novel architecture for autoregressive text generation. we also apply our framework to sequence-to-sequence generation , including text- and video-based question generation \( qg \) and neural machine translation \( nmt \) , and data augmentation for question answering \( qa \) . we use our language flow model to provide extra input features for qg and nmt , which achieves improvements over the strong qg baselines on squad and tvqa and nmt baseline on wmt16. we also augment qa data with new context by injecting noise to the latent features of the language flow and show this augmentation leads to a large performance improvement from strong baselines on squad and tvqa.
