Measuring and Increasing Context Usage in Context-Aware Machine Translation | Patrick Fernandes | recent work in neural machine translation has demonstrated both the necessity and feasibility of using inter-sentential context , context from sentences other than those currently being translated. however , while many current methods present model architectures that theoretically can use this extra context , it is often not clear how much they do actually utilize it at translation time. in this paper , we introduce a new metric , conditional cross-mutual information , to quantify usage of context by these models. using this metric , we measure how much document-level machine translation systems use particular varieties of context. we find that target context is referenced more than source context , and that including more context has a diminishing affect on results. we then introduce a new , simple training method , context-aware word dropout , to increase the usage of context by context-aware models. experiments show that our method not only increases context usage , but also improves the translation quality according to metrics such as bleu and comet , as well as performance on anaphoric pronoun resolution and lexical cohesion contrastive datasets.
