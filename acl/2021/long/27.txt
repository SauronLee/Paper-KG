Multimodal Sentiment Detection Based on Multi-channel Graph Neural Networks | Xiaocui Yang | with the popularity of smartphones , we have witnessed the rapid proliferation of multimodal posts on various social media platforms. we observe that the multimodal sentiment expression has specific global characteristics , such as the interdependencies of objects or scenes within the image. however , most previous studies only considered the representation of a single image-text post and failed to capture the global co-occurrence characteristics of the dataset. in this paper , we propose multi-channel graph neural networks with sentiment-awareness \( mgnns \) for image-text sentiment detection. specifically , we first encode different modalities to capture hidden representations. then , we introduce multi-channel graph neural networks to learn multimodal representations based on the global characteristics of the dataset. finally , we implement multimodal in-depth fusion with the multi-head attention mechanism to predict the sentiment of image-text pairs. extensive experiments conducted on three publicly available datasets demonstrate the effectiveness of our approach for multimodal sentiment detection.
