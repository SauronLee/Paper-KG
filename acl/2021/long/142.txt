Implicit Representations of Meaning in Neural Language Models | Belinda Z. Li | does the effectiveness of neural language models derive entirely from accurate modeling of surface word co-occurrence statistics , or do these models represent and reason about the world they describe \? in bart and t5 transformer language models , we identify contextual word representations that function as *models of entities and situations* as they evolve throughout a discourse. these neural representations have functional similarities to linguistic models of dynamic semantics: they support a linear readout of each entityâ€™s current properties and relations , and can be manipulated with predictable effects on language generation. our results indicate that prediction in pretrained neural language models is supported , at least in part , by dynamic representations of meaning and implicit simulation of entity state , and that this behavior can be learned with only text as training data.
