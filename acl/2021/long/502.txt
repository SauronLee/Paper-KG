BERTGen: Multi-task Generation through BERT | Faidon Mitzalis | we present bertgen , a novel , generative , decoder-only model which extends bert by fusing multimodal and multilingual pre-trained models vl-bert and m-bert , respectively. bertgen is auto-regressively trained for language generation tasks , namely image captioning , machine translation and multimodal machine translation , under a multi-task setting. with a comprehensive set of evaluations , we show that bertgen outperforms many strong baselines across the tasks explored. we also show bertgenâ€™s ability for zero-shot language generation , where it exhibits competitive performance to supervised counterparts. finally , we conduct ablation studies which demonstrate that bertgen substantially benefits from multi-tasking and effectively transfers relevant inductive biases from the pre-trained models.
