On Compositional Generalization of Neural Machine Translation | Yafu Li | modern neural machine translation \( nmt \) models have achieved competitive performance in standard benchmarks such as wmt. however , there still exist significant issues such as robustness , domain generalization , etc. in this paper , we study nmt models from the perspective of compositional generalization by building a benchmark dataset , cognition , consisting of 216k clean and consistent sentence pairs. we quantitatively analyze effects of various factors using compound translation error rate , then demonstrate that the nmt model fails badly on compositional generalization , although it performs remarkably well under traditional metrics.
