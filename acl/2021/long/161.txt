Weight Distillation: Transferring the Knowledge in Neural Network Parameters | Ye Lin | knowledge distillation has been proven to be effective in model acceleration and compression. it transfers knowledge from a large neural network to a small one by using the large neural network predictions as targets of the small neural network. but this way ignores the knowledge inside the large neural networks , e.g. , parameters. our preliminary study as well as the recent success in pre-training suggests that transferring parameters are more effective in distilling knowledge. in this paper , we propose weight distillation to transfer the knowledge in parameters of a large neural network to a small neural network through a parameter generator. on the wmt16 en-ro , nist12 zh-en , and wmt14 en-de machine translation tasks , our experiments show that weight distillation learns a small network that is 1.88 2.94x faster than the large network but with competitive bleu performance. when fixing the size of small networks , weight distillation outperforms knowledge distillation by 0.51 1.82 bleu points.
