Optimizing Deeper Transformers on Small Datasets | Peng Xu | it is a common belief that training deep transformers from scratch requires large datasets. consequently , for small datasets , people usually use shallow and simple additional layers on top of pre-trained models during fine-tuning. this work shows that this does not always need to be the case: with proper initialization and optimization , the benefits of very deep transformers can carry over to challenging tasks with small datasets , including text-to-sql semantic parsing and logical reading comprehension. in particular , we successfully train 48 layers of transformers , comprising 24 fine-tuned layers from pre-trained roberta and 24 relation-aware layers trained from scratch. with fewer training steps and no task-specific pre-training , we obtain the state of the art performance on the challenging cross-domain text-to-sql parsing benchmark spider. we achieve this by deriving a novel data dependent transformer fixed-update initialization scheme \( dt-fixup \) , inspired by the prior t-fixup work. further error analysis shows that increasing depth can help improve generalization on small datasets for hard cases that require reasoning and structural understanding.
