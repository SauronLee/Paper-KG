Better than Average: Paired Evaluation of NLP systems | Maxime Peyrard | evaluation in nlp is usually done by comparing the scores of competing systems independently averaged over a common set of test instances. in this work , we question the use of averages for aggregating evaluation scores into a final number used to decide which system is best , since the average , as well as alternatives such as the median , ignores the pairing arising from the fact that systems are evaluated on the same test instances. we illustrate the importance of taking the instancelevel pairing of evaluation scores into account and demonstrate , both theoretically and empirically , the advantages of aggregation methods based on pairwise comparisons , such as the bradleyâ€“terry \( bt \) model , a mechanism based on the estimated probability that a given system scores better than another on the test set. by re-evaluating 296 real nlp evaluation setups across four tasks and 18 evaluation metrics , we show that the choice of aggregation mechanism matters and yields different conclusions as to which systems are state of the art in about 30% of the setups. to facilitate the adoption of pairwise evaluation , we release a practical tool for performing the full analysis of evaluation scores with the mean , median , bt , and two variants of bt \( elo and trueskill \) , alongside functionality for appropriate statistical testing.
