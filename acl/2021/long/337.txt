HiddenCut: Simple Data Augmentation for Natural Language Understanding with Better Generalizability | Jiaao Chen | fine-tuning large pre-trained models with task-specific data has achieved great success in nlp. however , it has been demonstrated that the majority of information within the self-attention networks is redundant and not utilized effectively during the fine-tuning stage. this leads to inferior results when generalizing the obtained models to out-of-domain distributions. to this end , we propose a simple yet effective data augmentation technique , hiddencut , to better regularize the model and encourage it to learn more generalizable features. specifically , contiguous spans within the hidden space are dynamically and strategically dropped during training. experiments show that our hiddencut method outperforms the state-of-the-art augmentation methods on the glue benchmark , and consistently exhibits superior generalization performances on out-of-distribution and challenging counterexamples. we have publicly released our code at https://github.com/gt-salt/hiddencut.
