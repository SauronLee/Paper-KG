TGEA: An Error-Annotated Dataset and Benchmark Tasks for TextGeneration from Pretrained Language Models | Jie He | in order to deeply understand the capability of pretrained language models in text generation and conduct a diagnostic evaluation , we propose tgea , an error-annotated dataset with multiple benchmark tasks for text generation from pretrained language models \( plms \) . we use carefully selected prompt words to guide gpt-2 to generate candidate sentences , from which we select 47k for error annotation. crowdsourced workers manually check each of these sentences and detect 12k erroneous sentences. we create an error taxonomy to cover 24 types of errors occurring in these erroneous sentences according to the nature of errors with respect to linguistics and knowledge \( e.g. , common sense \) . for each erroneous span in plm-generated sentences , we also detect another span that is closely associated with it. each error is hence manually labeled with comprehensive annotations , including the span of the error , the associated span , minimal correction to the error , the type of the error , and rationale behind the error. apart from the fully annotated dataset , we also present a detailed description of the data collection procedure , statistics and analysis of the dataset. this is the first dataset with comprehensive annotations for plm-generated texts , which facilitates the diagnostic evaluation of plm-based text generation. furthermore , we use tgea as a benchmark dataset and propose a series of automatic diagnosis tasks , including error detection , error type classification , associated span detection , error rationale generation , to further promote future study on the automatic error detection and correction on texts generated by pretrained language models.
