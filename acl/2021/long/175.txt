KaggleDBQA: Realistic Evaluation of Text-to-SQL Parsers | Chia-Hsuan Lee | the goal of database question answering is to enable natural language querying of real-life relational databases in diverse application domains. recently , large-scale datasets such as spider and wikisql facilitated novel modeling techniques for text-to-sql parsing , improving zero-shot generalization to unseen databases. in this work , we examine the challenges that still prevent these techniques from practical deployment. first , we present kaggledbqa , a new cross-domain evaluation dataset of real web databases , with domain-specific data types , original formatting , and unrestricted questions. second , we re-examine the choice of evaluation tasks for text-to-sql parsers as applied in real-life settings. finally , we augment our in-domain evaluation task with database documentation , a naturally occurring source of implicit domain knowledge. we show that kaggledbqa presents a challenge to state-of-the-art zero-shot parsers but a more realistic evaluation setting and creative use of associated database documentation boosts their accuracy by over 13.2% , doubling their performance.
