Learning Dense Representations of Phrases at Scale | Jinhyuk Lee | open-domain question answering can be reformulated as a phrase retrieval problem , without the need for processing documents on-demand during inference \( seo et al. , 2019 \) . however , current phrase retrieval models heavily depend on sparse representations and still underperform retriever-reader approaches. in this work , we show for the first time that we can learn dense representations of phrases alone that achieve much stronger performance in open-domain qa. we present an effective method to learn phrase representations from the supervision of reading comprehension tasks , coupled with novel negative sampling methods. we also propose a query-side fine-tuning strategy , which can support transfer learning and reduce the discrepancy between training and inference. on five popular open-domain qa datasets , our model densephrases improves over previous phrase retrieval models by 15%-25% absolute accuracy and matches the performance of state-of-the-art retriever-reader models. our model is easy to parallelize due to pure dense representations and processes more than 10 questions per second on cpus. finally , we directly use our pre-indexed dense phrase representations for two slot filling tasks , showing the promise of utilizing densephrases as a dense knowledge base for downstream tasks.
