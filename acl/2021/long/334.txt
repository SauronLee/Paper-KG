Are Pretrained Convolutions Better than Pretrained Transformers? | Yi Tay | in the era of pre-trained language models , transformers are the de facto choice of model architectures. while recent research has shown promise in entirely convolutional , or cnn , architectures , they have not been explored using the pre-train-fine-tune paradigm. in the context of language models , are convolutional models competitive to transformers when pre-trained \? this paper investigates this research question and presents several interesting findings. across an extensive set of experiments on 8 datasets/tasks , we find that cnn-based pre-trained models are competitive and outperform their transformer counterpart in certain scenarios , albeit with caveats. overall , the findings outlined in this paper suggest that conflating pre-training and architectural advances is misguided and that both advances should be considered independently. we believe our research paves the way for a healthy amount of optimism in alternative architectures.
