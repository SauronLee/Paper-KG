StereoSet: Measuring stereotypical bias in pretrained language models | Moin Nadeem | a stereotype is an over-generalized belief about a particular group of people , e.g. , asians are good at math or african americans are athletic. such beliefs \( biases \) are known to hurt target groups. since pretrained language models are trained on large real-world data , they are known to capture stereotypical biases. it is important to quantify to what extent these biases are present in them. although this is a rapidly growing area of research , existing literature lacks in two important aspects: 1 \) they mainly evaluate bias of pretrained language models on a small set of artificial sentences , even though these models are trained on natural data 2 \) current evaluations focus on measuring bias without considering the language modeling ability of a model , which could lead to misleading trust on a model even if it is a poor language model. we address both these problems. we present stereoset , a large-scale natural english dataset to measure stereotypical biases in four domains: gender , profession , race , and religion. we contrast both stereotypical bias and language modeling ability of popular models like bert , gpt-2 , roberta , and xlnet. we show that these models exhibit strong stereotypical biases. our data and code are available at https://stereoset.mit.edu.
