StructuralLM: Structural Pre-training for Form Understanding | Chenliang Li | large pre-trained language models achieve state-of-the-art results when fine-tuned on downstream nlp tasks. however , they almost exclusively focus on text-only representation , while neglecting cell-level layout information that is important for form image understanding. in this paper , we propose a new pre-training approach , structurallm , to jointly leverage cell and layout information from scanned documents. specifically , we pre-train structurallm with two new designs to make the most of the interactions of cell and layout information: 1 \) each cell as a semantic unit; 2 \) classification of cell positions. the pre-trained structurallm achieves new state-of-the-art results in different types of downstream tasks , including form understanding \( from 78.95 to 85.14 \) , document visual question answering \( from 72.59 to 83.94 \) and document image classification \( from 94.43 to 96.08 \) .
