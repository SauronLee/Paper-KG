Consistency Regularization for Cross-Lingual Fine-Tuning | Bo Zheng | fine-tuning pre-trained cross-lingual language models can transfer task-specific supervision from one language to the others. in this work , we propose to improve cross-lingual fine-tuning with consistency regularization. specifically , we use example consistency regularization to penalize the prediction sensitivity to four types of data augmentations , i.e. , subword sampling , gaussian noise , code-switch substitution , and machine translation. in addition , we employ model consistency to regularize the models trained with two augmented versions of the same training set. experimental results on the xtreme benchmark show that our method significantly improves cross-lingual fine-tuning across various tasks , including text classification , question answering , and sequence labeling.
