When Do You Need Billions of Words of Pretraining Data? | Yian Zhang | nlp is currently dominated by language models like roberta which are pretrained on billions of words. but what exact knowledge or skills do transformer lms learn from large-scale pretraining that they cannot learn from less data \? to explore this question , we adopt five styles of evaluation: classifier probing , information-theoretic probing , unsupervised relative acceptability judgments , unsupervised language model knowledge probing , and fine-tuning on nlu tasks. we then draw learning curves that track the growth of these different measures of model ability with respect to pretraining data volume using the minibertas , a group of roberta models pretrained on 1m , 10m , 100m and 1b words. we find that these lms require only about 10m to 100m words to learn to reliably encode most syntactic and semantic features we test. they need a much larger quantity of data in order to acquire enough commonsense knowledge and other skills required to master typical downstream nlu tasks. the results suggest that , while the ability to encode linguistic features is almost certainly necessary for language understanding , it is likely that other , unidentified , forms of knowledge are the major drivers of recent improvements in language understanding among large pretrained models.
