Syntax-augmented Multilingual BERT for Cross-lingual Transfer | Wasi Ahmad | in recent years , we have seen a colossal effort in pre-training multilingual text encoders using large-scale corpora in many languages to facilitate cross-lingual transfer learning. however , due to typological differences across languages , the cross-lingual transfer is challenging. nevertheless , language syntax , e.g. , syntactic dependencies , can bridge the typological gap. previous works have shown that pre-trained multilingual encoders , such as mbert \( citation \) , capture language syntax , helping cross-lingual transfer. this work shows that explicitly providing language syntax and training mbert using an auxiliary objective to encode the universal dependency tree structure helps cross-lingual transfer. we perform rigorous experiments on four nlp tasks , including text classification , question answering , named entity recognition , and task-oriented semantic parsing. the experiment results show that syntax-augmented mbert improves cross-lingual transfer on popular benchmarks , such as paws-x and mlqa , by 1.4 and 1.6 points on average across all languages. in the
