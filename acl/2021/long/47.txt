COSY: COunterfactual SYntax for Cross-Lingual Understanding | Sicheng Yu | pre-trained multilingual language models , e.g. , multilingual-bert , are widely used in cross-lingual tasks , yielding the state-of-the-art performance. however , such models suffer from a large performance gap between source and target languages , especially in the zero-shot setting , where the models are fine-tuned only on english but tested on other languages for the same task. we tackle this issue by incorporating language-agnostic information , specifically , universal syntax such as dependency relations and pos tags , into language models , based on the observation that universal syntax is transferable across different languages. our approach , called counterfactual syntax \( cosy \) , includes the design of syntax-aware networks as well as a counterfactual training method to implicitly force the networks to learn not only the semantics but also the syntax. to evaluate cosy , we conduct cross-lingual experiments on natural language inference and question answering using mbert and xlm-r as network backbones. our results show that cosy achieves the state-of-the-art performance for both tasks , without using auxiliary training data.
