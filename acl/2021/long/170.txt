EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets | Xiaohan Chen | heavily overparameterized language models such as bert , xlnet and t5 have achieved impressive success in many nlp tasks. however , their high model complexity requires enormous computation resources and extremely long training time for both pre-training and fine-tuning. many works have studied model compression on large nlp models , but only focusing on reducing inference time while still requiring an expensive training process. other works use extremely large batch sizes to shorten the pre-training time , at the expense of higher computational resource demands. in this paper , inspired by the early-bird lottery tickets recently studied for computer vision tasks , we propose earlybert , a general computationally-efficient training algorithm applicable to both pre-training and fine-tuning of large-scale language models. by slimming the self-attention and fully-connected sub-layers inside a transformer , we are the first to identify structured winning tickets in the early stage of bert training. we apply those tickets towards efficient bert training , and conduct comprehensive pre-training and fine-tuning experiments on glue and squad downstream tasks. our results show that earlybert achieves comparable performance to standard bert , with 35 45% less training time. code is available at https://github.com/vita-group/earlybert.
