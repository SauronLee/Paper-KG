Rewriter-Evaluator Architecture for Neural Machine Translation | Yangming Li | a few approaches have been developed to improve neural machine translation \( nmt \) models with multiple passes of decoding. however , their performance gains are limited because of lacking proper policies to terminate the multi-pass process. to address this issue , we introduce a novel architecture of rewriter-evaluator. translating a source sentence involves multiple rewriting passes. in every pass , a rewriter generates a new translation to improve the past translation. termination of this multi-pass process is determined by a score of translation quality estimated by an evaluator. we also propose prioritized gradient descent \( pgd \) to jointly and efficiently train the rewriter and the evaluator. extensive experiments on three machine translation tasks show that our architecture notably improves the performances of nmt models and significantly outperforms prior methods. an oracle experiment reveals that it can largely reduce performance gaps to the oracle policy. experiments confirm that the evaluator trained with pgd is more accurate than prior methods in determining proper numbers of rewriting.
