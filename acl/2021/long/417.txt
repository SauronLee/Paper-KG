Enabling Lightweight Fine-tuning for Pre-trained Language Model Compression based on Matrix Product Operators | Peiyu Liu | this paper presents a novel pre-trained language models \( plm \) compression approach based on the matrix product operator \( short as mpo \) from quantum many-body physics. it can decompose an original matrix into central tensors \( containing the core information \) and auxiliary tensors \( with only a small proportion of parameters \) . with the decomposed mpo structure , we propose a novel fine-tuning strategy by only updating the parameters from the auxiliary tensors , and design an optimization algorithm for mpo-based approximation over stacked network architectures. our approach can be applied to the original or the compressed plms in a general way , which derives a lighter network and significantly reduces the parameters to be fine-tuned. extensive experiments have demonstrated the effectiveness of the proposed approach in model compression , especially the reduction in fine-tuning parameters \( 91% reduction on average \) . the code to reproduce the results of this paper can be found at https://github.com/rucaibox/mpop.
