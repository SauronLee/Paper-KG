GhostBERT: Generate More Features with Cheap Operations for BERT | Zhiqi Huang | transformer-based pre-trained language models like bert , though powerful in many tasks , are expensive in both memory and computation , due to their large number of parameters. previous works show that some parameters in these models can be pruned away without severe accuracy drop. however , these redundant features contribute to a comprehensive understanding of the training data and removing them weakens the modelâ€™s representation ability. in this paper , we propose ghostbert , which generates more features with very cheap operations from the remaining features. in this way , ghostbert has similar memory and computational cost as the pruned model , but enjoys much larger representation power. the proposed ghost module can also be applied to unpruned bert models to enhance their performance with negligible additional parameters and computation. empirical results on the glue benchmark on three backbone models \( i.e. , bert , roberta and electra \) verify the efficacy of our proposed method.
