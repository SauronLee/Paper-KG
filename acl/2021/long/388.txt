VisualSparta: An Embarrassingly Simple Approach to Large-scale Text-to-Image Search with Weighted Bag-of-words | Xiaopeng Lu | text-to-image retrieval is an essential task in cross-modal information retrieval , i.e. , retrieving relevant images from a large and unlabelled dataset given textual queries. in this paper , we propose visualsparta , a novel \( visual-text sparse transformer matching \) model that shows significant improvement in terms of both accuracy and efficiency. visualsparta is capable of outperforming previous state-of-the-art scalable methods in mscoco and flickr30k. we also show that it achieves substantial retrieving speed advantages , i.e. , for a 1 million image index , visualsparta using cpu gets ~391x speedup compared to cpu vector search and ~5.4x speedup compared to vector search with gpu acceleration. experiments show that this speed advantage even gets bigger for larger datasets because visualsparta can be efficiently implemented as an inverted index. to the best of our knowledge , visualsparta is the first transformer-based text-to-image retrieval model that can achieve real-time searching for large-scale datasets , with significant accuracy improvement compared to previous state-of-the-art methods.
