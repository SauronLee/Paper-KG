RADDLE: An Evaluation Benchmark and Analysis Platform for Robust Task-oriented Dialog Systems | Baolin Peng | for task-oriented dialog systems to be maximally useful , it must be able to process conversations in a way that is \( 1 \) generalizable with a small number of training examples for new task domains , and \( 2 \) robust to user input in various styles , modalities , or domains. in pursuit of these goals , we introduce the raddle benchmark , a collection of corpora and tools for evaluating the performance of models across a diverse set of domains. by including tasks with limited training data , raddle is designed to favor and encourage models with a strong generalization ability. raddle also includes a diagnostic checklist that facilitates detailed robustness analysis in aspects such as language variations , speech errors , unseen entities , and out-of-domain utterances. we evaluate recent state-of-the-art systems based on pre-training and fine-tuning , and find that grounded pre-training on heterogeneous dialog corpora performs better than training a separate model per domain. adversarial training is also proposed to improve model robustness against noisy inputs. overall , existing models are less than satisfactory in robustness evaluation , which suggests opportunities for future improvement.
