Diversifying Dialog Generation via Adaptive Label Smoothing | Yida Wang | neural dialogue generation models trained with the one-hot target distribution suffer from the over-confidence issue , which leads to poor generation diversity as widely reported in the literature. although existing approaches such as label smoothing can alleviate this issue , they fail to adapt to diverse dialog contexts. in this paper , we propose an adaptive label smoothing \( adalabel \) approach that can adaptively estimate a target label distribution at each time step for different contexts. the maximum probability in the predicted distribution is used to modify the soft target distribution produced by a novel light-weight bi-directional decoder module. the resulting target distribution is aware of both previous and future contexts and is adjusted to avoid over-training the dialogue model. our model can be trained in an endto-end manner. extensive experiments on two benchmark datasets show that our approach outperforms various competitive baselines in producing diverse responses.
