Common Sense Beyond English: Evaluating and Improving Multilingual Language Models for Commonsense Reasoning | Bill Yuchen Lin | commonsense reasoning research has so far been limited to english. we aim to evaluate and improve popular multilingual language models \( ml-lms \) to help advance commonsense reasoning \( csr \) beyond english. we collect the mickey corpus , consisting of 561k sentences in 11 different languages , which can be used for analyzing and improving ml-lms. we propose mickey probe , a language-general probing task for fairly evaluating the common sense of popular ml-lms across different languages. in addition , we also create two new datasets , x-csqa and x-codah , by translating their english versions to 14 other languages , so that we can evaluate popular ml-lms for cross-lingual commonsense reasoning. to improve the performance beyond english , we propose a simple yet effective method â€” multilingual contrastive pretraining \( mcp \) . it significantly enhances sentence representations , yielding a large performance gain on both benchmarks \( e.g. , +2.7% accuracy for x-csqa over xlm-r_l \) .
