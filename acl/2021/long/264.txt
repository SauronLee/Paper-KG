Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word Alignment | Zewen Chi | the cross-lingual language models are typically pretrained with masked language modeling on multilingual text or parallel sentences. in this paper , we introduce denoising word alignment as a new cross-lingual pre-training task. specifically , the model first self-label word alignments for parallel sentences. then we randomly mask tokens in a bitext pair. given a masked token , the model uses a pointer network to predict the aligned token in the other language. we alternately perform the above two steps in an expectation-maximization manner. experimental results show that our method improves cross-lingual transferability on various datasets , especially on the token-level tasks , such as question answering , and structured prediction. moreover , the model can serve as a pretrained word aligner , which achieves reasonably low error rate on the alignment benchmarks. the code and pretrained parameters are available at github.com/czwin32768/xlm-align.
