Multilingual Speech Translation from Efficient Finetuning of Pretrained Models | Xian Li | we present a simple yet effective approach to build multilingual speech-to-text \( st \) translation through efficient transfer learning from a pretrained speech encoder and text decoder. our key finding is that a minimalistic lna \( layernorm and attention \) finetuning can achieve zero-shot crosslingual and cross-modality transfer ability by only finetuning 10 50% of the pretrained parameters. this effectively leverages large pretrained models at low training cost such as wav2vec 2.0 for acoustic modeling , and mbart for multilingual text generation. this sets a new state-of-the-art for 36 translation directions \( and surpassing cascaded st for 26 of them \) on the large-scale multilingual st benchmark covost 2 \( +6.4 bleu on average for en-x directions and +6.7 bleu for x-en directions \) . our approach demonstrates strong zero-shot performance in a many-to-many multilingual model \( +5.6 bleu on average across 28 non-english directions \) , making it an appealing approach for attaining high-quality speech translation with improved parameter and data efficiency.
