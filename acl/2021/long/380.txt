WARP: Word-level Adversarial ReProgramming | Karen Hambardzumyan | transfer learning from pretrained language models recently became the dominant approach for solving many nlp tasks. a common approach to transfer learning for multiple tasks that maximize parameter sharing trains one or more task-specific layers on top of the language model. in this paper , we present an alternative approach based on adversarial reprogramming , which extends earlier work on automatic prompt generation. adversarial reprogramming attempts to learn task-specific word embeddings that , when concatenated to the input text , instruct the language model to solve the specified task. using up to 25k trainable parameters per task , this approach outperforms all existing methods with up to 25m trainable parameters on the public leaderboard of the glue benchmark. our method , initialized with task-specific human-readable prompts , also works in a few-shot setting , outperforming gpt-3 on two superglue tasks with just 32 training samples.
