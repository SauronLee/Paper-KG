Knowledgeable or Educated Guess? Revisiting Language Models as Knowledge Bases | Boxi Cao | previous literatures show that pre-trained masked language models \( mlms \) such as bert can achieve competitive factual knowledge extraction performance on some datasets , indicating that mlms can potentially be a reliable knowledge source. in this paper , we conduct a rigorous study to explore the underlying predicting mechanisms of mlms over different extraction paradigms. by investigating the behaviors of mlms , we find that previous decent performance mainly owes to the biased prompts which overfit dataset artifacts. furthermore , incorporating illustrative cases and external contexts improve knowledge prediction mainly due to entity type guidance and golden answer leakage. our findings shed light on the underlying predicting mechanisms of mlms , and strongly question the previous conclusion that current mlms can potentially serve as reliable factual knowledge bases.
