Breaking the Corpus Bottleneck for Context-Aware Neural Machine Translation with Cross-Task Pre-training | Linqing Chen | context-aware neural machine translation \( nmt \) remains challenging due to the lack of large-scale document-level parallel corpora. to break the corpus bottleneck , in this paper we aim to improve context-aware nmt by taking the advantage of the availability of both large-scale sentence-level parallel dataset and source-side monolingual documents. to this end , we propose two pre-training tasks. one learns to translate a sentence from source language to target language on the sentence-level parallel dataset while the other learns to translate a document from deliberately noised to original on the monolingual documents. importantly , the two pre-training tasks are jointly and simultaneously learned via the same model , thereafter fine-tuned on scale-limited parallel documents from both sentence-level and document-level perspectives. experimental results on four translation tasks show that our approach significantly improves translation performance. one nice property of our approach is that the fine-tuned model can be used to translate both sentences and documents.
