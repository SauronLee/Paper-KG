Challenges in Information-Seeking QA: Unanswerable Questions and Paragraph Retrieval | Akari Asai | recent pretrained language models “solved” many reading comprehension benchmarks , where questions are written with access to the evidence document. however , datasets containing information-seeking queries where evidence documents are provided after the queries are written independently remain challenging. we analyze why answering information-seeking queries is more challenging and where their prevalent unanswerabilities arise , on natural questions and tydi qa. our controlled experiments suggest two headrooms – paragraph selection and answerability prediction , i.e. whether the paired evidence document contains the answer to the query or not. when provided with a gold paragraph and knowing when to abstain from answering , existing models easily outperform a human annotator. however , predicting answerability itself remains challenging. we manually annotate 800 unanswerable examples across six languages on what makes them challenging to answer. with this new data , we conduct per-category answerability prediction , revealing issues in the current dataset collection as well as task formulation. together , our study points to avenues for future research in information-seeking question answering , both for dataset creation and model development. our code and annotated data is publicly available at https://github.com/akariasai/unanswerable_qa.
