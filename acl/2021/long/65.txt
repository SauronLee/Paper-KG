Adapting High-resource NMT Models to Translate Low-resource Related Languages without Parallel Data | Wei-Jen Ko | the scarcity of parallel data is a major obstacle for training high-quality machine translation systems for low-resource languages. fortunately , some low-resource languages are linguistically related or similar to high-resource languages; these related languages may share many lexical or syntactic structures. in this work , we exploit this linguistic overlap to facilitate translating to and from a low-resource language with only monolingual data , in addition to any parallel data in the related high-resource language. our method , nmt-adapt , combines denoising autoencoding , back-translation and adversarial objectives to utilize monolingual data for low-resource adaptation. we experiment on 7 languages from three different language families and show that our technique significantly improves translation into low-resource language compared to other translation baselines.
