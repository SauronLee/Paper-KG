E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning | Haiyang Xu | vision-language pre-training \( vlp \) on large-scale image-text pairs has achieved huge success for the cross-modal downstream tasks. the most existing pre-training methods mainly adopt a two-step training procedure , which firstly employs a pre-trained object detector to extract region-based visual features , then concatenates the image representation and text embedding as the input of transformer to train. however , these methods face problems of using task-specific visual representation of the specific object detector for generic cross-modal understanding , and the computation inefficiency of two-stage pipeline. in this paper , we propose the first end-to-end vision-language pre-trained model for both v+l understanding and generation , namely e2e-vlp , where we build a unified transformer framework to jointly learn visual representation , and semantic alignments between image and text. we incorporate the tasks of object detection and image captioning into pre-training with a unified transformer encoder-decoder architecture for enhancing visual learning. an extensive set of experiments have been conducted on well-established vision-language downstream tasks to demonstrate the effectiveness of this novel vlp paradigm.
