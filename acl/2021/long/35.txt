Introducing Orthogonal Constraint in Structural Probes | Tomasz Limisiewicz | with the recent success of pre-trained models in nlp , a significant focus was put on interpreting their representations. one of the most prominent approaches is structural probing \( hewitt and manning , 2019 \) , where a linear projection of word embeddings is performed in order to approximate the topology of dependency structures. in this work , we introduce a new type of structural probing , where the linear projection is decomposed into 1. iso-morphic space rotation; 2. linear scaling that identifies and scales the most relevant dimensions. in addition to syntactic dependency , we evaluate our method on two novel tasks \( lexical hypernymy and position in a sentence \) . we jointly train the probes for multiple tasks and experimentally show that lexical and syntactic information is separated in the representations. moreover , the orthogonal constraint makes the structural probes less vulnerable to memorization.
