UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning | Wei Li | existed pre-training methods either focus on single-modal tasks or multi-modal tasks , and cannot effectively adapt to each other. they can only utilize single-modal data \( i.e. , text or image \) or limited multi-modal data \( i.e. , image-text pairs \) . in this work , we propose a unified-modal pre-training architecture , namely unimo , which can effectively adapt to both single-modal and multi-modal understanding and generation tasks. large scale of free text corpus and image collections are utilized to improve the capability of visual and textual understanding , and cross-modal contrastive learning \( cmcl \) is leveraged to align the textual and visual information into a unified semantic space , over a corpus of image-text pairs augmented with related images and texts. with the help of rich non-paired single-modal data , our model is able to learn more generalizable representations , by allowing textual knowledge and visual knowledge to enhance each other in the unified semantic space. the experimental results show that unimo greatly improves the performance of several single-modal and multi-modal downstream tasks. our code and pre-trained models are public at
