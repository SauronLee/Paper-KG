Long-Span Summarization via Local Attention and Content Selection | Potsawee Manakul | transformer-based models have achieved state-of-the-art results in a wide range of natural language processing \( nlp \) tasks including document summarization. typically these systems are trained by fine-tuning a large pre-trained model to the target task. one issue with these transformer-based models is that they do not scale well in terms of memory and compute requirements as the input length grows. thus , for long document summarization , it can be challenging to train or fine-tune these models. in this work , we exploit large pre-trained transformer-based models and address long-span dependencies in abstractive summarization using two methods: local self-attention; and explicit content selection. these approaches are compared on a range of network configurations. experiments are carried out on standard long-span summarization tasks , including spotify podcast , arxiv , and pubmed datasets. we demonstrate that by combining these methods , we can achieve state-of-the-art results on all three tasks in the rouge scores. moreover , without a large-scale gpu card , our approach can achieve comparable or better results than existing approaches.
