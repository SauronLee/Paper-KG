A Large-Scale Chinese Multimodal NER Dataset with Speech Clues | Dianbo Sui | in this paper , we aim to explore an uncharted territory , which is chinese multimodal named entity recognition \( ner \) with both textual and acoustic contents. to achieve this , we construct a large-scale human-annotated chinese multimodal ner dataset , named cnerta. our corpus totally contains 42 , 987 annotated sentences accompanying by 71 hours of speech data. based on this dataset , we propose a family of strong and representative baseline models , which can leverage textual features or multimodal features. upon these baselines , to capture the natural monotonic alignment between the textual modality and the acoustic modality , we further propose a simple multimodal multitask model by introducing a speech-to-text alignment auxiliary task. through extensive experiments , we observe that: \( 1 \) progressive performance boosts as we move from unimodal to multimodal , verifying the necessity of integrating speech clues into chinese ner. \( 2 \) our proposed model yields state-of-the-art \( sota \) results on cnerta , demonstrating its effectiveness. for further research , the annotated dataset is publicly available at
