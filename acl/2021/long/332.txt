Convolutions and Self-Attention: Re-interpreting Relative Positions in Pre-trained Language Models | Tyler Chang | in this paper , we detail the relationship between convolutions and self-attention in natural language tasks. we show that relative position embeddings in self-attention layers are equivalent to recently-proposed dynamic lightweight convolutions , and we consider multiple new ways of integrating convolutions into transformer self-attention. specifically , we propose composite attention , which unites previous relative position encoding methods under a convolutional framework. we conduct experiments by training bert with composite attention , finding that convolutions consistently improve performance on multiple downstream tasks , replacing absolute position embeddings. to inform future work , we present results comparing lightweight convolutions , dynamic convolutions , and depthwise-separable convolutions in language model pre-training , considering multiple injection points for convolutions in self-attention layers.
