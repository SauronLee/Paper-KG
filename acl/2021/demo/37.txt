LEGOEval: An Open-Source Toolkit for Dialogue System Evaluation via Crowdsourcing | Yu Li | we present legoeval , an open-source toolkit that enables researchers to easily evaluate dialogue systems in a few lines of code using the online crowdsource platform , amazon mechanical turk. compared to existing toolkits , legoeval features a flexible task design by providing a python api that maps to commonly used react.js interface components. researchers can personalize their evaluation procedures easily with our built-in pages as if playing with lego blocks. thus , legoeval provides a fast , consistent method for reproducing human evaluation results. besides the flexible task design , legoeval also offers an easy api to review collected data.
