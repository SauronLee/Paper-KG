Ecco: An Open Source Library for the Explainability of Transformer Language Models | J Alammar | our understanding of why transformer-based nlp models have been achieving their recent success lags behind our ability to continue scaling these models. to increase the transparency of transformer-based language models , we present ecco – an open-source library for the explainability of transformer-based nlp models. ecco provides a set of tools to capture , analyze , visualize , and interactively explore the inner mechanics of these models. this includes \( 1 \) gradient-based feature attribution for natural language generation \( 2 \) hidden states and their evolution between model layers \( 3 \) convenient access and examination tools for neuron activations in the under-explored feed-forward neural network sublayer of transformer layers. \( 4 \) convenient examination of activation vectors via canonical correlation analysis \( cca \) , non-negative matrix factorization \( nmf \) , and probing classifiers. we find that syntactic information can be retrieved from bert’s ffnn representations in levels comparable to those in hidden state representations. more curiously , we find that the model builds up syntactic information in its hidden states even when intermediate ffnns indicate diminished levels of syntactic information. ecco is available at https://www.eccox.io/
