FastSeq: Make Sequence Generation Faster | Yu Yan | transformer-based models have made tremendous impacts in natural language generation. however the inference speed is a bottleneck due to large model size and intensive computing involved in auto-regressive decoding process. we develop fastseq framework to accelerate sequence generation without accuracy loss. the proposed optimization techniques include an attention cache optimization , an efficient algorithm for detecting repeated n-grams , and an asynchronous generation pipeline with parallel i/o. these optimizations are general enough to be applicable to transformer-based models \( e.g. , t5 , gpt2 , and unilm \) . our benchmark results on a set of widely used and diverse models demonstrate 4-9x inference speed gain. additionally , fastseq is easy to use with a simple one-line code change. the source code is available at https://github.com/microsoft/fastseq.
