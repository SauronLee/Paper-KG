What transfers in morphological inflection? Experiments with analogical models | Micha Elsner | we investigate how abstract processes like suffixation can be learned from morphological inflection task data using an analogical memory-based framework. in this framework , the inflection target form is specified by providing an example inflection of another word in the language. we show that this model is capable of near-baseline performance on the sigmorphon 2020 inflection challenge. such a model can make predictions for unseen languages , allowing us to perform one-shot inflection on natural languages and investigate morphological transfer with synthetic probes. accuracy for one-shot transfer can be unexpectedly high for some target languages \( 88% in shona \) and language families \( 53% across romance \) . probe experiments show that the model learns partially generalizable representations of prefixation , suffixation and reduplication , aiding its ability to transfer. we argue that the degree of generality of these process representations also helps to explain transfer results from previous research.
