NICT-5’s Submission To WAT 2021: MBART Pre-training And In-Domain Fine Tuning For Indic Languages | Raj Dabre | in this paper we describe our submission to the multilingual indic language translation wtask “multiindicmt” under the team name “nict-5”. this task involves translation from 10 indic languages into english and vice-versa. the objective of the task was to explore the utility of multilingual approaches using a variety of in-domain and out-of-domain parallel and monolingual corpora. given the recent success of multilingual nmt pre-training we decided to explore pre-training an mbart model on a large monolingual corpus collection covering all languages in this task followed by multilingual fine-tuning on small in-domain corpora. firstly , we observed that a small amount of pre-training followed by fine-tuning on small bilingual corpora can yield large gains over when pre-training is not used. furthermore , multilingual fine-tuning leads to further gains in translation quality which significantly outperforms a very strong multilingual baseline that does not rely on any pre-training.
