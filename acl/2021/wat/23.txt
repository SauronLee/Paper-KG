How far can we get with one GPU in 100 hours? CoAStaL at MultiIndicMT Shared Task | Rahul Aralikatte | this work shows that competitive translation results can be obtained in a constrained setting by incorporating the latest advances in memory and compute optimization. we train and evaluate large multilingual translation models using a single gpu for a maximum of 100 hours and get within 4-5 bleu points of the top submission on the leaderboard. we also benchmark standard baselines on the pmi corpus and re-discover well-known shortcomings of translation systems and metrics.
