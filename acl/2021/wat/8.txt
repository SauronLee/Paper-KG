Rakuten’s Participation in WAT 2021: Examining the Effectiveness of Pre-trained Models for Multilingual and Multimodal Machine Translation | Raymond Hendy Susanto | this paper introduces our neural machine translation systems’ participation in the wat 2021 shared translation tasks \( team id: sakura \) . we participated in the \( i \) nict-sap , \( ii \) japanese-english multimodal translation , \( iii \) multilingual indic , and \( iv \) myanmar-english translation tasks. multilingual approaches such as mbart \( liu et al. , 2020 \) are capable of pre-training a complete , multilingual sequence-to-sequence model through denoising objectives , making it a great starting point for building multilingual translation systems. our main focus in this work is to investigate the effectiveness of multilingual finetuning on such a multilingual language model on various translation tasks , including low-resource , multimodal , and mixed-domain translation. we further explore a multimodal approach based on universal visual representation \( zhang et al. , 2019 \) and compare its performance against a unimodal approach based on mbart alone.
