ViTA: Visual-Linguistic Translation by Aligning Object Tags | Kshitij Gupta | multimodal machine translation \( mmt \) enriches the source text with visual information for translation. it has gained popularity in recent years , and several pipelines have been proposed in the same direction. yet , the task lacks quality datasets to illustrate the contribution of visual modality in the translation systems. in this paper , we propose our system under the team name volta for the multimodal translation task of wat 2021 from english to hindi. we also participate in the textual-only subtask of the same language pair for which we use mbart , a pretrained multilingual sequence-to-sequence model. for multimodal translation , we propose to enhance the textual input by bringing the visual information to a textual domain by extracting object tags from the image. we also explore the robustness of our system by systematically degrading the source text. finally , we achieve a bleu score of 44.6 and 51.6 on the test set and challenge set of the multimodal task.
