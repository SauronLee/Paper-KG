BTS: Back TranScription for Speech-to-Text Post-Processor using Text-to-Speech-to-Text | Chanjun Park | with the growing popularity of smart speakers , such as amazon alexa , speech is becoming one of the most important modes of human-computer interaction. automatic speech recognition \( asr \) is arguably the most critical component of such systems , as errors in speech recognition propagate to the downstream components and drastically degrade the user experience. a simple and effective way to improve the speech recognition accuracy is to apply automatic post-processor to the recognition result. however , training a post-processor requires parallel corpora created by human annotators , which are expensive and not scalable. to alleviate this problem , we propose back transcription \( bts \) , a denoising-based method that can create such corpora without human labor. using a raw corpus , bts corrupts the text using text-to-speech \( tts \) and speech-to-text \( stt \) systems. then , a post-processing model can be trained to reconstruct the original text given the corrupted input. quantitative and qualitative evaluations show that a post-processor trained using our approach is highly effective in fixing non-trivial speech recognition errors such as mishandling foreign words. we present the generated parallel corpus and post-processing platform to make our results publicly available.
