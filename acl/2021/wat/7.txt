NICT-2 Translation System at WAT-2021: Applying a Pretrained Multilingual Encoder-Decoder Model to Low-resource Language Pairs | Kenji Imamura | in this paper , we present the nict system \( nict-2 \) submitted to the nict-sap shared task at the 8th workshop on asian translation \( wat-2021 \) . a feature of our system is that we used a pretrained multilingual bart \( bidirectional and auto-regressive transformer; mbart \) model. because publicly available models do not support some languages in the nict-sap task , we added these languages to the mbart model and then trained it using monolingual corpora extracted from wikipedia. we fine-tuned the expanded mbart model using the parallel corpora specified by the nict-sap task. the bleu scores greatly improved in comparison with those of systems without the pretrained model , including the additional languages.
