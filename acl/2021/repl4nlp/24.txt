Simultaneously Self-Attending to Text and Entities for Knowledge-Informed Text Representations | Dung Thai | pre-trained language models have emerged as highly successful methods for learning good text representations. however , the amount of structured knowledge retained in such models , and how \( if at all \) it can be extracted , remains an open question. in this work , we aim at directly learning text representations which leverage structured knowledge about entities mentioned in the text. this can be particularly beneficial for downstream tasks which are knowledge-intensive. our approach utilizes self-attention between words in the text and knowledge graph \( kg \) entities mentioned in the text. while existing methods require entity-linked data for pre-training , we train using a mention-span masking objective and a candidate ranking objective – which doesn’t require any entity-links and only assumes access to an alias table for retrieving candidates , enabling large-scale pre-training. we show that the proposed model learns knowledge-informed text representations that yield improvements on the downstream tasks over existing methods.
