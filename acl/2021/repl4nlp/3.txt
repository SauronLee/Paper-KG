Larger-Scale Transformers for Multilingual Masked Language Modeling | Naman Goyal | recent work has demonstrated the effectiveness of cross-lingual language model pretraining for cross-lingual understanding. in this study , we present the results of two larger multilingual masked language models , with 3.5b and 10.7b parameters. our two new models dubbed and outperform xlm-r by 1.8% and 2.4% average accuracy on xnli. our model also outperforms the roberta-large model on several english tasks of the glue benchmark by 0.3% on average while handling 99 more languages. this suggests larger capacity models for language understanding may obtain strong performance on high-resource languages while greatly improving low-resource languages. we make our code and models publicly available.
