Direction is what you need: Improving Word Embedding Compression in Large Language Models | Klaudia Ba≈Çazy | the adoption of transformer-based models in natural language processing \( nlp \) has led to great success using a massive number of parameters. however , due to deployment constraints in edge devices , there has been a rising interest in the compression of these models to improve their inference time and memory footprint. this paper presents a novel loss objective to compress token embeddings in the transformer-based models by leveraging an autoencoder architecture. more specifically , we emphasize the importance of the direction of compressed embeddings with respect to original uncompressed embeddings. the proposed method is task-agnostic and does not require further language modeling pre-training. our method significantly outperforms the commonly used svd-based matrix-factorization approach in terms of initial language model perplexity. moreover , we evaluate our proposed approach over squad v1.1 dataset and several downstream tasks from the glue benchmark , where we also outperform the baseline in most scenarios. our code is public.
