Probing Cross-Modal Representations in Multi-Step Relational Reasoning | Iuliia Parfenova | we investigate the representations learned by vision and language models in tasks that require relational reasoning. focusing on the problem of assessing the relative size of objects in abstract visual contexts , we analyse both one-step and two-step reasoning. for the latter , we construct a new dataset of three-image scenes and define a task that requires reasoning at the level of the individual images and across images in a scene. we probe the learned model representations using diagnostic classifiers. our experiments show that pretrained multimodal transformer-based architectures can perform higher-level relational reasoning , and are able to learn representations for novel tasks and data that are very different from what was seen in pretraining.
