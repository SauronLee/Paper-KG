Zero-shot Sequence Labeling for Transformer-based Sentence Classifiers | Kamil Bujel | we investigate how sentence-level transformers can be modified into effective sequence labelers at the token level without any direct supervision. existing approaches to zero-shot sequence labeling do not perform well when applied on transformer-based architectures. as transformers contain multiple layers of multi-head self-attention , information in the sentence gets distributed between many tokens , negatively affecting zero-shot token-level performance. we find that a soft attention module which explicitly encourages sharpness of attention weights can significantly outperform existing methods.
