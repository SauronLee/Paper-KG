Structure-aware Sentence Encoder in Bert-Based Siamese Network | Qiwei Peng | recently , impressive performance on various natural language understanding tasks has been achieved by explicitly incorporating syntax and semantic information into pre-trained models , such as bert and roberta. however , this approach depends on problem-specific fine-tuning , and as widely noted , bert-like models exhibit weak performance , and are inefficient , when applied to unsupervised similarity comparison tasks. sentence-bert \( sbert \) has been proposed as a general-purpose sentence embedding method , suited to both similarity comparison and downstream tasks. in this work , we show that by incorporating structural information into sbert , the resulting model outperforms sbert and previous general sentence encoders on unsupervised semantic textual similarity \( sts \) datasets and transfer classification tasks.
