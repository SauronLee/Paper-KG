Recovering Lexically and Semantically Reused Texts | Ansel MacLaughlin | writers often repurpose material from existing texts when composing new documents. because most documents have more than one source , we cannot trace these connections using only models of document-level similarity. instead , this paper considers methods for local text reuse detection \( ltrd \) , detecting localized regions of lexically or semantically similar text embedded in otherwise unrelated material. in extensive experiments , we study the relative performance of four classes of neural and bag-of-words models on three ltrd tasks – detecting plagiarism , modeling journalists’ use of press releases , and identifying scientists’ citation of earlier papers. we conduct evaluations on three existing datasets and a new , publicly-available citation localization dataset. our findings shed light on a number of previously-unexplored questions in the study of ltrd , including the importance of incorporating document-level context for predictions , the applicability of of-the-shelf neural models pretrained on “general” semantic textual similarity tasks such as paraphrase detection , and the trade-offs between more efficient bag-of-words and feature-based neural models and slower pairwise neural models.
