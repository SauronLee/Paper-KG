Inducing Language-Agnostic Multilingual Representations | Wei Zhao | cross-lingual representations have the potential to make nlp techniques available to the vast majority of languages in the world. however , they currently require large pretraining corpora or access to typologically similar languages. in this work , we address these obstacles by removing language identity signals from multilingual embeddings. we examine three approaches for this: \( i \) re-aligning the vector spaces of target languages \( all together \) to a pivot source language; \( ii \) removing language-specific means and variances , which yields better discriminativeness of embeddings as a by-product; and \( iii \) increasing input similarity across languages by removing morphological contractions and sentence reordering. we evaluate on xnli and reference-free mt evaluation across 19 typologically diverse languages. our findings expose the limitations of these approaches—unlike vector normalization , vector space re-alignment and text normalization do not achieve consistent gains across encoders and languages. due to the approaches’ additive effects , their combination decreases the cross-lingual transfer gap by 8.9 points \( m-bert \) and 18.2 points \( xlm-r \) on average across all tasks and languages , however.
