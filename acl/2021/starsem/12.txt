Realistic Evaluation Principles for Cross-document Coreference Resolution | Arie Cattan | we point out that common evaluation practices for cross-document coreference resolution have been unrealistically permissive in their assumed settings , yielding inflated results. we propose addressing this issue via two evaluation methodology principles. first , as in other tasks , models should be evaluated on predicted mentions rather than on gold mentions. doing this raises a subtle issue regarding singleton coreference clusters , which we address by decoupling the evaluation of mention detection from that of coreference linking. second , we argue that models should not exploit the synthetic topic structure of the standard ecb+ dataset , forcing models to confront the lexical ambiguity challenge , as intended by the dataset creators. we demonstrate empirically the drastic impact of our more realistic evaluation principles on a competitive model , yielding a score which is 33 f1 lower compared to evaluating by prior lenient practices.
