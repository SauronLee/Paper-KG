Measuring and Improving Model-Moderator Collaboration using Uncertainty Estimation | Ian Kivlichan | content moderation is often performed by a collaboration between humans and machine learning models. however , it is not well understood how to design the collaborative process so as to maximize the combined moderator-model system performance. this work presents a rigorous study of this problem , focusing on an approach that incorporates model uncertainty into the collaborative process. first , we introduce principled metrics to describe the performance of the collaborative system under capacity constraints on the human moderator , quantifying how efficiently the combined system utilizes human decisions. using these metrics , we conduct a large benchmark study evaluating the performance of state-of-the-art uncertainty models under different collaborative review strategies. we find that an uncertainty-based strategy consistently outperforms the widely used strategy based on toxicity scores , and moreover that the choice of review strategy drastically changes the overall system performance. our results demonstrate the importance of rigorous metrics for understanding and developing effective moderator-model systems for content moderation , as well as the utility of uncertainty estimation in this domain.
