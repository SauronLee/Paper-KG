Improving Counterfactual Generation for Fair Hate Speech Detection | Aida Mostafazadeh Davani | bias mitigation approaches reduce modelsâ€™ dependence on sensitive features of data , such as social group tokens \( sgts \) , resulting in equal predictions across the sensitive features. in hate speech detection , however , equalizing model predictions may ignore important differences among targeted social groups , as hate speech can contain stereotypical language specific to each sgt. here , to take the specific language about each sgt into account , we rely on counterfactual fairness and equalize predictions among counterfactuals , generated by changing the sgts. our method evaluates the similarity in sentence likelihoods \( via pre-trained language models \) among counterfactuals , to treat sgts equally only within interchangeable contexts. by applying logit pairing to equalize outcomes on the restricted set of counterfactuals for each instance , we improve fairness metrics while preserving model performance on hate speech detection.
