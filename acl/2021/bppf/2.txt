We Need to Consider Disagreement in Evaluation | Valerio Basile | evaluation is of paramount importance in data-driven research fields such as natural language processing \( nlp \) and computer vision \( cv \) . current evaluation practice largely hinges on the existence of a single “ground truth” against which we can meaningfully compare the prediction of a model. however , this comparison is flawed for two reasons. 1 \) in many cases , more than one answer is correct. 2 \) even where there is a single answer , disagreement among annotators is ubiquitous , making it difficult to decide on a gold standard. we argue that the current methods of adjudication , agreement , and evaluation need serious reconsideration. some researchers now propose to minimize disagreement and to fix datasets. we argue that this is a gross oversimplification , and likely to conceal the underlying complexity. instead , we suggest that we need to better capture the sources of disagreement to improve today’s evaluation practice. we discuss three sources of disagreement: from the annotator , the data , and the context , and show how this affects even seemingly objective tasks. datasets with multiple annotations are becoming more common , as are methods to integrate disagreement into modeling. the logical next step is to extend this to evaluation.
