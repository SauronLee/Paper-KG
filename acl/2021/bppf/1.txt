Guideline Bias in Wizard-of-Oz Dialogues | Victor Petr√©n Bach Hansen | nlp models struggle with generalization due to sampling and annotator bias. this paper focuses on a different kind of bias that has received very little attention: guideline bias , i.e. , the bias introduced by how our annotator guidelines are formulated. we examine two recently introduced dialogue datasets , ccpe-m and taskmaster-1 , both collected by trained assistants in a wizard-of-oz set-up. for ccpe-m , we show how a simple lexical bias for the word like in the guidelines biases the data collection. this bias , in effect , leads to poor performance on data without this bias: a preference elicitation architecture based on bert suffers a 5.3% absolute drop in performance , when like is replaced with a synonymous phrase , and a 13.2% drop in performance when evaluated on out-of-sample data. for taskmaster-1 , we show how the order in which instructions are resented , biases the data collection.
