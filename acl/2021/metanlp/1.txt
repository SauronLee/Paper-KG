Soft Layer Selection with Meta-Learning for Zero-Shot Cross-Lingual Transfer | Weijia Xu | multilingual pre-trained contextual embedding models \( devlin et al. , 2019 \) have achieved impressive performance on zero-shot cross-lingual transfer tasks. finding the most effective fine-tuning strategy to fine-tune these models on high-resource languages so that it transfers well to the zero-shot languages is a non-trivial task. in this paper , we propose a novel meta-optimizer to soft-select which layers of the pre-trained model to freeze during fine-tuning. we train the meta-optimizer by simulating the zero-shot transfer scenario. results on cross-lingual natural language inference show that our approach improves over the simple fine-tuning baseline and x-maml \( nooralahzadeh et al. , 2020 \) .
