Semi-supervised Meta-learning for Cross-domain Few-shot Intent Classification | Yue Li | meta learning aims to optimize the modelâ€™s capability to generalize to new tasks and domains. lacking a data-efficient way to create meta training tasks has prevented the application of meta-learning to the real-world few shot learning scenarios. recent studies have proposed unsupervised approaches to create meta-training tasks from unlabeled data for free , e.g. , the smlmt method \( bansal et al. , 2020a \) constructs unsupervised multi-class classification tasks from the unlabeled text by randomly masking words in the sentence and let the meta learner choose which word to fill in the blank. this study proposes a semi-supervised meta-learning approach that incorporates both the representation power of large pre-trained language models and the generalization capability of prototypical networks enhanced by smlmt. the semi-supervised meta training approach avoids overfitting prototypical networks on a small number of labeled training examples and quickly learns cross-domain task-specific representation only from a few supporting examples. by incorporating smlmt with prototypical networks , the meta learner generalizes better to unseen domains and gains higher accuracy on out-of-scope examples without the heavy lifting of pre-training. we observe significant improvement in few-shot generalization after training only a few epochs on the intent classification tasks evaluated in a multi-domain setting.
