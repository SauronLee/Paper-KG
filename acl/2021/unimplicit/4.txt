Improvements and Extensions on Metaphor Detection | Weicheng Ma | metaphors are ubiquitous in human language. the metaphor detection task \( md \) aims at detecting and interpreting metaphors from written language , which is crucial in natural language understanding \( nlu \) research. in this paper , we introduce a pre-trained transformer-based model into md. our model outperforms the previous state-of-the-art models by large margins in our evaluations , with relative improvements on the f-1 score from 5.33% to 28.39%. second , we extend md to a classification task about the metaphoricity of an entire piece of text to make md applicable in more general nlu scenes. finally , we clean up the improper or outdated annotations in one of the md benchmark datasets and re-benchmark it with our transformer-based model. this approach could be applied to other existing md datasets as well , since the metaphoricity annotations in these benchmark datasets may be outdated. future research efforts are also necessary to build an up-to-date and well-annotated dataset consisting of longer and more complex texts.
