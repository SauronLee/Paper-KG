Uppsala NLP at SemEval-2021 Task 2: Multilingual Language Models for Fine-tuning and Feature Extraction in Word-in-Context Disambiguation | Huiling You | we describe the uppsala nlp submission to semeval-2021 task 2 on multilingual and cross-lingual word-in-context disambiguation. we explore the usefulness of three pre-trained multilingual language models , xlm-roberta \( xlmr \) , multilingual bert \( mbert \) and multilingual distilled bert \( mdistilbert \) . we compare these three models in two setups , fine-tuning and as feature extractors. in the second case we also experiment with using dependency-based information. we find that fine-tuning is better than feature extraction. xlmr performs better than mbert in the cross-lingual setting both with fine-tuning and feature extraction , whereas these two models give a similar performance in the multilingual setting. mdistilbert performs poorly with fine-tuning but gives similar results to the other models when used as a feature extractor. we submitted our two best systems , fine-tuned with xlmr and mbert.
