Zhestyatsky at SemEval-2021 Task 2: ReLU over Cosine Similarity for BERT Fine-tuning | Boris Zhestiankin | this paper presents our contribution to semeval-2021 task 2: multilingual and cross-lingual word-in-context disambiguation \( mcl-wic \) . our experiments cover english \( en-en \) sub-track from the multilingual setting of the task. we experiment with several pre-trained language models and investigate an impact of different top-layers on fine-tuning. we find the combination of cosine similarity and relu activation leading to the most effective fine-tuning procedure. our best model results in accuracy 92.7% , which is the fourth-best score in en-en sub-track.
