GlossReader at SemEval-2021 Task 2: Reading Definitions Improves Contextualized Word Embeddings | Maxim Rachinskiy | consulting a dictionary or a glossary is a familiar way for many humans to figure out what does a word in a particular context mean. we hypothesize that a system that can select a proper definition for a particular word occurrence can also naturally solve tasks related to word senses. to verify this hypothesis we developed a solution for the multilingual and cross-lingual word-in-context \( mcl-wic \) task , that does not use any of the shared task data or other wic data for training. instead , it is trained to embed word definitions from english wordnet and word occurrences in english texts into the same vector space following an approach previously proposed for word sense disambiguation \( wsd \) . to estimate the similarity in meaning of two word occurrences , we compared different metrics in this shared vector space and found that l1-distance between normalized contextualized word embeddings outperforms traditionally employed cosine similarity and several other metrics. to solve the task for languages other than english , we rely on zero-shot cross-lingual transfer capabilities of the multilingual xlm-r masked language model. despite not using mcl-wic training data , in the shared task our approach achieves an accuracy of 89.5% on the english test set , which is only 4% less than the best system. in the multilingual subtask zero-shot cross-lingual transfer shows competitive results , that are within 2% from the best systems for russian , french , and arabic. in the cross-lingual subtask are within 2-4% from the best systems.
