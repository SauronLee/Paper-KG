JUST-BLUE at SemEval-2021 Task 1: Predicting Lexical Complexity using BERT and RoBERTa Pre-trained Language Models | Tuqa Bani Yaseen | predicting the complexity level of a word or a phrase is considered a challenging task. it is even recognized as a crucial step in numerous nlp applications , such as text rearrangements and text simplification. early research treated the task as a binary classification task , where the systems anticipated the existence of a wordâ€™s complexity \( complex versus uncomplicated \) . other studies had been designed to assess the level of word complexity using regression models or multi-labeling classification models. deep learning models show a significant improvement over machine learning models with the rise of transfer learning and pre-trained language models. this paper presents our approach that won the first rank in the semeval-task1 \( sub stask1 \) . we have calculated the degree of word complexity from 0-1 within a text. we have been ranked first place in the competition using the pre-trained language models bert and roberta , with a pearson correlation score of 0.788.
