UoR at SemEval-2021 Task 4: Using Pre-trained BERT Token Embeddings for Question Answering of Abstract Meaning | Thanet Markchom | most question answering tasks focuses on predicting concrete answers , e.g. , named entities. these tasks can be normally achieved by understanding the contexts without additional information required. in reading comprehension of abstract meaning \( recam \) task , the abstract answers are introduced. to understand abstract meanings in the context , additional knowledge is essential. in this paper , we propose an approach that leverages the pre-trained bert token embeddings as a prior knowledge resource. according to the results , our approach using the pre-trained bert outperformed the baselines. it shows that the pre-trained bert token embeddings can be used as additional knowledge for understanding abstract meanings in question answering.
