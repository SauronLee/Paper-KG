YoungSheldon at SemEval-2021 Task 5: Fine-tuning Pre-trained Language Models for Toxic Spans Detection using Token classification Objective | Mayukh Sharma | in this paper , we describe our system used for semeval 2021 task 5: toxic spans detection. our proposed system approaches the problem as a token classification task. we trained our model to find toxic words and concatenate their spans to predict the toxic spans within a sentence. we fine-tuned pre-trained language models \( plms \) for identifying the toxic words. for fine-tuning , we stacked the classification layer on top of the plm features of each word to classify if it is toxic or not. plms are pre-trained using different objectives and their performance may differ on downstream tasks. we , therefore , compare the performance of bert , electra , roberta , xlm-roberta , t5 , xlnet , and mpnet for identifying toxic spans within a sentence. our best performing system used roberta. it performed well , achieving an f1 score of 0.6841 and secured a rank of 16 on the official leaderboard.
