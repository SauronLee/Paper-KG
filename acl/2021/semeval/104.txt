XRJL-HKUST at SemEval-2021 Task 4: WordNet-Enhanced Dual Multi-head Co-Attention for Reading Comprehension of Abstract Meaning | Yuxin Jiang | this paper presents our submitted system to semeval 2021 task 4: reading comprehension of abstract meaning. our system uses a large pre-trained language model as the encoder and an additional dual multi-head co-attention layer to strengthen the relationship between passages and question-answer pairs , following the current state-of-the-art model duma. the main difference is that we stack the passage-question and question-passage attention modules instead of calculating parallelly to simulate re-considering process. we also add a layer normalization module to improve the performance of our model. furthermore , to incorporate our known knowledge about abstract concepts , we retrieve the definitions of candidate answers from wordnet and feed them to the model as extra inputs. our system , called wordnet-enhanced dual multi-head co-attention \( wn-duma \) , achieves 86.67% and 89.99% accuracy on the official blind test set of subtask 1 and subtask 2 respectively.
