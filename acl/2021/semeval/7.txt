Alpha at SemEval-2021 Task 6: Transformer Based Propaganda Classification | Zhida Feng | this paper describes our system participated in task 6 of semeval-2021: the task focuses on multimodal propaganda technique classification and it aims to classify given image and text into 22 classes. in this paper , we propose to use transformer based architecture to fuse the clues from both image and text. we explore two branches of techniques including fine-tuning the text pretrained transformer with extended visual features , and fine-tuning the multimodal pretrained transformers. for the visual features , we have tested both grid features based on resnet and salient region features from pretrained object detector. among the pretrained multimodal transformers , we choose ernie-vil , a two-steam cross-attended transformers pretrained on large scale image-caption aligned data. fine-tuing ernie-vil for our task produce a better performance due to general joint multimodal representation for text and image learned by ernie-vil. besides , as the distribution of the classification labels is very unbalanced , we also make a further attempt on the loss function and the experiment result shows that focal loss would perform better than cross entropy loss. last we have won first for subtask c in the final competition.
