LRG at SemEval-2021 Task 4: Improving Reading Comprehension with Abstract Words using Augmentation, Linguistic Features and Voting | Abheesht Sharma | we present our approaches and methods for semeval-2021 task-4 reading comprehension of abstract meaning. given a question with a fill-in-the-blank , and a corresponding context , the task is to predict the most suitable word from a list of 5 options. there are three subtasks: imperceptibility , non-specificity and intersection. we use encoders of transformers-based models pretrained on the mlm task to build our fill-in-the-blank \( fitb \) models. moreover , to model imperceptibility , we define certain linguistic features , and to model non-specificity , we leverage information from hypernyms and hyponyms provided by a lexical database. specifically , for non-specificity , we try out augmentation techniques , and other statistical techniques. we also propose variants , namely chunk voting and max context , to take care of input length restrictions for bert , etc. additionally , we perform a thorough ablation study , and use integrated gradients to explain our predictions on a few samples. our models achieve accuracies of 75.31% and 77.84% , on the test sets for subtask-i and subtask-ii , respectively. for subtask-iii , we achieve accuracies of 65.64% and 64.27%.
