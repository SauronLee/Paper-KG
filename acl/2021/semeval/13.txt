IAPUCP at SemEval-2021 Task 1: Stacking Fine-Tuned Transformers is Almost All You Need for Lexical Complexity Prediction | Kervy Rivas Rojas | this paper describes our submission to semeval-2021 task 1: predicting the complexity score for single words. our model leverages standard morphosyntactic and frequency-based features that proved helpful for complex word identification \( a related task \) , and combines them with predictions made by transformer-based pre-trained models that were fine-tuned on the shared task data. our submission system stacks all previous models with a lightgbm at the top. one novelty of our approach is the use of multi-task learning for fine-tuning a pre-trained model for both lexical complexity prediction and word sense disambiguation. our analysis shows that all independent models achieve a good performance in the task , but that stacking them obtains a pearson correlation of 0.7704 , merely 0.018 points behind the winning submission.
