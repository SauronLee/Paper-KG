TA-MAMC at SemEval-2021 Task 4: Task-adaptive Pretraining and Multi-head Attention for Abstract Meaning Reading Comprehension | Jing Zhang | this paper describes our system used in the semeval-2021 task4 reading comprehension of abstract meaning , achieving 1st for subtask 1 and 2nd for subtask 2 on the leaderboard. we propose an ensemble of electra-based models with task-adaptive pretraining and a multi-head attention multiple-choice classifier on top of the pre-trained model. the main contributions of our system are 1 \) revealing the performance discrepancy of different transformer-based pretraining models on the downstream task , 2 \) presentation of an efficient method to generate large task-adaptive corpora for pretraining. we also investigated several pretraining strategies and contrastive learning objectives. our system achieves a test accuracy of 95.11 and 94.89 on subtask 1 and subtask 2 respectively.
