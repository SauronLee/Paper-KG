CS-UM6P at SemEval-2021 Task 1: A Deep Learning Model-based Pre-trained Transformer Encoder for Lexical Complexity | Nabil El Mamoun | lexical complexity prediction \( lcp \) involves assigning a difficulty score to a particular word or expression , in a text intended for a target audience. in this paper , we introduce a new deep learning-based system for this challenging task. the proposed system consists of a deep learning model , based on pre-trained transformer encoder , for word and multi-word expression \( mwe \) complexity prediction. first , on top of the encoderâ€™s contextualized word embedding , our model employs an attention layer on the input context and the complex word or mwe. then , the attention output is concatenated with the pooled output of the encoder and passed to a regression module. we investigate both single-task and joint training on both sub-tasks data using multiple pre-trained transformer-based encoders. the obtained results are very promising and show the effectiveness of fine-tuning pre-trained transformers for lcp task.
