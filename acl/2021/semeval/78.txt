RG PA at SemEval-2021 Task 1: A Contextual Attention-based Model with RoBERTa for Lexical Complexity Prediction | Gang Rao | in this paper we propose a contextual attention based model with two-stage fine-tune training using roberta. first , we perform the first-stage fine-tune on corpus with roberta , so that the model can learn some prior domain knowledge. then we get the contextual embedding of context words based on the token-level embedding with the fine-tuned model. and we use kfold cross-validation to get k models and ensemble them to get the final result. finally , we attain the 2nd place in the final evaluation phase of sub-task 2 with pearson correlation of 0.8575.
