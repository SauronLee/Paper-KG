YNU-HPCC at SemEval-2021 Task 5: Using a Transformer-based Model with Auxiliary Information for Toxic Span Detection | Ruijun Chen | toxic span detection requires the detection of spans that make a text toxic instead of simply classifying the text. in this paper , a transformer-based model with auxiliary information is proposed for semeval-2021 task 5. the proposed model was implemented based on the bert-crf architecture. it consists of three parts: a transformer-based model that can obtain the token representation , an auxiliary information module that combines features from different layers , and an output layer used for the classification. various bert-based models , such as bert , albert , roberta , and xlnet , were used to learn contextual representations. the predictions of these models were assembled to improve the sequence labeling tasks by using a voting strategy. experimental results showed that the introduced auxiliary information can improve the performance of toxic spans detection. the proposed model ranked 5th of 91 in the competition. the code of this study is available at https://github.com/chenrj233/semeval2021_task5
