NLP-IIS@UT at SemEval-2021 Task 4: Machine Reading Comprehension using the Long Document Transformer | Hossein Basafa | this paper presents a technical report of our submission to the 4th task of semeval-2021 , titled: reading comprehension of abstract meaning. in this task , we want to predict the correct answer based on a question given a context. usually , contexts are very lengthy and require a large receptive field from the model. thus , common contextualized language models like bert miss fine representation and performance due to the limited capacity of the input tokens. to tackle this problem , we used the longformer model to better process the sequences. furthermore , we utilized the method proposed in the longformer benchmark on wikihop dataset which improved the accuracy on our task data from \( 23.01% and 22.95% \) achieved by the baselines for subtask 1 and 2 , respectively , to \( 70.30% and 64.38% \) .
