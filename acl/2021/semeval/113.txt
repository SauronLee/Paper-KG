GHOST at SemEval-2021 Task 5: Is explanation all you need? | Kamil Pluciński | this paper discusses different approaches to the toxic spans detection task. the problem posed by the task was to determine which words contribute mostly to recognising a document as toxic. as opposed to binary classification of entire texts , word-level assessment could be of great use during comment moderation , also allowing for a more in-depth comprehension of the model’s predictions. as the main goal was to ensure transparency and understanding , this paper focuses on the current state-of-the-art approaches based on the explainable ai concepts and compares them to a supervised learning solution with word-level labels. the work consists of two xai approaches that automatically provide the explanation for models trained for binary classification of toxic documents: an lstm model with attention as a model-specific approach and the shapley values for interpreting bert predictions as a model-agnostic method. the competing approach considers this problem as supervised token classification , where models like bert and its modifications were tested. the paper aims to explore , compare and assess the quality of predictions for different methods on the task. the advantages of each approach and further research direction are also discussed.
