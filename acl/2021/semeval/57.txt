YNU-HPCC at SemEval-2021 Task 11: Using a BERT Model to Extract Contributions from NLP Scholarly Articles | Xinge Ma | this paper describes the system we built as the ynu-hpcc team in the semeval-2021 task 11: nlpcontributiongraph. this task involves first identifying sentences in the given natural language processing \( nlp \) scholarly articles that reflect research contributions through binary classification; then identifying the core scientific terms and their relation phrases from these contribution sentences by sequence labeling; and finally , these scientific terms and relation phrases are categorized , identified , and organized into subject-predicate-object triples to form a knowledge graph with the help of multiclass classification and multi-label classification. we developed a system for this task using a pre-trained language representation model called bert that stands for bidirectional encoder representations from transformers , and achieved good results. the average f1-score for evaluation phase 2 , part 1 was 0.4562 and ranked 7th , and the average f1-score for evaluation phase 2 , part 2 was 0.6541 , and also ranked 7th.
