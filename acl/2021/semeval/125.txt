SkoltechNLP at SemEval-2021 Task 5: Leveraging Sentence-level Pre-training for Toxic Span Detection | David Dale | this work describes the participation of the skoltech nlp group team \( sk \) in the toxic spans detection task at semeval-2021. the goal of the task is to identify the most toxic fragments of a given sentence , which is a binary sequence tagging problem. we show that fine-tuning a roberta model for this problem is a strong baseline. this baseline can be further improved by pre-training the roberta model on a large dataset labeled for toxicity at the sentence level. while our solution scored among the top 20% participating models , it is only 2 points below the best result. this suggests the viability of our approach.
