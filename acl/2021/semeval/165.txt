UoR at SemEval-2021 Task 7: Utilizing Pre-trained DistilBERT Model and Multi-scale CNN for Humor Detection | Zehao Liu | humour detection is an interesting but difficult task in nlp. because humorous might not be obvious in text , it can be embedded into context , hide behind the literal meaning and require prior knowledge to understand. we explored different shallow and deep methods to create a humour detection classifier for task 7-1a. models like logistic regression , lstm , mlp , cnn were used , and pre-trained models like distilbert were introduced to generate accurate vector representation for textual data. we focused on applying multi-scale strategy on modelling , and compared different models. our best model is the distilbert+multiscale cnn , it used different sizes of cnn kernel to get multiple scales of features , which achieved 93.7% f1-score and 92.1% accuracy on the test set.
