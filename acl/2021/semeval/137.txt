LZ1904 at SemEval-2021 Task 5: Bi-LSTM-CRF for Toxic Span Detection using Pretrained Word Embedding | Liang Zou | recurrent neural networks \( rnn \) have been widely used in various natural language processing \( nlp \) tasks such as text classification , sequence tagging , and machine translation. long short term memory \( lstm \) , a special unit of rnn , has the benefit of memorizing past and even future information in a sentence \( especially for bidirectional lstm \) . in the shared task of detecting spans which make texts toxic , we first apply pretrained word embedding \( glove \) to generate the word vectors after tokenization. and then we construct bidirectional long short term memory-conditional random field \( bi-lstm-crf \) model by baidu research to predict whether each word in the sentence is toxic or not. we tune hyperparameters of dropout rate , number of lstm units , embedding size with 10 epochs and choose the best epoch with validation recall. our model achieves an f1 score of 66.99 percent in test dataset.
