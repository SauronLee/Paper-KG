Meta Learning and Its Applications to Natural Language Processing | Hung-yi Lee | deep learning based natural language processing \( nlp \) has become the mainstream of research in recent years and significantly outperforms conventional methods. however , deep learning models are notorious for being data and computation hungry. these downsides limit the application of such models from deployment to different domains , languages , countries , or styles , since collecting in-genre data and model training from scratch are costly. the long-tail nature of human language makes challenges even more significant. meta-learning , or ‘learning to learn’ , aims to learn better learning algorithms , including better parameter initialization , optimization strategy , network architecture , distance metrics , and beyond. meta-learning has been shown to allow faster fine-tuning , converge to better performance , and achieve amazing results for few-shot learning in many applications. meta-learning is one of the most important new techniques in machine learning in recent years. there is a related tutorial in icml 2019 and a related course at stanford , but most of the example applications given in these materials are about image processing. it is believed that meta-learning has great potential to be applied in nlp , and some works have been proposed with notable achievements in several relevant problems , e.g. , relation extraction , machine translation , and dialogue generation and state tracking. however , it does not catch the same level of attention as in the image processing community. in the tutorial , we will first introduce meta-learning approaches and the theory behind them , and then review the works of applying this technology to nlp problems. this tutorial intends to facilitate researchers in the nlp community to understand this new technology better and promote more research studies using this new technology.
