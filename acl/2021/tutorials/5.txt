Recognizing Multimodal Entailment | Cesar Ilharco | how information is created , shared and consumed has changed rapidly in recent decades , in part thanks to new social platforms and technologies on the web. with ever-larger amounts of unstructured and limited labels , organizing and reconciling information from different sources and modalities is a central challenge in machine learning. this cutting-edge tutorial aims to introduce the multimodal entailment task , which can be useful for detecting semantic alignments when a single modality alone does not suffice for a whole content understanding. starting with a brief overview of natural language processing , computer vision , structured data and neural graph learning , we lay the foundations for the multimodal sections to follow. we then discuss recent multimodal learning literature covering visual , audio and language streams , and explore case studies focusing on tasks which require fine-grained understanding of visual and linguistic semantics question answering , veracity and hatred classification. finally , we introduce a new dataset for recognizing multimodal entailment , exploring it in a hands-on collaborative section. overall , this tutorial gives an overview of multimodal learning , introduces a multimodal entailment dataset , and encourages future research in the topic.
